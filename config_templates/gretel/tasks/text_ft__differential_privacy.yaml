schema_version: "1.0"
name: default
task:
  name: text_ft
  config:
    train:
      pretrained_model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
      column_name: null  # Specify column name for data if using multiple columns
      params:
        batch_size: 16  # Number of samples used to compute gradient
        gradient_accumulation_steps: 8  # Number of steps to accumulate gradients before updating
        epochs: 3  # Number of times to iterate over the entire dataset
        weight_decay: 0.01
        warmup_steps: 100
        lr_scheduler: "linear"
        learning_rate: 0.001 # Initial learning rate for training
        max_tokens: 128  # Increase to allow for longer sequences
      peft_params:
        lora_r: 8
        lora_alpha_over_r: 1
      privacy_params:
        dp: true  # Enable differentially private fine-tuning via DP-SGD
        epsilon: 5  # Privacy budget (lower values = stronger privacy)
        delta: auto  # Probability of privacy leakage (auto-calculated)
    generate:
      num_records: 80  # Number of records to generate
      maximum_text_length: 128  # Maximum length of generated texts in tokens
