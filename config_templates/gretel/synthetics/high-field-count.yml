# For highly dimensional or sparse data, you ideally want to
# collapse this into the most compact representation possible
# to help the model learn quickly. For example:
#
# - Remove fields / columns that are not required for downstream ML training
# - Encode sparse one-hot encoded variables into a single categorical field
#
# Lowering the softmax, by setting the gen_temp parameter may help
# with records that fail validation.
#
# Also lowering the vocab size to 0, which will use a character-based
# tokenizer may help the model learn semantics one character at a time

schema_version: "1.0"

models:
  - synthetics:
      data_source: "__tmp__"
      params:
        epochs: 100
        vocab_size: 0
        learning_rate: 0.001
        rnn_units: 256
        gen_temp: 0.8
