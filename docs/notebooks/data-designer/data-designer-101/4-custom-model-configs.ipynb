{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a759e70",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/navigator/data-designer-101/4-custom-model-configs.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm0zecvop88n"
   },
   "source": [
    "# ðŸŽ¨ Data Designer 101: Using Custom Model Configurations\n",
    "\n",
    "In this notebook, we will see how to create and use custom model configurations in `DataDesigner`.\n",
    "\n",
    "If this is your first time using `DataDesigner`, we recommend starting with the [first notebook](https://github.com/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/navigator/data-designer-101/1-the-basics.ipynb) in this 101 series.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ðŸ’¾ Install `gretel-client` and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2-9-lPup--8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/gretelai/gretel-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnX4D3hzt7f0"
   },
   "outputs": [],
   "source": [
    "from gretel_client.navigator_client import Gretel\n",
    "from gretel_client.workflows.configs.workflows import ModelConfig, GenerationParameters\n",
    "\n",
    "# We import AIDD column and parameter types using this shorthand for convenience.\n",
    "import gretel_client.data_designer.params as P\n",
    "import gretel_client.data_designer.columns as C\n",
    "\n",
    "# The Gretel object is the SDK's main entry point for interacting with Gretel's API.\n",
    "gretel = Gretel(api_key=\"prompt\", endpoint=\"https://api.dev.gretel.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "1ykd5Fkkp88p",
    "outputId": "616cfa43-d0d8-452b-9967-f718cb57fc59"
   },
   "source": [
    "## âš™ï¸ Custom Model Configurations\n",
    "\n",
    "- `DataDesigner` comes with sensible defaults for LLMs and their generation settings, but sometimes you need more control.\n",
    "\n",
    "- This is where custom model configurations come in.\n",
    "\n",
    "- Below, we create two new \"model aliases\" that we can set as the LLM for any task that has `model_alias` as an argument. \n",
    "\n",
    "- Note that the models selected for the model alias must be one of the allowed models with in the selected Model Suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "sdD4GaJ6p88q",
    "outputId": "26e22ccf-a9e8-4733-e344-27e919fb35d2"
   },
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    # Configuration with static temperature\n",
    "    ModelConfig(\n",
    "        alias=\"mistral-small-static-higher-temp\",\n",
    "        model_name=\"gretel/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "        generation_parameters=GenerationParameters(temperature=0.75, top_p=0.9)\n",
    "    ),\n",
    "    # Configuration with variable temperature (uniform distribution), which \n",
    "    # is sampled for every LLM call.\n",
    "    ModelConfig(\n",
    "        alias=\"mistral-small-variable-higher-temp\",\n",
    "        model_name=\"gretel/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "        generation_parameters=GenerationParameters(\n",
    "            temperature={\"type\": \"uniform\", \"params\": {\"low\": 0.50, \"high\": 0.90}},\n",
    "            top_p=0.9\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data designer with our custom model configurations\n",
    "aidd = gretel.data_designer.new(model_suite=\"apache-2.0\", model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘©â€âš•ï¸ Designing our synthetic dataset\n",
    "\n",
    "New features demonstrated below:\n",
    "\n",
    "- Using custom model aliases \n",
    "\n",
    "- Conditional params for samplers\n",
    "\n",
    "- If/else logic in Jinja expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"person1\",  \n",
    "        type=P.SamplerType.PERSON,\n",
    "        params=P.PersonSamplerParams(sex=\"Male\")\n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"person2\",  \n",
    "        type=P.SamplerType.PERSON,\n",
    "        params=P.PersonSamplerParams(sex=\"Female\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add pet_type column with conditional parameters.\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"pet_type\",\n",
    "        type=P.SamplerType.CATEGORY,\n",
    "        # These will be the default values for the sampler.\n",
    "        params=P.CategorySamplerParams(values=[\"dog\", \"cat\", \"fish\"], weights=[0.5, 0.3, 0.2]),\n",
    "        # These will be the values for the sampler if the condition is met.\n",
    "        conditional_params={\n",
    "            \"number_of_pets == 0\": P.CategorySamplerParams(values=[\"none\"])\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"first_pet_name\",\n",
    "        type=P.SamplerType.SUBCATEGORY,\n",
    "        params=P.SubcategorySamplerParams(\n",
    "            category=\"pet_type\",\n",
    "            values={\n",
    "                \"dog\": [\"Buddy\", \"Max\", \"Charlie\", \"Cooper\", \"Daisy\", \"Lucy\"],\n",
    "                \"cat\": [\"Oliver\", \"Leo\", \"Milo\", \"Charlie\", \"Simba\", \"Luna\"],\n",
    "                \"fish\": [\"Bubbles\", \"Nemo\", \"Goldie\", \"Dory\", \"Finley\", \"Splash\"],\n",
    "                \"none\": [\"n/a\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"number_of_pets\",\n",
    "        type=P.SamplerType.POISSON,\n",
    "        params=P.PoissonSamplerParams(mean=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Use jinja if/else logic to set the number of children.\n",
    "aidd.add_column(\n",
    "    C.ExpressionColumn(\n",
    "        name=\"number_of_children\",\n",
    "        expr=\"{% if number_of_pets > 0 %}{{ 2 * number_of_pets - 1}}{% else %}0{% endif %}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.ExpressionColumn(\n",
    "        name=\"person1_full_name\",\n",
    "        expr=\"{{ person1.first_name }} {{ person1.last_name }}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.ExpressionColumn(\n",
    "        name=\"person2_full_name\",\n",
    "        expr=\"{{ person2.first_name }} {{ person2.last_name }}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "aidd.add_column(\n",
    "    C.LLMTextColumn(\n",
    "        name=\"first_pet_backstory\",\n",
    "        prompt=(\n",
    "            \"{% if number_of_pets > 0 %}\"\n",
    "            \"Write a sweet backstory for {{ person1.first_name }} and \"\n",
    "            \"{{ person2.first_name }}'s first pet {{ pet_type }} named {{ first_pet_name }}. \"\n",
    "            \"Keep it concise, no more than 8 sentences.\"\n",
    "            \"{% else %}\"\n",
    "            \"Repeat exactly these words: 'They had no pets.'\"\n",
    "            \"{% endif %}\"\n",
    "        ),\n",
    "        # We're using our custom model with static temperature.\n",
    "        model_alias=\"mistral-small-static-higher-temp\",  \n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.LLMTextColumn(\n",
    "        name=\"couple_backstory\",\n",
    "        prompt=(\n",
    "            \"Write a thoughtful, funny backstory for how {{ person1_full_name }} and {{ person2_full_name }} met. \"\n",
    "            \"{% if number_of_pets > 0 %}\"\n",
    "            \"Make sure to include how they decided to get a pet together, ultimately leading to {{ number_of_pets }} pets. \"\n",
    "            \"Note their first pet was named {{ first_pet_name }}, with the following backstory:\\n\\n{{ first_pet_backstory }}\"\n",
    "            \"{% else %}\"\n",
    "            \"Make sure to include how they decided to not get a pet together.\"\n",
    "            \"{% endif %}\"\n",
    "        ),\n",
    "        # We're using our custom model with variable temperature.\n",
    "        model_alias=\"mistral-small-variable-higher-temp\",  \n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "aidd.with_evaluation_report().validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘€ Preview the dataset\n",
    "\n",
    "- Iteration is key to generating high-quality synthetic data.\n",
    "\n",
    "- Use the `preview` method to generate 10 records for inspection.\n",
    "\n",
    "- We set `verbose_logging` to `True` to see additional logging to verify our custom model aliases are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = aidd.preview(verbose_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preview dataset is available as a pandas DataFrame.\n",
    "preview.dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to cycle through the 10 preview records.\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ†™ Scale up!\n",
    "\n",
    "- Once you are happy with the preview, scale up to a larger dataset by submitting a batch workflow.\n",
    "\n",
    "- You can view the evaluation report by following the workflow link in the output of `create` below.\n",
    "\n",
    "- Click the link to follow along with the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_run = aidd.create(num_records=100, name=\"aidd-101-notebook-4-custom-model-configs\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
