{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/kirit-branch/docs/notebooks/data-designer/rag-examples/generate-rag-evaluation-dataset.ipynb\">\n",
                "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
                "</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "118d856f",
            "metadata": {},
            "source": [
                "# ðŸŽ¨ Data Designer: Generate Diverse RAG Evaluations\n",
                "\n",
                "This tutorial demonstrates how to generate comprehensive evaluation datasets for Retrieval-Augmented Generation (RAG) systems, customized to your content and use cases. \n",
                "\n",
                "You'll learn how to create diverse question-answer pairs at scale, covering a variety of difficulty levels and reasoning types, including both answerable and unanswerable scenarios.\n",
                "\n",
                "### What You'll Learn\n",
                "- How to process and chunk source documents for RAG evaluation\n",
                "\n",
                "- How to configure categorical distributions for controlled diversity\n",
                "\n",
                "- How to generate high-quality Q&A pairs with structured output\n",
                "\n",
                "- How to evaluate the quality of generated pairs with rubric-based scoring\n",
                "\n",
                "- How to analyze and export the complete dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8d1e5986",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation\n",
                "\n",
                "First, we'll install the required packages for document processing, text generation, and data handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7adc5ee0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries\n",
                "!pip install -qq langchain smart_open git+https://github.com/gretelai/gretel-python-client@main\n",
                "!pip install 'unstructured[pdf]'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e78f8070",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "\n",
                "Let's define our source documents and the total number of evaluation pairs we want to generate. You can replace the document list with your own PDFs, web pages, or other text sources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "fd6f9e64",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define source documents and total number of evaluation pairs to generate\n",
                "# You can replace this with your own documents\n",
                "DOCUMENT_LIST = [\"https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/rag_evals/databricks-state-of-data-ai-report.pdf\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e0c98449",
            "metadata": {},
            "source": [
                "## 3. Document Processing\n",
                "\n",
                "Now we'll create a Document Processor class that handles loading and chunking the source documents. \n",
                "\n",
                "This class uses langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "bfec3608",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Union\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from unstructured.partition.auto import partition\n",
                "from smart_open import open\n",
                "import tempfile\n",
                "import os\n",
                "\n",
                "class DocumentProcessor:\n",
                "    \"\"\"Handles loading and chunking source documents for RAG evaluation.\"\"\"\n",
                "    \n",
                "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
                "        \"\"\"Initialize with configurable chunk size and overlap.\"\"\"\n",
                "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
                "            chunk_size=chunk_size,\n",
                "            chunk_overlap=chunk_overlap,\n",
                "            length_function=len,\n",
                "        )\n",
                "\n",
                "    def parse_document(self, uri: str) -> str:\n",
                "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
                "        with open(uri, 'rb') as file:\n",
                "            content = file.read()\n",
                "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
                "                temp_file.write(content)\n",
                "                temp_file.flush()\n",
                "                elements = partition(temp_file.name)\n",
                "\n",
                "        os.unlink(temp_file.name)\n",
                "        return \"\\n\\n\".join([str(element) for element in elements])\n",
                "\n",
                "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
                "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
                "        if isinstance(uris, str):\n",
                "            uris = [uris]\n",
                "\n",
                "        all_chunks = []\n",
                "        for uri in uris:\n",
                "            text = self.parse_document(uri)\n",
                "            chunks = self.text_splitter.split_text(text)\n",
                "            all_chunks.extend(chunks)\n",
                "\n",
                "        return all_chunks"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c44785c",
            "metadata": {},
            "source": [
                "## 4. Data Models\n",
                "\n",
                "Let's define Pydantic models for structured output generation. These schemas will ensure our generated data has consistent structure and validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "9cab035f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "\n",
                "class QAPair(BaseModel):\n",
                "    question: str = Field(\n",
                "        ..., description=\"A specific question related to the domain of the context\"\n",
                "    )\n",
                "    answer: str = Field(\n",
                "        ..., description=\"Either a context-supported answer or explanation of why the question cannot be answered\"\n",
                "    )\n",
                "    reasoning: str = Field(\n",
                "        ..., description=\"A clear and traceable explanation of the reasoning behind the answer\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ada29f90",
            "metadata": {},
            "source": [
                "## 5. Processing Documents and Setting Up Data Designer\n",
                "\n",
                "Now we'll process our document chunks and set up the Data Designer with our seed dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5325b303",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from gretel_client.navigator_client import Gretel\n",
                "\n",
                "# Process document chunks\n",
                "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
                "chunks = processor.process_documents(DOCUMENT_LIST)\n",
                "\n",
                "# Create a seed DataFrame with the document chunks\n",
                "seed_df = pd.DataFrame({\"context\": chunks})\n",
                "\n",
                "# Initialize Gretel client and Data Designer\n",
                "# You can use \"prompt\" for API key to be prompted interactively\n",
                "gretel = Gretel(api_key=\"prompt\")\n",
                "aidd = gretel.data_designer.new(model_suite=\"llama-3.x\")\n",
                "\n",
                "# Upload the seed dataset with document chunks\n",
                "# Using shuffle with replacement allows the model to reuse context chunks\n",
                "aidd.with_seed_dataset(seed_df, sampling_strategy=\"shuffle\", with_replacement=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "280e2fec",
            "metadata": {},
            "source": [
                "## 6. Adding Categorical Columns for Controlled Diversity\n",
                "\n",
                "Now we'll add categorical columns to control the diversity of our RAG evaluation pairs. We'll define:\n",
                "\n",
                "1. **Difficulty levels**: easy, medium, hard\n",
                "\n",
                "2. **Reasoning types**: factual recall, inferential reasoning, etc.\n",
                "\n",
                "3. **Question types**: answerable vs. unanswerable (with weighting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3e27cac",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gretel_client.data_designer import columns as C\n",
                "from gretel_client.data_designer import params as P\n",
                "\n",
                "aidd.add_column(\n",
                "    C.SamplerColumn(\n",
                "        name=\"difficulty\",\n",
                "        type=P.SamplerType.CATEGORY,\n",
                "        params=P.CategorySamplerParams(\n",
                "            values=[\"easy\", \"medium\", \"hard\"],\n",
                "            description=\"The difficulty level of the question\"\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "aidd.add_column(\n",
                "    C.SamplerColumn(\n",
                "        name=\"reasoning_type\",\n",
                "        type=P.SamplerType.CATEGORY,\n",
                "        params=P.CategorySamplerParams(\n",
                "            values=[\n",
                "                \"factual recall\",\n",
                "                \"inferential reasoning\",\n",
                "                \"comparative analysis\",\n",
                "                \"procedural understanding\",\n",
                "                \"cause and effect\"\n",
                "            ],\n",
                "            description=\"The type of reasoning required to answer the question\"\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "aidd.add_column(\n",
                "    C.SamplerColumn(\n",
                "        name=\"question_type\",\n",
                "        type=P.SamplerType.CATEGORY,\n",
                "        params=P.CategorySamplerParams(\n",
                "            values=[\"answerable\", \"unanswerable\"],\n",
                "            # 10:1 ratio of answerable to unanswerable questions.\n",
                "            weights=[10, 1],  \n",
                "        )\n",
                "    )\n",
                ").validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "735cbbea",
            "metadata": {},
            "source": [
                "## 7. Adding LLM-Structured Column for Q&A Pair Generation\n",
                "\n",
                "Now let's set up the core of our data generation: the Q&A pair column that will produce structured question-answer pairs based on our document context and control parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecf44d9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gretel_client.data_designer import columns as C\n",
                "\n",
                "# Add Q&A pair generation column\n",
                "aidd.add_column(\n",
                "    C.LLMStructuredColumn(\n",
                "        name=\"qa_pair\",\n",
                "        system_prompt=(\n",
                "            \"You are an expert at generating high-quality RAG evaluation pairs. \"\n",
                "            \"You are very careful in assessing whether the question can be answered from the provided context. \"\n",
                "        ),\n",
                "        prompt=\"\"\"\\\n",
                "{{context}}\n",
                "\n",
                "Generate a {{difficulty}} {{reasoning_type}} question-answer pair.\n",
                "The question should be {{question_type}} using the provided context.\n",
                "\n",
                "For answerable questions:\n",
                "- Ensure the answer is fully supported by the context\n",
                "\n",
                "For unanswerable questions:\n",
                "- Keep the question topically relevant\n",
                "- Make it clearly beyond the context's scope\n",
                "\"\"\",\n",
                "        output_format=QAPair\n",
                "    )\n",
                ").validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41e6cc02",
            "metadata": {},
            "source": [
                "## 8. Adding Evaluation Metrics with Custom Rubrics\n",
                "\n",
                "To assess the quality of our generated Q&A pairs, we'll add evaluation metrics using detailed rubrics for scoring. \n",
                "\n",
                "We use Data Designer's `LLMJudgeColumn` for this, defining a set of custom Rubrics designed for our task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "953bca63",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gretel_client.data_designer.params import Rubric\n",
                "from gretel_client.data_designer import columns as C\n",
                "\n",
                "context_relevance_rubric = Rubric(\n",
                "    name=\"Context Relevance\",\n",
                "    description=\"Evaluates how relevant the answer is to the provided context\",\n",
                "    scoring={\n",
                "        \"5\": \"Perfect relevance to context with no extraneous information\",\n",
                "        \"4\": \"Highly relevant with minor deviations from context\",\n",
                "        \"3\": \"Moderately relevant but includes some unrelated information\",\n",
                "        \"2\": \"Minimally relevant with significant departure from context\",\n",
                "        \"1\": \"Almost entirely irrelevant to the provided context\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_precision_rubric = Rubric(\n",
                "    name=\"Answer Precision\",\n",
                "    description=\"Evaluates the accuracy and specificity of the answer\",\n",
                "    scoring={\n",
                "        \"5\": \"Extremely precise with exact, specific information\",\n",
                "        \"4\": \"Very precise with minor imprecisions\",\n",
                "        \"3\": \"Adequately precise but could be more specific\",\n",
                "        \"2\": \"Imprecise with vague or ambiguous information\",\n",
                "        \"1\": \"Completely imprecise or inaccurate\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_completeness_rubric = Rubric(\n",
                "    name=\"Answer Completeness\",\n",
                "    description=\"Evaluates how thoroughly the answer addresses all aspects of the question\",\n",
                "    scoring={\n",
                "        \"5\": \"Fully complete, addressing all aspects of the question\",\n",
                "        \"4\": \"Mostly complete with minor omissions\",\n",
                "        \"3\": \"Adequately complete but missing some details\",\n",
                "        \"2\": \"Substantially incomplete, missing important aspects\",\n",
                "        \"1\": \"Severely incomplete, barely addresses the question\"\n",
                "    }\n",
                ")\n",
                "\n",
                "hallucination_avoidance_rubric = Rubric(\n",
                "    name=\"Hallucination Avoidance\",\n",
                "    description=\"Evaluates the absence of made-up or incorrect information\",\n",
                "    scoring={\n",
                "        \"5\": \"No hallucinations, all information is factual and verifiable\",\n",
                "        \"4\": \"Minimal hallucinations that don't impact the core answer\",\n",
                "        \"3\": \"Some hallucinations that partially affect the answer quality\",\n",
                "        \"2\": \"Significant hallucinations that undermine the answer\",\n",
                "        \"1\": \"Severe hallucinations making the answer entirely unreliable\"\n",
                "    }\n",
                ")\n",
                "\n",
                "EVAL_METRICS_PROMPT_TEMPLATE = \"\"\"\\\n",
                "You are an expert evaluator of question-answer pairs. Analyze the following Q&A pair and evaluate it objectively.\n",
                "\n",
                "For this {{difficulty}} {{reasoning_type}} Q&A pair:\n",
                "{{qa_pair}}\n",
                "\n",
                "Take a deep breath and carefully evaluate each criterion based on the provided rubrics, considering the difficulty level and reasoning type indicated.\n",
                "\"\"\"\n",
                "\n",
                "aidd.add_column(\n",
                "    C.LLMJudgeColumn(\n",
                "        name=\"eval_metrics\",\n",
                "        prompt=EVAL_METRICS_PROMPT_TEMPLATE,\n",
                "        rubrics=[context_relevance_rubric, answer_precision_rubric, answer_completeness_rubric, hallucination_avoidance_rubric],\n",
                "    )\n",
                ").validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fb3dc84",
            "metadata": {},
            "source": [
                "## 9. Preview Sample Records\n",
                "\n",
                "Let's generate a preview to see what our data will look like before running the full generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b55913d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "preview = aidd.preview()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b655a45f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# The preview dataset is available as a pandas DataFrame.\n",
                "preview.dataset.df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d4364c13",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this cell multiple times to cycle through the 10 preview records.\n",
                "preview.display_sample_record()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40099da2",
            "metadata": {},
            "source": [
                "## 11. Generate the Full Dataset\n",
                "\n",
                "Now let's generate our full dataset of RAG evaluation pairs, analyze the coverage, and export it to a JSONL file for use in evaluating RAG systems."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb57388d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's add an evaluation report to the dataset\n",
                "aidd.with_evaluation_report()\n",
                "\n",
                "# Generate the full dataset\n",
                "workflow_run = aidd.create(\n",
                "   num_records=100,\n",
                "   name=\"rag_eval_generation\"\n",
                ")\n",
                "\n",
                "# This will block until the workflow is done.\n",
                "workflow_run.wait_until_done()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac909600",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nGenerated dataset shape:\", workflow_run.dataset.df.shape)\n",
                "\n",
                "# Export the dataset to JSONL format.\n",
                "workflow_run.dataset.df.to_json('rag_evals.jsonl', orient='records', lines=True)\n",
                "print(\"\\nDataset exported to rag_evals.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "19c674f4",
            "metadata": {},
            "source": [
                "## 12. Using Your RAG Evaluation Dataset\n",
                "\n",
                "Now that you've generated a diverse RAG evaluation dataset, here are some ways to use it:\n",
                "\n",
                "1. **Benchmarking**: Test your RAG system against these evaluation pairs to measure performance\n",
                "\n",
                "2. **Error Analysis**: Identify patterns in where your RAG system struggles\n",
                "\n",
                "3. **Optimization**: Use insights to tune retrieval and generation parameters\n",
                "\n",
                "4. **Regression Testing**: Track performance over time as you improve your system\n",
                "\n",
                "5. **Model Comparison**: Compare different LLMs, retrievers, or RAG architectures\n",
                "\n",
                "The JSONL file contains structured data with questions, ground truth answers, and quality metrics that you can use with most evaluation frameworks."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
