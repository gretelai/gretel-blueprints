{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/kirit-branch/docs/notebooks/demo/navigator/rag-examples/generate-rag-evaluation-dataset.ipynb\">\n",
                "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
                "</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "118d856f",
            "metadata": {},
            "source": [
                "# ðŸŽ¨ Data Designer: Generate Diverse RAG Evaluations\n",
                "\n",
                "This tutorial demonstrates how to generate comprehensive evaluation datasets for Retrieval-Augmented Generation (RAG) systems, customized to your content and use cases. You'll learn how to create diverse question-answer pairs at scale, covering a variety of difficulty levels and reasoning types, including both answerable and unanswerable scenarios.\n",
                "\n",
                "## What You'll Learn\n",
                "- How to process and chunk source documents for RAG evaluation\n",
                "- How to configure categorical distributions for controlled diversity\n",
                "- How to generate high-quality Q&A pairs with structured output\n",
                "- How to evaluate the quality of generated pairs with rubric-based scoring\n",
                "- How to analyze and export the complete dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8d1e5986",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation\n",
                "\n",
                "First, we'll install the required packages for document processing, text generation, and data handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7adc5ee0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries\n",
                "!pip install -qq langchain smart_open git+https://github.com/gretelai/gretel-python-client@main\n",
                "!pip install 'unstructured[pdf]'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e78f8070",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "\n",
                "Let's define our source documents and the total number of evaluation pairs we want to generate. You can replace the document list with your own PDFs, web pages, or other text sources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "fd6f9e64",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define source documents and total number of evaluation pairs to generate\n",
                "# You can replace this with your own documents\n",
                "DOCUMENT_LIST = [\"https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/rag_evals/databricks-state-of-data-ai-report.pdf\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e0c98449",
            "metadata": {},
            "source": [
                "## 3. Document Processing\n",
                "\n",
                "Now we'll create a Document Processor class that handles loading and chunking the source documents. This class uses langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "bfec3608",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Union\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from unstructured.partition.auto import partition\n",
                "from smart_open import open\n",
                "import tempfile\n",
                "import os\n",
                "\n",
                "class DocumentProcessor:\n",
                "    \"\"\"Handles loading and chunking source documents for RAG evaluation.\"\"\"\n",
                "    \n",
                "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
                "        \"\"\"Initialize with configurable chunk size and overlap.\"\"\"\n",
                "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
                "            chunk_size=chunk_size,\n",
                "            chunk_overlap=chunk_overlap,\n",
                "            length_function=len,\n",
                "        )\n",
                "\n",
                "    def parse_document(self, uri: str) -> str:\n",
                "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
                "        with open(uri, 'rb') as file:\n",
                "            content = file.read()\n",
                "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
                "                temp_file.write(content)\n",
                "                temp_file.flush()\n",
                "                elements = partition(temp_file.name)\n",
                "\n",
                "        os.unlink(temp_file.name)\n",
                "        return \"\\n\\n\".join([str(element) for element in elements])\n",
                "\n",
                "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
                "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
                "        if isinstance(uris, str):\n",
                "            uris = [uris]\n",
                "\n",
                "        all_chunks = []\n",
                "        for uri in uris:\n",
                "            text = self.parse_document(uri)\n",
                "            chunks = self.text_splitter.split_text(text)\n",
                "            all_chunks.extend(chunks)\n",
                "\n",
                "        return all_chunks"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c44785c",
            "metadata": {},
            "source": [
                "## 4. Data Models\n",
                "\n",
                "Let's define Pydantic models for structured output generation. These schemas will ensure our generated data has consistent structure and validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "9cab035f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "from typing import Optional, Literal\n",
                "\n",
                "class QAPair(BaseModel):\n",
                "    \"\"\"Schema for question-answer evaluation pairs\"\"\"\n",
                "    question: str = Field(..., description=\"A specific question related to the domain of the context\")\n",
                "    answer: str = Field(..., description=\"Either a context-supported answer or explanation of why the question cannot be answered\")\n",
                "\n",
                "class EvalMetrics(BaseModel):\n",
                "    \"\"\"Schema for scoring generation quality\"\"\"\n",
                "    context_relevance: int = Field(..., \n",
                "                                  description=\"How relevant the retrieved context is (1=irrelevant, 5=perfectly relevant)\", \n",
                "                                  ge=1, le=5)\n",
                "    answer_precision: int = Field(..., \n",
                "                                 description=\"Answer accuracy or appropriateness (1=incorrect/inappropriate, 5=perfect)\", \n",
                "                                 ge=1, le=5)\n",
                "    answer_completeness: int = Field(..., \n",
                "                                    description=\"Information completeness (1=missing critical info, 5=fully complete)\", \n",
                "                                    ge=1, le=5)\n",
                "    hallucination_avoidance: int = Field(..., \n",
                "                                        description=\"Adherence to facts (1=complete fabrication, 5=no hallucination)\", \n",
                "                                        ge=1, le=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ada29f90",
            "metadata": {},
            "source": [
                "## 5. Processing Documents and Setting Up Data Designer\n",
                "\n",
                "Now we'll process our document chunks and set up the Data Designer with our seed dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5325b303",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from gretel_client.navigator_client import Gretel\n",
                "\n",
                "# Process document chunks\n",
                "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
                "chunks = processor.process_documents(DOCUMENT_LIST)\n",
                "\n",
                "# Create a seed DataFrame with the document chunks\n",
                "seed_df = pd.DataFrame({\"context\": chunks})\n",
                "\n",
                "# Initialize Gretel client and Data Designer\n",
                "# You can use \"prompt\" for API key to be prompted interactively\n",
                "gretel = Gretel(api_key=\"prompt\", endpoint=\"https://api.dev.gretel.ai\")\n",
                "aidd = gretel.data_designer.new(model_suite=\"llama-3.x\")\n",
                "\n",
                "# Upload the seed dataset with document chunks\n",
                "# Using shuffle with replacement allows the model to reuse context chunks\n",
                "aidd.with_seed_dataset(seed_df, sampling_strategy=\"shuffle\", with_replacement=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "280e2fec",
            "metadata": {},
            "source": [
                "## 6. Adding Categorical Columns for Controlled Diversity\n",
                "\n",
                "Now we'll add categorical columns to control the diversity of our RAG evaluation pairs. We'll define:\n",
                "\n",
                "1. **Difficulty levels**: easy, medium, hard\n",
                "2. **Reasoning types**: factual recall, inferential reasoning, etc.\n",
                "3. **Question types**: answerable vs. unanswerable (with weighting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3e27cac",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from gretel_client.data_designer import columns as C\n",
                "from gretel_client.data_designer import params as P\n",
                "\n",
                "# Add difficulty levels\n",
                "aidd.add_column(C.SamplerColumn(\n",
                "    name=\"difficulty\",\n",
                "    type=P.SamplerType.CATEGORY,\n",
                "    params=P.CategorySamplerParams(\n",
                "        values=[\"easy\", \"medium\", \"hard\"],\n",
                "        description=\"The difficulty level of the question\"\n",
                "    )\n",
                "))\n",
                "\n",
                "# Add reasoning types\n",
                "aidd.add_column(C.SamplerColumn(\n",
                "    name=\"reasoning_type\",\n",
                "    type=P.SamplerType.CATEGORY,\n",
                "    params=P.CategorySamplerParams(\n",
                "        values=[\n",
                "            \"factual recall\",\n",
                "            \"inferential reasoning\",\n",
                "            \"comparative analysis\",\n",
                "            \"procedural understanding\",\n",
                "            \"cause and effect\"\n",
                "        ],\n",
                "        description=\"The type of reasoning required to answer the question\"\n",
                "    )\n",
                "))\n",
                "\n",
                "# Add question type with weights to generate more answerable questions than unanswerable\n",
                "aidd.add_column(C.SamplerColumn(\n",
                "    name=\"question_type\",\n",
                "    type=P.SamplerType.CATEGORY,\n",
                "    params=P.CategorySamplerParams(\n",
                "        values=[\"answerable\", \"unanswerable\"],\n",
                "        weights=[10, 1],  # 10:1 ratio of answerable to unanswerable questions\n",
                "        description=\"Whether the question can be answered from the provided context\"\n",
                "    )\n",
                "))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "735cbbea",
            "metadata": {},
            "source": [
                "## 7. Adding LLM-Structured Column for Q&A Pair Generation\n",
                "\n",
                "Now let's set up the core of our data generation: the Q&A pair column that will produce structured question-answer pairs based on our document context and control parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecf44d9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gretel_client.data_designer import columns as C\n",
                "\n",
                "# Add Q&A pair generation column\n",
                "aidd.add_column(\n",
                "    C.LLMStructuredColumn(\n",
                "        name=\"qa_pair\",\n",
                "        system_prompt=\"\"\"You are an expert at generating high-quality RAG evaluation pairs.\n",
                "    Your output should include both answerable and unanswerable questions to properly test RAG systems.\"\"\",\n",
                "        prompt=\"\"\"\\n{{context}}\\n\\n\n",
                "    Generate a {{difficulty}} {{reasoning_type}} question-answer pair.\n",
                "    The question should be {{question_type}} using the provided context.\n",
                "\n",
                "    For answerable questions:\n",
                "    - Ensure the answer is fully supported by the context\n",
                "    - Make the reasoning clear and traceable\n",
                "\n",
                "    For unanswerable questions:\n",
                "    - Keep the question topically relevant\n",
                "    - Make it clearly beyond the context's scope\n",
                "    - Explain why it cannot be answered\n",
                "\n",
                "    Put your thoughts within <think>...</think> before providing the JSON.\"\"\",\n",
                "        output_format=QAPair\n",
                "))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41e6cc02",
            "metadata": {},
            "source": [
                "## 8. Adding Evaluation Metrics with Rubrics\n",
                "\n",
                "To assess the quality of our generated Q&A pairs, we'll add evaluation metrics using detailed rubrics for scoring. We use Data Designer's `LLMJudgeColumn` for this. We define a set of Rubrics that we want used to evaluate the question and answer pairs generated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "953bca63",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gretel_client.data_designer.params import Rubric\n",
                "from gretel_client.data_designer import columns as C\n",
                "\n",
                "# Define evaluation rubrics\n",
                "context_relevance_rubric = Rubric(\n",
                "    name=\"Context Relevance\",\n",
                "    description=\"Evaluates how relevant the answer is to the provided context\",\n",
                "    scoring={\n",
                "        \"5\": \"Perfect relevance to context with no extraneous information\",\n",
                "        \"4\": \"Highly relevant with minor deviations from context\",\n",
                "        \"3\": \"Moderately relevant but includes some unrelated information\",\n",
                "        \"2\": \"Minimally relevant with significant departure from context\",\n",
                "        \"1\": \"Almost entirely irrelevant to the provided context\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_precision_rubric = Rubric(\n",
                "    name=\"Answer Precision\",\n",
                "    description=\"Evaluates the accuracy and specificity of the answer\",\n",
                "    scoring={\n",
                "        \"5\": \"Extremely precise with exact, specific information\",\n",
                "        \"4\": \"Very precise with minor imprecisions\",\n",
                "        \"3\": \"Adequately precise but could be more specific\",\n",
                "        \"2\": \"Imprecise with vague or ambiguous information\",\n",
                "        \"1\": \"Completely imprecise or inaccurate\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_completeness_rubric = Rubric(\n",
                "    name=\"Answer Completeness\",\n",
                "    description=\"Evaluates how thoroughly the answer addresses all aspects of the question\",\n",
                "    scoring={\n",
                "        \"5\": \"Fully complete, addressing all aspects of the question\",\n",
                "        \"4\": \"Mostly complete with minor omissions\",\n",
                "        \"3\": \"Adequately complete but missing some details\",\n",
                "        \"2\": \"Substantially incomplete, missing important aspects\",\n",
                "        \"1\": \"Severely incomplete, barely addresses the question\"\n",
                "    }\n",
                ")\n",
                "\n",
                "hallucination_avoidance_rubric = Rubric(\n",
                "    name=\"Hallucination Avoidance\",\n",
                "    description=\"Evaluates the absence of made-up or incorrect information\",\n",
                "    scoring={\n",
                "        \"5\": \"No hallucinations, all information is factual and verifiable\",\n",
                "        \"4\": \"Minimal hallucinations that don't impact the core answer\",\n",
                "        \"3\": \"Some hallucinations that partially affect the answer quality\",\n",
                "        \"2\": \"Significant hallucinations that undermine the answer\",\n",
                "        \"1\": \"Severe hallucinations making the answer entirely unreliable\"\n",
                "    }\n",
                ")\n",
                "\n",
                "# Define the prompt template for evaluation\n",
                "EVAL_METRICS_PROMPT_TEMPLATE = \"\"\"\\\n",
                "You are an expert evaluator of question-answer pairs. Analyze the following Q&A pair and evaluate it objectively.\n",
                "\n",
                "For this {{difficulty}} {{reasoning_type}} Q&A pair:\n",
                "{{qa_pair}}\n",
                "\n",
                "Take a deep breath and carefully evaluate each criterion based on the provided rubrics, considering the difficulty level and reasoning type indicated.\n",
                "\"\"\"\n",
                "\n",
                "# Add evaluation metrics column\n",
                "aidd.add_column(\n",
                "    C.LLMJudgeColumn(\n",
                "        name=\"eval_metrics\",\n",
                "        prompt=EVAL_METRICS_PROMPT_TEMPLATE,\n",
                "        rubrics=[context_relevance_rubric, answer_precision_rubric, answer_completeness_rubric, hallucination_avoidance_rubric]\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fb3dc84",
            "metadata": {},
            "source": [
                "## 9. Preview Sample Records\n",
                "\n",
                "Let's generate a preview to see what our data will look like before running the full generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b55913d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview a sample of generated records\n",
                "preview = aidd.preview()\n",
                "\n",
                "# Display a single sample record\n",
                "preview.display_sample_record()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e4d2b2e",
            "metadata": {},
            "source": [
                "Let's also explore the preview data as a DataFrame to better understand the structure:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d59a3ae0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the generated preview as a Pandas DataFrame\n",
                "preview.dataset.df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c1370fb",
            "metadata": {},
            "source": [
                "## 10. Analyze RAG Coverage\n",
                "\n",
                "Now let's create a function to analyze the coverage and quality of our RAG evaluation dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea1f6cd8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from rich.console import Console\n",
                "from rich.table import Table\n",
                "from collections import Counter\n",
                "import pandas as pd\n",
                "\n",
                "def analyze_rag_coverage(df: pd.DataFrame) -> None:\n",
                "    \"\"\"\n",
                "    Analyze the coverage of RAG evaluation examples with consistent formatting.\n",
                "    \n",
                "    Args:\n",
                "        df: DataFrame containing RAG evaluation data\n",
                "    \"\"\"\n",
                "    # Normalize the DataFrame\n",
                "    qa_df = pd.json_normalize(\n",
                "        df.assign(eval_metrics=lambda _df: _df[\"eval_metrics\"].apply(\n",
                "            lambda x: json.loads(x) if isinstance(x, str) else x\n",
                "        )).to_dict(orient=\"records\")\n",
                "    )\n",
                "\n",
                "    console = Console()\n",
                "    categories = ['question_type', 'difficulty', 'reasoning_type']\n",
                "\n",
                "    # Print header\n",
                "    console.print(\"\\n[bold blue]ðŸ“Š RAG Evaluation Report[/bold blue]\", justify=\"center\")\n",
                "    console.print(\"=\" * 80, justify=\"center\")\n",
                "    console.print(f\"\\n[bold]Total Examples:[/bold] {len(qa_df)}\")\n",
                "\n",
                "    # Category distributions\n",
                "    for category in categories:\n",
                "        if category in qa_df.columns:\n",
                "            # Count non-empty values\n",
                "            counts = Counter(x for x in qa_df[category] if pd.notna(x) and x != '')\n",
                "            if not counts:\n",
                "                continue\n",
                "\n",
                "            table = Table(title=f\"\\n{category.title()} Distribution\")\n",
                "            table.add_column(\"Category\", style=\"cyan\")\n",
                "            table.add_column(\"Count\", justify=\"right\")\n",
                "            table.add_column(\"Percentage\", justify=\"right\")\n",
                "\n",
                "            total = sum(counts.values())\n",
                "            for value, count in sorted(counts.items()):\n",
                "                percentage = (count / total) * 100\n",
                "                table.add_row(str(value), str(count), f\"{percentage:.1f}%\")\n",
                "\n",
                "            console.print(table)\n",
                "\n",
                "    # Extract evaluation metrics from the dataframe\n",
                "    if 'eval_metrics' in df.columns:\n",
                "        # Create a table for metrics\n",
                "        metrics_table = Table(title=\"\\nQuality Metrics Summary\")\n",
                "        metrics_table.add_column(\"Metric\")\n",
                "        metrics_table.add_column(\"Average Score\", justify=\"right\")\n",
                "        \n",
                "        # Get all metrics from the first row to understand structure\n",
                "        metrics_dict = df['eval_metrics'][0]\n",
                "        \n",
                "        # Process each metric category\n",
                "        for category, details in metrics_dict.items():\n",
                "            if 'score' in details:\n",
                "                # Convert score to float if it's stored as string\n",
                "                try:\n",
                "                    score_value = float(details['score'])\n",
                "                    metrics_table.add_row(\n",
                "                        category,\n",
                "                        f\"{score_value:.2f}/5.00\"\n",
                "                    )\n",
                "                except (ValueError, TypeError):\n",
                "                    # Handle case where score might not be a number\n",
                "                    metrics_table.add_row(\n",
                "                        category,\n",
                "                        details['score']\n",
                "                    )\n",
                "        \n",
                "        # Print the metrics table\n",
                "        console.print(metrics_table)\n",
                "\n",
                "# Analyze the preview data\n",
                "analyze_rag_coverage(preview.dataset.df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40099da2",
            "metadata": {},
            "source": [
                "## 11. Generate the Full Dataset\n",
                "\n",
                "Now let's generate our full dataset of RAG evaluation pairs, analyze the coverage, and export it to a JSONL file for use in evaluating RAG systems."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb57388d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate the full dataset\n",
                "workflow_run = aidd.create(\n",
                "   num_records=100,\n",
                "   name=\"rag_eval_generation\"\n",
                ")\n",
                "\n",
                "workflow_run.wait_until_done()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac909600",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze and export the generated dataset\n",
                "print(\"\\nGenerated dataset shape:\", workflow_run.dataset.df.shape)\n",
                "\n",
                "# Analyze the full dataset coverage\n",
                "analyze_rag_coverage(workflow_run.dataset.df)\n",
                "\n",
                "# Export the dataset to JSONL format\n",
                "workflow_run.dataset.df.to_json('rag_evals.jsonl', orient='records', lines=True)\n",
                "print(\"\\nDataset exported to rag_evals.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "19c674f4",
            "metadata": {},
            "source": [
                "## 12. Using Your RAG Evaluation Dataset\n",
                "\n",
                "Now that you've generated a diverse RAG evaluation dataset, here are some ways to use it:\n",
                "\n",
                "1. **Benchmarking**: Test your RAG system against these evaluation pairs to measure performance\n",
                "2. **Error Analysis**: Identify patterns in where your RAG system struggles\n",
                "3. **Optimization**: Use insights to tune retrieval and generation parameters\n",
                "4. **Regression Testing**: Track performance over time as you improve your system\n",
                "5. **Model Comparison**: Compare different LLMs, retrievers, or RAG architectures\n",
                "\n",
                "The JSONL file contains structured data with questions, ground truth answers, and quality metrics that you can use with most evaluation frameworks."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3472c6e7",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "base_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
