{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/data-designer/qa-generation/generate-qa-pairs.ipynb\">\n",
                "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
                "</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¨ Data Designer: Generate Diverse Q&A Pairs\n",
                "\n",
                "## Overview\n",
                "\n",
                "This notebook demonstrates a versatile approach for generating high-quality question-answer (Q&A) pairs from any source document (text, markdown, or PDF files). The generated Q&A pairs are valuable for:\n",
                "\n",
                "- **Instruction Tuning:** Training models with clear, self-contained examples\n",
                "- **Retrieval-Augmented Generation (RAG):** Enhancing retrieval systems with precise Q&A pairs\n",
                "- **Search and FAQ Systems:** Powering natural language query systems and documentation\n",
                "- **Educational Content:** Creating quizzes and learning materials\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "- How to process and chunk source documents for Q&A generation\n",
                "- How to configure categorical distributions for controlled diversity\n",
                "- How to generate high-quality Q&A pairs with structured output\n",
                "- How to evaluate the quality of generated pairs with rubric-based scoring\n",
                "- How to analyze and export the complete dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation\n",
                "\n",
                "First, let's install and import the required packages for document processing, text generation, and data handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries\n",
                "!pip install -qq langchain smart_open git+https://github.com/gretelai/gretel-python-client@main \n",
                "!pip install 'unstructured[pdf]'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "\n",
                "Define your source document(s) and the number of Q&A pairs to generate. You can replace this with your own documents in PDF, markdown, or text formats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "# -------------\n",
                "# Define your source document(s) and the number of Q&A pairs to generate.\n",
                "# You can replace this with your own documents in PDF, markdown, or text formats.\n",
                "\n",
                "DOCUMENT_LIST = [\"https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/rag_evals/databricks-state-of-data-ai-report.pdf\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Document Processing\n",
                "\n",
                "The DocumentProcessor class handles loading and chunking source documents for Q&A generation. We use langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Union\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from unstructured.partition.auto import partition\n",
                "from smart_open import open\n",
                "import tempfile\n",
                "import os\n",
                "\n",
                "class DocumentProcessor:\n",
                "    \"\"\"Handles loading and chunking source documents for Q&A generation.\"\"\"\n",
                "    \n",
                "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
                "        \"\"\"Initialize with configurable chunk size and overlap.\"\"\"\n",
                "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
                "            chunk_size=chunk_size,\n",
                "            chunk_overlap=chunk_overlap,\n",
                "            length_function=len,\n",
                "        )\n",
                "\n",
                "    def parse_document(self, uri: str) -> str:\n",
                "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
                "        with open(uri, 'rb') as file:\n",
                "            content = file.read()\n",
                "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
                "                temp_file.write(content)\n",
                "                temp_file.flush()\n",
                "                elements = partition(temp_file.name)\n",
                "\n",
                "        os.unlink(temp_file.name)\n",
                "        return \"\\n\\n\".join([str(element) for element in elements])\n",
                "\n",
                "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
                "        \"\"\"Process one or more documents into chunks for Q&A generation.\"\"\"\n",
                "        if isinstance(uris, str):\n",
                "            uris = [uris]\n",
                "\n",
                "        all_chunks = []\n",
                "        for uri in uris:\n",
                "            text = self.parse_document(uri)\n",
                "            chunks = self.text_splitter.split_text(text)\n",
                "            all_chunks.extend(chunks)\n",
                "\n",
                "        return all_chunks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Models\n",
                "\n",
                "Let's define Pydantic models for structured output generation. These schemas will ensure our generated data has consistent structure and validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "from typing import Optional, Literal\n",
                "\n",
                "class QAPair(BaseModel):\n",
                "    \"\"\"Schema for question-answer pairs\"\"\"\n",
                "    context: str = Field(..., description=\"The context used to make the question and answer.\")\n",
                "    question: str = Field(..., description=\"A clear and concise question derived from the context.\")\n",
                "    answer: str = Field(..., description=\"A detailed and accurate answer fully supported by the context.\")\n",
                "\n",
                "\n",
                "class EvalMetrics(BaseModel):\n",
                "    \"\"\"Schema for scoring generation quality\"\"\"\n",
                "    clarity: int = Field(\n",
                "        ...,\n",
                "        description=\"How clear and understandable is the question? (1=vague/confusing, 5=perfectly clear and well-structured)\",\n",
                "        ge=1,\n",
                "        le=5\n",
                "    )\n",
                "    factual_accuracy: int = Field(\n",
                "        ...,\n",
                "        description=\"Is the answer fully supported by the context? (1=contains errors/unsupported claims, 5=completely accurate and supported)\",\n",
                "        ge=1,\n",
                "        le=5\n",
                "    )\n",
                "    comprehensiveness: int = Field(\n",
                "        ...,\n",
                "        description=\"Does the answer cover all necessary details? (1=missing crucial information, 5=complete coverage of all relevant points)\",\n",
                "        ge=1,\n",
                "        le=5\n",
                "    )\n",
                "    answer_relevance: int = Field(\n",
                "        ...,\n",
                "        description=\"Does the answer directly address what was asked in the question? (1=misaligned/tangential, 5=perfectly addresses the question)\",\n",
                "        ge=1,\n",
                "        le=5\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Processing Documents and Setting Up Data Designer\n",
                "\n",
                "Now we'll process our document chunks and set up the Data Designer with our seed dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from gretel_client.navigator_client import Gretel\n",
                "\n",
                "# Process document chunks\n",
                "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
                "chunks = processor.process_documents(DOCUMENT_LIST)\n",
                "\n",
                "# Create a seed DataFrame with the document chunks\n",
                "seed_df = pd.DataFrame({\"context\": chunks})\n",
                "\n",
                "# Initialize Gretel client and Data Designer\n",
                "# You can use \"prompt\" for API key to be prompted interactively\n",
                "gretel = Gretel(api_key=\"prompt\", endpoint=\"https://api.dev.gretel.ai\")\n",
                "aidd = gretel.data_designer.new(model_suite=\"llama-3.x\")  # or \"apache-2.0\" as needed\n",
                "\n",
                "# Upload the seed dataset with document chunks\n",
                "# Using shuffle with replacement allows the model to reuse context chunks\n",
                "aidd.with_seed_dataset(seed_df, sampling_strategy=\"shuffle\", with_replacement=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Adding Categorical Columns for Controlled Diversity\n",
                "\n",
                "Let's add categorical columns to control the diversity of our Q&A pairs. We'll define different levels of difficulty, question styles, and target audiences to create a rich dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add categorical columns for controlled diversity\n",
                "aidd.add_column(\n",
                "    name=\"difficulty\",\n",
                "    type=\"category\",\n",
                "    params={\"values\": [\"easy\", \"medium\", \"hard\"]}\n",
                ")\n",
                "\n",
                "# Add sophistication as a subcategory of difficulty\n",
                "aidd.add_column(\n",
                "    name=\"sophistication\",\n",
                "    type=\"subcategory\",\n",
                "    params= {\n",
                "        \"category\": \"difficulty\",\n",
                "        \"values\": {\n",
                "            \"easy\": [\n",
                "                \"basic\", \n",
                "                \"straightforward\"\n",
                "            ],\n",
                "            \"medium\": [\n",
                "                \"intermediate\", \n",
                "                \"moderately complex\"\n",
                "            ],\n",
                "            \"hard\": [\n",
                "                \"advanced\", \n",
                "                \"sophisticated\"\n",
                "            ]\n",
                "        }\n",
                "    }\n",
                ")\n",
                "\n",
                "# Add other categorical columns\n",
                "aidd.add_column(\n",
                "    name=\"question_style\",\n",
                "    type=\"category\",\n",
                "    params={\"values\": [\"factual\", \"exploratory\", \"analytical\", \"comparative\"]}\n",
                ")\n",
                "\n",
                "aidd.add_column(\n",
                "    name=\"target_audience\",\n",
                "    type=\"category\",\n",
                "    params={\"values\": [\"novice\", \"intermediate\", \"expert\"]}\n",
                ")\n",
                "\n",
                "aidd.add_column(\n",
                "    name=\"response_format\",\n",
                "    type=\"category\",\n",
                "    params={\"values\": [\"short\", \"detailed\", \"step-by-step\", \"list\"]}\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Adding LLM-Structured Column for Q&A Pair Generation\n",
                "\n",
                "Now let's set up the core of our data generation: the Q&A pair column that will produce structured question-answer pairs based on our document context and control parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add Q&A pair generation column\n",
                "aidd.add_column(\n",
                "    name=\"qa_pair\",\n",
                "    type=\"llm-structured\",\n",
                "    system_prompt=\"\"\"You are an expert at generating high-quality, context-supported Q&A pairs.\n",
                "Your output should be clear, concise, and factually correct.\n",
                "Ensure that every question is self-contained and every answer is comprehensive and derived solely from the provided context.\n",
                "\"\"\",\n",
                "    prompt=\"\"\"\\n{{context}}\\n\\n\n",
                "Based on the above context, generate a high-quality Q&A pair. The question should be clear and concise, \n",
                "and tailored for an audience at the '{{target_audience}}' level. The overall difficulty should be '{{difficulty}}', \n",
                "with a corresponding sophistication level (e.g., {{sophistication}}). The question style should be '{{question_style}}', \n",
                "and the answer should be provided in a '{{response_format}}' format. \n",
                "Ensure that the answer is factually accurate, fully derived from the context, and comprehensive.\\n\n",
                "Put your thoughts within <think>...</think> before providing the JSON.\"\"\",\n",
                "    output_format=QAPair\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Adding Evaluation Metrics with Rubrics\n",
                "\n",
                "To assess the quality of our generated Q&A pairs, we'll add evaluation metrics using detailed rubrics for scoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gretel_client.data_designer.params import Rubric\n",
                "\n",
                "# Define evaluation rubrics\n",
                "clarity_rubric = Rubric(\n",
                "    name=\"Clarity\",\n",
                "    description=\"Evaluates how clear and understandable the response is\",\n",
                "    scoring={\n",
                "        \"5\": \"Exceptionally clear and easy to understand\",\n",
                "        \"4\": \"Very clear with minor ambiguities\",\n",
                "        \"3\": \"Adequately clear but could be improved\",\n",
                "        \"2\": \"Somewhat unclear or confusing\",\n",
                "        \"1\": \"Very unclear or difficult to understand\"\n",
                "    }\n",
                ")\n",
                "\n",
                "factual_accuracy_rubric = Rubric(\n",
                "    name=\"Factual Accuracy\",\n",
                "    description=\"Evaluates the correctness of factual information in the response\",\n",
                "    scoring={\n",
                "        \"5\": \"Completely accurate with no errors\",\n",
                "        \"4\": \"Mostly accurate with minor errors\",\n",
                "        \"3\": \"Somewhat accurate with some errors\",\n",
                "        \"2\": \"Mostly inaccurate with significant errors\",\n",
                "        \"1\": \"Completely inaccurate or misleading\"\n",
                "    }\n",
                ")\n",
                "\n",
                "comprehensiveness_rubric = Rubric(\n",
                "    name=\"Comprehensiveness\",\n",
                "    description=\"Evaluates how thoroughly the response addresses the question\",\n",
                "    scoring={\n",
                "        \"5\": \"Completely comprehensive, covering all aspects\",\n",
                "        \"4\": \"Very comprehensive with minor omissions\",\n",
                "        \"3\": \"Adequately comprehensive but missing some details\",\n",
                "        \"2\": \"Not very comprehensive, missing important aspects\",\n",
                "        \"1\": \"Severely lacking in coverage of necessary information\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_relevance_rubric = Rubric(\n",
                "    name=\"Answer Relevance\",\n",
                "    description=\"Evaluates how relevant the response is to the question asked\",\n",
                "    scoring={\n",
                "        \"5\": \"Perfectly relevant and directly addresses the question\",\n",
                "        \"4\": \"Highly relevant with minor tangential information\",\n",
                "        \"3\": \"Mostly relevant but includes some irrelevant content\",\n",
                "        \"2\": \"Somewhat relevant but misses the main point\",\n",
                "        \"1\": \"Largely irrelevant to the question asked\"\n",
                "    }\n",
                ")\n",
                "\n",
                "EVAL_METRICS_PROMPT_TEMPLATE = \"\"\"\\\n",
                "You are an expert evaluator of question-answer pairs. Analyze the following Q&A pair within the given context and evaluate it objectively.\n",
                "\n",
                "## CONTEXT\n",
                "{{context}}\n",
                "\n",
                "## Q&A PAIR\n",
                "{{qa_pair}}\n",
                "\n",
                "Take a deep breath and carefully evaluate each criterion based on the provided rubrics.\n",
                "\"\"\"\n",
                "\n",
                "# Add evaluation metrics column\n",
                "aidd.add_column(\n",
                "    name=\"eval_metrics\",\n",
                "    type=\"llm-judge\",\n",
                "    prompt=EVAL_METRICS_PROMPT_TEMPLATE,\n",
                "    rubrics=[clarity_rubric, factual_accuracy_rubric, comprehensiveness_rubric, answer_relevance_rubric]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Preview Sample Records\n",
                "\n",
                "Let's generate a preview to see what our data will look like before running the full generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview a Sample of Generated Records\n",
                "preview = aidd.preview()\n",
                "preview.display_sample_record()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's also explore the preview data as a DataFrame to better understand the structure:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the generated preview as a Pandas DataFrame\n",
                "preview.dataset.df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Generate the Full Dataset\n",
                "\n",
                "Now let's generate our full dataset of Q&A pairs:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Submit batch job\n",
                "workflow_run = aidd.create(\n",
                "    num_records=100,\n",
                "    name=\"qa_pair_generation\"\n",
                ")\n",
                "\n",
                "workflow_run.wait_until_done()\n",
                "print(\"\\nGenerated dataset shape:\", workflow_run.dataset.df.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Analyze and Export the Dataset\n",
                "\n",
                "Let's examine the generated Q&A pairs:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inspect the generated dataset\n",
                "workflow_run.dataset.df.head(10)\n",
                "\n",
                "# Export the dataset to JSONL format\n",
                "workflow_run.dataset.df.to_json('qa_pairs.jsonl', orient='records', lines=True)\n",
                "print(\"\\nDataset exported to qa_pairs.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Using Your Q&A Pairs Dataset\n",
                "\n",
                "Now that you've generated a diverse Q&A dataset, here are some ways to use it:\n",
                "\n",
                "1. **Model Training:** Use these pairs to fine-tune language models for specific domains\n",
                "2. **Knowledge Base:** Create a searchable Q&A knowledge base for your documentation\n",
                "3. **Testing:** Evaluate how well your models or search systems handle different types of questions\n",
                "4. **Content Creation:** Generate FAQs, quizzes, or educational content\n",
                "5. **Chatbot Development:** Provide a foundation for chatbot responses in your domain\n",
                "\n",
                "The JSONL file contains structured data with questions, answers, and quality metrics that you can use across various applications."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "base_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
