{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/classify_freetext_with_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIBl7LPg0Zzc"
   },
   "source": [
    "# Using Gretel Classify to Label Free Text\n",
    "\n",
    "In this blueprint, we analyze and label a set of Yelp reviews looking for PII and other potentially sensitive information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zlWDeUZ0Zzd"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First we install our python dependencies and configure the Gretel client.\n",
    "\n",
    "_Note: we install spacy for their visualization helper, displacy_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmcTAKie0Zze"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq gretel-client spacy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DuZ3OP-0Zzf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from gretel_client import poll, configure_session\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "dataset_file_path = \"reviews.csv\"\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDNRpc-l0Zzf"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "Using Hugging Face's [datasets](https://github.com/huggingface/datasets) library, we load a dataset containing a dump of [Yelp reviews](https://huggingface.co/datasets/yelp_review_full). This data contains unstructured review text that we pass through a NER pipeline for labeling and PII discovery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw1QMDr40Zzg"
   },
   "outputs": [],
   "source": [
    "source_dataset = datasets.load_dataset(\"yelp_review_full\")\n",
    "source_df = pd.DataFrame(source_dataset[\"train\"]).sample(n=300, random_state=99)\n",
    "source_df.to_csv(dataset_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ywqtTxf0Zzh"
   },
   "outputs": [],
   "source": [
    "source_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoClMb1E0Zzh"
   },
   "source": [
    "## Configure a Gretel Project and Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VI-uVY70Zzi"
   },
   "outputs": [],
   "source": [
    "project = create_or_get_unique_project(name=\"Gretel NLP Yelp Reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0l3tqQX0Zzi"
   },
   "outputs": [],
   "source": [
    "# Passing `use_nlp: true` into the model config,\n",
    "# enables additional predictions using NLP models.\n",
    "classify_config = \"\"\"\n",
    "schema_version: \"1.0\"\n",
    "models:\n",
    "  - classify:\n",
    "      data_source: \"_\"\n",
    "      use_nlp: true\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pvl7qbr0Zzj"
   },
   "source": [
    "If you wish to transform the dataset instead, you may pass the same `use_nlp: true` property into a transformation pipeline. For an example of a transform pipeline, see the [Redact PII Notebook](https://github.com/gretelai/gretel-blueprints/blob/main/docs/notebooks/redact_pii.ipynb). Below is an example that uses nlp.\n",
    "\n",
    "```yaml\n",
    "schema_version: \"1.0\"\n",
    "models:\n",
    "  - transforms:\n",
    "      data_source: \"_\"\n",
    "      use_nlp: true\n",
    "      policies:\n",
    "        - name: remove_pii\n",
    "          rules:\n",
    "            - name: redact_pii\n",
    "              conditions:\n",
    "                value_label:\n",
    "                  - person_name\n",
    "                  - location\n",
    "                  - credit_card_number\n",
    "                  - phone_number\n",
    "                  - email_address\n",
    "              transforms:\n",
    "                - type: fake\n",
    "                - type: redact_with_char\n",
    "                  attrs:\n",
    "                    char: X\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1LAysyo0Zzj"
   },
   "source": [
    "### Create the Classification Model\n",
    "\n",
    "This next cell will create the classification model. After we verify the model is working correctly, the the entire dataset will be passed into the model for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0jjxsWu0Zzk"
   },
   "outputs": [],
   "source": [
    "model = project.create_model_obj(yaml.safe_load(classify_config), dataset_file_path)\n",
    "model.submit_cloud()\n",
    "poll(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By5EcgYP0Zzk"
   },
   "source": [
    "Using the created model, we download the report to get a summary view of found entities. This report is based on a sample of the original dataset, and is used to ensure the model has been configured correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D76WDvpM0Zzk"
   },
   "outputs": [],
   "source": [
    "# `report_json` contains a summary of entities by field\n",
    "with open(model.get_artifact_link(\"report_json\")) as fh:\n",
    "    report = json.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25FPWSd40Zzl"
   },
   "outputs": [],
   "source": [
    "# By converting these summaries into a dataframe we can quickly view\n",
    "# entities found by the model.\n",
    "summary = []\n",
    "for field in report[\"metadata\"][\"fields\"]:\n",
    "    row = {\"name\": field[\"name\"]}\n",
    "    for entity in field[\"entities\"]:\n",
    "        row[entity[\"label\"]] = entity[\"count\"]\n",
    "    summary.append(row)\n",
    "\n",
    "pd.DataFrame(summary).set_index(\"name\").fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otkUQlrf0Zzl"
   },
   "source": [
    "### Classify the reviews\n",
    "\n",
    "Now that the model has been configured and verified, let's run the full dataset through the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQz1bRNc0Zzl"
   },
   "outputs": [],
   "source": [
    "records = model.create_record_handler_obj(data_source=dataset_file_path)\n",
    "records.submit_cloud()\n",
    "poll(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dJdAA2C0Zzm"
   },
   "outputs": [],
   "source": [
    "# the `data` artifact returns a JSONL formatted file containing\n",
    "# entity predictions by row.\n",
    "with open(records.get_artifact_link(\"data\")) as fh:\n",
    "    records = [json.loads(line) for line in fh]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AN2mQqyW0Zzm"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "for row, entities in zip(source_df.values, records):\n",
    "    label, text = row\n",
    "\n",
    "    colors = {}\n",
    "    palette = [\n",
    "        \"#7aecec\",\n",
    "        \"#bfeeb7\",\n",
    "        \"#feca74\",\n",
    "        \"#ff9561\",\n",
    "        \"#aa9cfc\",\n",
    "        \"#c887fb\",\n",
    "        \"#9cc9cc\",\n",
    "        \"#ffeb80\",\n",
    "        \"#ff8197\",\n",
    "        \"#ff8197\",\n",
    "        \"#f0d0ff\",\n",
    "        \"#bfe1d9\",\n",
    "        \"#e4e7d2\",\n",
    "    ]\n",
    "\n",
    "    for index, label in enumerate([x[\"label\"] for x in entities[\"entities\"]]):\n",
    "        colors[label.upper()] = palette[index % len(palette)]\n",
    "\n",
    "    options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "\n",
    "    displacy.render(\n",
    "        {\n",
    "            \"text\": text,\n",
    "            \"ents\": [e for e in entities[\"entities\"] if e[\"field\"] == \"text\"],\n",
    "        },\n",
    "        style=\"ent\",\n",
    "        jupyter=True,\n",
    "        manual=True,\n",
    "        options=options,\n",
    "    )\n",
    "    input(\"\\nPress [enter] to see the next review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIraRtkI0Zzm"
   },
   "outputs": [],
   "source": [
    "# now that you've run the notebook, you can also view the same\n",
    "# project using our web console.\n",
    "project.get_console_url()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "classify_freetext_with_nlp.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c75c16813cbc397c72b905dbcc7225ac3e9db4701ee07b7887dce2bbda1e49e9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
