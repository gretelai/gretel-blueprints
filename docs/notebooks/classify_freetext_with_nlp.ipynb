{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/classify_freetext_with_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gretel Classify to Label Free Text\n",
    "\n",
    "In this blueprint, we analyze and label a set of email dumps looking for PII and other potentially sensitive information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we install our python dependencies and configure the Gretel client.\n",
    "\n",
    "_Note: we install spacy for their visualization helper, displacy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq gretel-client spacy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from getpass import getpass\n",
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from smart_open import open\n",
    "from gretel_client import create_project, poll, configure_session, ClientConfig\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "dataset_file_path = \"emails.csv\"\n",
    "\n",
    "configure_session(\n",
    "    ClientConfig(\n",
    "        api_key=getpass(prompt=\"Enter Gretel API key\"),\n",
    "        endpoint=\"https://api.gretel.cloud\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Using Hugging Face's [datasets](https://github.com/huggingface/datasets) library, we load a dataset containing a dump of [Enron emails](https://huggingface.co/datasets/aeslc). This data contains unstructured emails that we pass through a NER pipeline for labeling and PII discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = datasets.load_dataset(\"aeslc\")\n",
    "source_df = pd.DataFrame(source_dataset[\"train\"]).sample(n=10000, random_state=99)\n",
    "source_df.to_csv(dataset_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a Gretel Project and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = create_project(display_name=\"Gretel NLP Blueprint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing `use_nlp: true` into the model config,\n",
    "# enables additional predictions using NLP models.\n",
    "classify_config = \"\"\"\n",
    "schema_version: \"1.0\"\n",
    "models:\n",
    "  - classify:\n",
    "      data_source: \"_\"\n",
    "      use_nlp: true\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to transform, instead of classify the dataset, you may pass the same `use_nlp: true` property into a transformation pipeline. For an example transform pipeline, see the [Redact PII Notebook](https://github.com/gretelai/gretel-blueprints/blob/main/docs/notebooks/redact_pii.ipynb). Below is an example transform pipeline that uses nlp.\n",
    "\n",
    "```yaml\n",
    "schema_version: \"1.0\"\n",
    "models:\n",
    "  - transforms:\n",
    "      data_source: \"_\"\n",
    "      use_nlp: true\n",
    "      policies:\n",
    "        - name: remove_pii\n",
    "          rules:\n",
    "            - name: redact_pii\n",
    "              conditions: \n",
    "                value_label:\n",
    "                  - person_name\n",
    "                  - location\n",
    "                  - credit_card_number\n",
    "                  - phone_number\n",
    "                  - email_address\n",
    "              transforms:\n",
    "                - type: fake\n",
    "                - type: redact_with_char\n",
    "                  attrs:\n",
    "                    char: X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Classification Model\n",
    "\n",
    "This next cell will create the classification model. After we verify the model is working correctly, the the entire dataset will be passed into the model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = project.create_model_obj(yaml.safe_load(classify_config), dataset_file_path)\n",
    "model.submit_cloud()\n",
    "poll(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the created model, we download the report to get a summary view of found entities. This report is based on a sample of the original dataset, and is used to ensure the model has been configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `report_json` contains a summary of entities by field\n",
    "with open(model.get_artifact_link(\"report_json\")) as fh:\n",
    "    report = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By converting these summaries into a dataframe we can quickly view\n",
    "# entities found by the model.\n",
    "summary = []\n",
    "for field in report[\"metadata\"][\"fields\"]:\n",
    "    row = {\"name\": field[\"name\"]}\n",
    "    for entity in field[\"entities\"]:\n",
    "        row[entity[\"label\"]] = entity[\"count\"]\n",
    "    summary.append(row)\n",
    "\n",
    "pd.DataFrame(summary).set_index(\"name\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the emails\n",
    "\n",
    "Now that the model has been configured and verified, let's run the full dataset through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = model.create_record_handler_obj(data_source=dataset_file_path)\n",
    "records.submit_cloud()\n",
    "poll(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `data` artifact returns a JSONL formatted file containing\n",
    "# entity predictions by row.\n",
    "with open(records.get_artifact_link(\"data\")) as fh:\n",
    "    records = [json.loads(line) for line in fh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "for row, entities in zip(source_df.values, records):\n",
    "    email_body, subject = row\n",
    "    displacy.render(\n",
    "        {\n",
    "            \"title\": f\"Subject: {subject}\",\n",
    "            \"text\": email_body,\n",
    "            \"ents\": [e for e in entities[\"entities\"] if e[\"field\"] == \"email_body\"]\n",
    "        },\n",
    "        style=\"ent\",\n",
    "        jupyter=True,\n",
    "        manual=True\n",
    "    )\n",
    "    input(\"\\nPress [enter] to see the next email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that you've ran the notebook, you can also view the same\n",
    "# project using our web console.\n",
    "project.get_console_url()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c75c16813cbc397c72b905dbcc7225ac3e9db4701ee07b7887dce2bbda1e49e9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('blueprints': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
