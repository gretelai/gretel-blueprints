{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vXfYHX6QSJu"
   },
   "source": [
    "# Synthesizing dialogs for better conversational AI\n",
    "\n",
    "* This notebook demonstrates how to use Gretel GPT to generate synthetic conversations.\n",
    "* To run this notebook, you will need an API key from the [Gretel Console](https://console.gretel.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhwZL2atTilv"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_iIkqnUQK2l"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kixD67x_TSC4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from gretel_client import configure_session\n",
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects import create_or_get_unique_project, get_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a Gretel session\n",
    "\n",
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(\n",
    "    api_key=\"prompt\",\n",
    "    endpoint=\"https://api.gretel.cloud\",\n",
    "    validate=True,\n",
    "    clear=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pMwi0RghUzh"
   },
   "source": [
    "## Load and preview training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set DATASET to either [commonsense, counselchat, dailydialog]\n",
    "DATASET = \"dailydialog\"\n",
    "MAX_NUMBER_RECORDS = 1000\n",
    "SPLIT = \"train\"\n",
    "\n",
    "datasets = {\n",
    "    \"commonsense\": \"mvansegb123/commonsense-dialogues\",\n",
    "    \"counselchat\": \"nbertagnolli/counsel-chat\",\n",
    "    \"dailydialog\": \"daily_dialog\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(datasets[DATASET])\n",
    "\n",
    "df = pd.DataFrame(dataset[SPLIT])\n",
    "\n",
    "display(df.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions\n",
    "These functions convert the structured source data into the Gretel-GPT desired format and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from structure to paragraph\n",
    "def process_text_data(df, dataset):\n",
    "\n",
    "    text_data = []\n",
    "\n",
    "    if dataset == \"commonsense\":\n",
    "        for index, row in df.iterrows():\n",
    "            text_string = f\"The context of the following conversation is {row['context']}:  \"\n",
    "            speakers = [row['speaker'], \"The friend\"] * int(len(row['turns'])/2+1)\n",
    "            turns = row['turns']\n",
    "            for k in range(len(turns)):\n",
    "                if k == 0:\n",
    "                    text_string += f\"{speakers[k]} says {turns[k]}  \"\n",
    "                else:\n",
    "                    text_string += f\"{speakers[k]} responds {turns[k]}  \"\n",
    "            text_data += [text_string]\n",
    "        \n",
    "    if dataset == \"counselchat\":\n",
    "        for index, row in df.iterrows():\n",
    "            if row[\"questionText\"] and row[\"answerText\"]:\n",
    "                question_topic = f\"Within the topic of {row['topic']}\"\n",
    "                question_title = f\"the following patient-therapist question is in the theme of \\\"{row['questionTitle']}\\\"\"\n",
    "                patient_question = row[\"questionText\"].replace(\"\\n\",\"\")\n",
    "                therapist_response = row[\"answerText\"].replace(\"\\n\",\"\")\n",
    "                text_string = f\"{question_topic}, {question_title}:  The patient asks \\\"{patient_question}\\\".  The therapist responds \\\"{therapist_response}\\\".\"\n",
    "                text_data += [text_string]\n",
    "\n",
    "    if dataset == \"dailydialog\":\n",
    "        \n",
    "        MIN_EMOTIONS_IN_DIALOG = 3\n",
    "        ACTS = {0: \"says\", 1: \"informs\", 2: \"questions\", 3: \"says in a directive tone\", 4: \"says in a commissive manner\"}\n",
    "        EMOTION = {0: None, 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            text_string = \"\"\n",
    "            if np.count_nonzero(row[\"emotion\"]) >= MIN_EMOTIONS_IN_DIALOG:\n",
    "                for k in range(len(row[\"dialog\"])):\n",
    "                    act = ACTS[row[\"act\"][k]]\n",
    "                    emotion = EMOTION[row[\"emotion\"][k]]\n",
    "                    if emotion:\n",
    "                        emotion_str = f\"while revealing the emotion of {emotion}\"\n",
    "                    else:\n",
    "                        emotion_str = f\"while revealing no emotion\"\n",
    "                    sentence = row[\"dialog\"][k]\n",
    "                    text_string += f\"Person {k%2+1} {act} \\\"{sentence}\\\" {emotion_str}.   \"\n",
    "                text_data += [text_string]\n",
    "\n",
    "    data_source = pd.DataFrame(text_data, columns=['text'])\n",
    "    \n",
    "    return data_source\n",
    "\n",
    "# convert from paragrap to structure\n",
    "def convert_to_struct(df, dataset):\n",
    "\n",
    "    synth_df = None\n",
    "\n",
    "    if dataset == \"commonsense\":\n",
    "        speakers = []\n",
    "        context = []\n",
    "        turns = []\n",
    "        for index, row in df.iterrows():\n",
    "            context_text, turns_text = row['text'].split(\":\")\n",
    "            context_text = context_text.replace(f\"The context of the following conversation is \",\"\")\n",
    "            turns_text = turns_text.split(\".  \")\n",
    "            turns_sentences = []\n",
    "            for k in range(len(turns_text)):\n",
    "                sentence = turns_text[k].strip()\n",
    "                if k == 0:\n",
    "                    speaker = sentence.split(\" \")[0]\n",
    "                    speakers += [speaker]\n",
    "                sentence = sentence.replace(f\"{speaker} says \",\"\").replace(f\"{speaker} responds \",\"\").replace(f\"The friend responds \",\"\")\n",
    "                if len(sentence) > 0:\n",
    "                    turns_sentences += [sentence]\n",
    "            turns += [turns_sentences]\n",
    "            context += [context_text]\n",
    "\n",
    "        synth_df = pd.DataFrame([context, speakers, turns]).T\n",
    "        synth_df.columns = [\"context\", \"speaker\", \"turns\"]\n",
    "        synth_df.T.to_json(f'synth_{dataset}.json', indent=4, ensure_ascii=False)\n",
    "\n",
    "    if dataset == \"counselchat\":\n",
    "        \n",
    "        topic = []\n",
    "        questionTitle = []\n",
    "        questionText = []\n",
    "        answerText = []\n",
    "        for index, row in df.iterrows():\n",
    "            row['text'] = row['text'].replace(\".  The therapist responds with:\",\".  The therapist responds with \")\n",
    "            row['text'] = row['text'].replace(\"Within the topic of \",\"\")\n",
    "            row['text'] = row['text'].replace(\", the following patient-therapist question is in the theme of \",\"|\")\n",
    "            row['text'] = row['text'].replace(\":  The patient asks \",\"|\")\n",
    "            row['text'] = row['text'].replace(\".  The therapist responds\",\"|\")\n",
    "            row['text'] = row['text'].replace(\"\\\"\",\"\")\n",
    "\n",
    "            # validate if we have all\n",
    "            fields = row['text'].split(\"|\")\n",
    "            if len(fields) == 4:\n",
    "                question_topic, question_title, question_text, answer_text = row['text'].split(\"|\")\n",
    "                topic += [question_topic]\n",
    "                questionTitle += [question_title]\n",
    "                questionText += [question_text]\n",
    "                answerText += [answer_text]\n",
    "\n",
    "        \n",
    "        synth_df = pd.DataFrame([questionTitle, questionText, topic, answerText]).T\n",
    "        synth_df.columns = [\"questionTitle\", \"questionText\", \"topic\", \"answerText\"]\n",
    "        synth_df.to_csv(f'synth_{dataset}.csv', index=None)\n",
    "\n",
    "    if dataset == \"dailydialog\":\n",
    "\n",
    "        ACTS = {0: \"says\", 1: \"informs\", 2: \"questions\", 3: \"says in a directive tone\", 4: \"says in a commissive manner\"}\n",
    "        EMOTION = {0: None, 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
    "\n",
    "        dialog = []\n",
    "        act = []\n",
    "        emotion = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            dialog_text = row['text'].split(\".  \")\n",
    "            dialog_sentences = []\n",
    "            act_sentences = []\n",
    "            emotion_sentences = []\n",
    "            for k in range(len(dialog_text)):\n",
    "                sentence = dialog_text[k].strip()\n",
    "                if len(sentence) > 0:\n",
    "                    prefix = sentence.split(\" \\\"\")[0]\n",
    "                    suffix = sentence.split(\"\\\" while revealing \")[1]\n",
    "                    prefix_parsed = prefix.replace(\"Person 1\",\"\").replace(\"Person 2\",\"\").strip()\n",
    "                    for key, val in ACTS.items():\n",
    "                        if val in prefix_parsed:\n",
    "                            act_sentences += [key]\n",
    "                    for key, val in EMOTION.items():\n",
    "                        if not val and suffix==\"no emotion\":\n",
    "                            emotion_sentences += [key]\n",
    "                        if val and val in suffix:\n",
    "                            emotion_sentences += [key]\n",
    "                    sentence = sentence.split(\"\\\"\")[1].strip() + \" \"\n",
    "                    dialog_sentences += [sentence]\n",
    "\n",
    "            dialog += [dialog_sentences]\n",
    "            act += [act_sentences]\n",
    "            emotion += [emotion_sentences]\n",
    "        \n",
    "        synth_df = pd.DataFrame([dialog, act, emotion]).T\n",
    "        synth_df.columns = [\"dialog\", \"act\", \"emotion\"]\n",
    "        synth_df.T.to_json(f'synth_{dataset}.json', indent=4)\n",
    "    \n",
    "    display(synth_df.head())\n",
    "\n",
    "# Helper functions for the Gretel-GPT config\n",
    "def calc_steps(num_rows, batch_size, minutes=60) -> float:\n",
    "    \"\"\"Estimate the number of rows that can be trained within a time period\"\"\"\n",
    "    rows_per_minute = 102.0\n",
    "    epochs = (rows_per_minute * minutes) / num_rows\n",
    "    return int(epochs * num_rows / batch_size)\n",
    "\n",
    "def calc_text_length(df, max_tokens=2048) -> float:\n",
    "    tokens_per_word = 3\n",
    "    max_string_length = int(df.str.len().max()/tokens_per_word)\n",
    "    return min(int(np.ceil(max_string_length/100)*100), max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = process_text_data(df, dataset=DATASET)\n",
    "data_source = data_source.sample(n=MAX_NUMBER_RECORDS, ignore_index=True)\n",
    "display(data_source.head(n=5))\n",
    "\n",
    "MAX_STRING_LENGTH = data_source['text'].str.len().max()\n",
    "AVG_STRING_LENGTH = data_source['text'].str.len().mean()\n",
    "print(f\"Nb records in training data: {len(data_source)}\")\n",
    "print(f\"Average string length: {AVG_STRING_LENGTH:.0f}\")\n",
    "print(f\"Maximum string length: {MAX_STRING_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Gretel-GPT Model\n",
    "\n",
    "In this example, we will finetune Gretel GPT to generate synthetic dialogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpjRvCmjU5qG"
   },
   "outputs": [],
   "source": [
    "from gretel_client.projects.models import read_model_config\n",
    "\n",
    "config = read_model_config(\"synthetics/natural-language\")\n",
    "config['models'][0]['gpt_x']['pretrained_model'] = \"gretelai/mpt-7b\"\n",
    "config['models'][0]['gpt_x']['steps'] = calc_steps(len(data_source), config['models'][0]['gpt_x']['batch_size'])\n",
    "config['models'][0]['gpt_x']['generate'] = {\n",
    "    'num_records': 3, \n",
    "    'num_beams': 5,\n",
    "    'maximum_text_length': calc_text_length(data_source[\"text\"], MAX_STRING_LENGTH), \n",
    "    }\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh4-8dddoTWb"
   },
   "source": [
    "## Train the synthetic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsOXO4YPoSA1"
   },
   "outputs": [],
   "source": [
    "# Create project\n",
    "GRETEL_PROJECT = f'project-{DATASET}'\n",
    "project = create_or_get_unique_project(name=GRETEL_PROJECT)\n",
    "\n",
    "# Create and submit model\n",
    "model = project.create_model_obj(model_config=config, data_source=data_source)\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"maximum_text_length\": calc_text_length(data_source[\"text\"]), \n",
    "    \"top_p\": 0.95, \n",
    "    \"num_records\": 100, \n",
    "    \"num_beams\": 5\n",
    "    }\n",
    "\n",
    "record_handler = model.create_record_handler_obj(params = params)\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the synthetic data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output = pd.read_csv(record_handler.get_artifact_link(\"data\"), compression='gzip')\n",
    "display(gpt_output.head())\n",
    "convert_to_struct(gpt_output, dataset=DATASET)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/gretelai/gretel-blueprints/blob/main/docs/notebooks/generate_taylor_swift_lyrics_with_gpt.ipynb",
     "timestamp": 1691626155661
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
