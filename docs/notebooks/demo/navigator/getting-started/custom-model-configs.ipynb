{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a759e70",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/navigator/getting-started/custom-model-configs.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm0zecvop88n"
   },
   "source": [
    "# ðŸŽ¨ Navigator Data Designer SDK: Using Custom Model Configurations\n",
    "\n",
    "This notebook demonstrates how to use custom model configurations with Data Designer. We'll show how to:\n",
    "\n",
    "1. Set up custom model configurations with different parameters\n",
    "2. Create a data designer with these custom models\n",
    "3. Define various column types (samplers, expressions, LLM-generated)\n",
    "4. Preview and generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2-9-lPup--8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the latest version of Gretel client and dependencies\n",
    "%pip install -U gretel_client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw7gB2nep88o",
    "outputId": "90b56d21-2f77-472b-f596-4bc0e3a2fd21"
   },
   "source": [
    "## Setup and Initialization\n",
    "\n",
    "Import the necessary libraries and initialize the Gretel client. We're using:\n",
    "- `ModelConfig` and `GenerationParameters` for configuring custom models\n",
    "- Column types (`C`) for defining data structure\n",
    "- Parameter types (`P`) for configuring column behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnX4D3hzt7f0"
   },
   "outputs": [],
   "source": [
    "from gretel_client.navigator_client import Gretel\n",
    "from gretel_client.workflows.configs.workflows import ModelConfig, GenerationParameters\n",
    "\n",
    "# We have a new way to build with concrete types.\n",
    "from gretel_client.data_designer import columns as C\n",
    "from gretel_client.data_designer import params as P\n",
    "\n",
    "# Initialize the Gretel client\n",
    "# Note: In a production environment, you would use your actual API key\n",
    "gretel = Gretel(api_key=\"prompt\", endpoint=\"https://api.dev.gretel.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "1ykd5Fkkp88p",
    "outputId": "616cfa43-d0d8-452b-9967-f718cb57fc59"
   },
   "source": [
    "## Custom Model Configurations\n",
    "\n",
    "In this section, we define two custom model configurations:\n",
    "\n",
    "1. `mistral-small-static-higher-temp` - Uses a fixed temperature (0.75) for more diverse outputs\n",
    "2. `mistral-small-variable-higher-temp` - Uses a variable temperature range (0.50 to 0.90) for each generation\n",
    "\n",
    "These configurations allow us to control the creativity and variability of the LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "sdD4GaJ6p88q",
    "outputId": "26e22ccf-a9e8-4733-e344-27e919fb35d2"
   },
   "outputs": [],
   "source": [
    "# Define custom model configurations\n",
    "model_configs = [\n",
    "    # Configuration with static temperature\n",
    "    ModelConfig(alias=\"mistral-small-static-higher-temp\",\n",
    "                model_name=\"gretel/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "                generation_parameters=GenerationParameters(temperature=0.75, top_p=0.9)),\n",
    "    \n",
    "    # Configuration with variable temperature (uniform distribution)\n",
    "    ModelConfig(alias=\"mistral-small-variable-higher-temp\",\n",
    "                model_name=\"gretel/mistralai/Mistral-Small-24B-Instruct-2501\",\n",
    "                generation_parameters=GenerationParameters(\n",
    "                    temperature={\"type\": \"uniform\", \"params\": {\"low\": 0.50, \"high\": 0.90}},\n",
    "                    top_p=0.9\n",
    "                ))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9glkmBXAp88q",
    "outputId": "8257beaa-4778-474d-c583-f938bf1b5050"
   },
   "source": [
    "## Initialize Data Designer with Custom Models\n",
    "\n",
    "Create a new data designer instance with our custom model configurations. We're using the \"apache-2.0\" model suite, which provides models that can be used under the Apache 2.0 license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data designer with our custom model configurations\n",
    "aidd = gretel.data_designer.new(model_suite=\"apache-2.0\",\n",
    "                                model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Person Samplers\n",
    "\n",
    "Person samplers are pre-configured generators for realistic person data. Here we define two samplers:\n",
    "1. `person1` - A male person located in San Francisco\n",
    "2. `person2` - A female person with default location\n",
    "\n",
    "These samplers can generate a wide range of personal attributes as shown in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidd.with_person_samplers({\n",
    "    \"person1\": {\"sex\": \"Male\"},\n",
    "    \"person2\": {\"sex\": \"Female\"},\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Category Samplers\n",
    "\n",
    "Next, we'll add a category sampler for pet types with weighted probabilities:\n",
    "- dog: 50% probability\n",
    "- cat: 30% probability\n",
    "- fish: 20% probability\n",
    "\n",
    "We also define a conditional parameter that will return \"none\" if the number of pets is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pet_type column with conditional logic\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"pet_type\",\n",
    "        type=P.SamplingSourceType.CATEGORY,\n",
    "        params=P.CategorySamplerParams(values=[\"dog\", \"cat\", \"fish\"], weights=[0.5, 0.3, 0.2]),\n",
    "        conditional_params={\n",
    "            \"number_of_pets == 0\": P.CategorySamplerParams(values=[\"none\"])\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Subcategory Samplers\n",
    "\n",
    "Subcategory samplers allow us to select values based on another column's value. Here, we'll create a pet name sampler that depends on the pet type:\n",
    "- Different name options for each pet type (dog, cat, fish)\n",
    "- \"n/a\" for those with no pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add first_pet_name column that depends on pet_type\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"first_pet_name\",\n",
    "        type=P.SamplingSourceType.SUBCATEGORY,\n",
    "        params=P.SubcategoryParams(\n",
    "            category=\"pet_type\",\n",
    "            values={\n",
    "                \"dog\": [\"Buddy\", \"Max\", \"Charlie\", \"Cooper\", \"Daisy\", \"Lucy\"],\n",
    "                \"cat\": [\"Oliver\", \"Leo\", \"Milo\", \"Charlie\", \"Simba\", \"Luna\"],\n",
    "                \"fish\": [\"Bubbles\", \"Nemo\", \"Goldie\", \"Dory\", \"Finley\", \"Splash\"],\n",
    "                \"none\": [\"n/a\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Statistical Samplers\n",
    "\n",
    "Here we add a Poisson sampler for the number of pets. A Poisson distribution is good for modeling count data, where we expect a certain average number (in this case, 2 pets on average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number_of_pets column using Poisson distribution\n",
    "aidd.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"number_of_pets\",\n",
    "        type=P.SamplingSourceType.POISSON,\n",
    "        params=P.PoissonSamplerParams(mean=2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Expression Columns\n",
    "\n",
    "Expression columns allow us to create new columns based on expressions involving other columns. Here we:\n",
    "1. Calculate the number of children based on the number of pets\n",
    "2. Create full name columns for both person samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number_of_children column based on number_of_pets\n",
    "aidd.add_column(\n",
    "    C.ExpressionColumn(\n",
    "        name=\"number_of_children\",\n",
    "        expr=\"{% if number_of_pets > 0 %}{{ 2 * number_of_pets - 1}}{% else %}0{% endif %}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add full name columns for both person samplers\n",
    "aidd.add_column(\n",
    "    C.ExpressionColumn(\n",
    "        name=\"person1_full_name\",\n",
    "        expr=\"{{ person1.first_name }} {{ person1.last_name }}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "aidd.add_column(\n",
    "    C.ExpressionColumn(\n",
    "        name=\"person2_full_name\",\n",
    "        expr=\"{{ person2.first_name }} {{ person2.last_name }}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding LLM-Generated Columns\n",
    "\n",
    "Finally, we'll add columns that use our custom model configurations to generate text. We're creating:\n",
    "\n",
    "1. `first_pet_backstory` - A backstory for the couple's first pet using the static temperature model\n",
    "2. `couple_backstory` - A narrative of how the couple met using the variable temperature model\n",
    "\n",
    "Notice how we use Jinja templating to conditionally format the prompts and incorporate values from other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add first_pet_backstory column using static temperature model\n",
    "aidd.add_column(\n",
    "    C.LLMTextColumn(\n",
    "        name=\"first_pet_backstory\",\n",
    "        prompt=(\n",
    "            \"{% if number_of_pets > 0 %}\"\n",
    "            \"Write a sweet backstory for {{ person1.first_name }} and \"\n",
    "            \"{{ person2.first_name }}'s first pet {{ pet_type }} named {{ first_pet_name }}. \"\n",
    "            \"Keep it concise, no more than 8 sentences.\"\n",
    "            \"{% else %}\"\n",
    "            \"Repeat exactly these words: 'They had no pets.'\"\n",
    "            \"{% endif %}\"\n",
    "        ),\n",
    "        model_alias=\"mistral-small-static-higher-temp\",  # Using our custom model with static temperature\n",
    "    )\n",
    ")\n",
    "# Add couple_backstory column using variable temperature model\n",
    "aidd.add_column(\n",
    "    C.LLMTextColumn(\n",
    "        name=\"couple_backstory\",\n",
    "        prompt=(\n",
    "            \"Write a thoughtful, funny backstory for how {{ person1_full_name }} and {{ person2_full_name }} met. \"\n",
    "            \"{% if number_of_pets > 0 %}\"\n",
    "            \"Make sure to include how they decided to get a pet together, ultimately leading to {{ number_of_pets }} pets. \"\n",
    "            \"Note their first pet was named {{ first_pet_name }}, with the following backstory:\\n\\n{{ first_pet_backstory }}\"\n",
    "            \"{% else %}\"\n",
    "            \"Make sure to include how they decided to not get a pet together.\"\n",
    "            \"{% endif %}\"\n",
    "        ),\n",
    "        model_alias=\"mistral-small-variable-higher-temp\",  # Using our custom model with variable temperature\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Generated Data\n",
    "\n",
    "Now that we've configured all our columns, let's preview a sample record to see how our data will look. The `verbose_logging` parameter will show detailed information about the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a preview with verbose logging\n",
    "preview = aidd.preview(verbose_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample record\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Full Dataset\n",
    "\n",
    "Finally, we can generate a full dataset with our configured columns. Here we'll create 100 records and assign a workflow run name to help identify this run later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a full dataset of 100 records\n",
    "aidd.create(num_records=100, name=\"custom-model-config-demo\", wait_until_done=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Create custom model configurations with different temperature settings\n",
    "2. Initialize a data designer with these custom models\n",
    "3. Configure various column types:\n",
    "   - Person samplers for realistic personal data\n",
    "   - Category and subcategory samplers with conditional logic\n",
    "   - Statistical samplers (Poisson)\n",
    "   - Expression columns to derive new values\n",
    "   - LLM-generated text columns using our custom models\n",
    "\n",
    "These techniques allow you to create rich, diverse synthetic datasets with fine-grained control over the generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Person Samplers\n",
    "\n",
    "Person samplers are pre-configured generators for realistic person data. Here we define two samplers:\n",
    "1. `dude` - A male person located in San Francisco\n",
    "2. `lady` - A female person with default location\n",
    "\n",
    "These samplers can generate a wide range of personal attributes as shown in the table below."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
