{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6e8f02ab",
            "metadata": {},
            "source": [
                "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/navigator/qa-generation/product-question-answer-generator.ipynb\">\n",
                "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
                "</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38ebcf4d",
            "metadata": {
                "id": "vIH9DEmYimTb"
            },
            "source": [
                "# ðŸŽ¨ Data Designer: Product Information Dataset Generator with Q&A\n",
                "\n",
                "This notebook demonstrates how to use Gretel's Data Designer to create a synthetic dataset of product information with corresponding questions and answers. This dataset can be used for training and evaluating Q&A systems focused on product information.\n",
                "\n",
                "The generator creates:\n",
                "- Product details (name, features, description, price)\n",
                "- User questions about the products\n",
                "- AI-generated answers\n",
                "- Evaluation metrics for answer quality"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "First, let's install the required packages and initialize the Gretel client."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Installing Required Packages\n",
                "\n",
                "First, let's install the Gretel Python client from GitHub."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "h8RI8LG0igeo"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install -U git+https://github.com/gretelai/gretel-python-client@main"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "DC5Yf-Yyidzz",
                "outputId": "e0b91815-1bbf-4219-e6cc-d1c7184f5e73"
            },
            "outputs": [],
            "source": [
                "from gretel_client.navigator_client import Gretel\n",
                "\n",
                "gretel = Gretel(\n",
                "    api_key=\"prompt\",  # Will prompt for your API key\n",
                "    endpoint=\"https://api.dev.gretel.ai\"  # Gretel API endpoint\n",
                ")\n",
                "\n",
                "# Initialize Data Designer with the Apache-2.0 model suite\n",
                "aidd = gretel.data_designer.new(model_suite=\"apache-2.0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining Data Structures\n",
                "\n",
                "Now we'll define the data models and evaluation rubrics for our product information dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "l5q7YysHji8O"
            },
            "outputs": [],
            "source": [
                "import string\n",
                "from gretel_client.data_designer.params import Rubric\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "# Define product information structure\n",
                "class ProductInfo(BaseModel):\n",
                "  product_name: str = Field(..., description=\"A realistic product name for the market.\")\n",
                "  key_features: list[str] = Field(..., min_length=1, max_length=3, description=\"Key product features.\")\n",
                "  description: str = Field(..., description=\"A short, engaging description of what the product does, highlighting a unique but believable feature.\")\n",
                "  price_usd: float = Field(..., description=\"The stated price in USD.\")\n",
                "\n",
                "\n",
                "# Define evaluation rubrics for answer quality\n",
                "CompletenessRubric = Rubric(\n",
                "    name=\"Completeness\",\n",
                "    description=\"Evaluation of AI assistant's thoroughness in addressing all aspects of the user's query.\",\n",
                "    scoring={\n",
                "        \"Complete\": \"The response thoroughly covers all key points requested in the question, providing sufficient detail to satisfy the user's information needs.\",\n",
                "        \"PartiallyComplete\": \"The response addresses the core question but omits certain important details or fails to elaborate on relevant aspects that were requested.\",\n",
                "        \"Incomplete\": \"The response significantly lacks necessary information, missing major components of what was asked and leaving the query largely unanswered.\",\n",
                "    }\n",
                ")\n",
                "\n",
                "AccuracyRubric = Rubric(\n",
                "    name=\"Accuracy\",\n",
                "    description=\"Evaluation of how factually correct the AI assistant's response is relative to the product information.\",\n",
                "    scoring={\n",
                "        \"Accurate\": \"The information provided aligns perfectly with the product specifications without introducing any misleading or incorrect details.\",\n",
                "        \"PartiallyAccurate\": \"While some information is correctly stated, the response contains minor factual errors or potentially misleading statements about the product.\",\n",
                "        \"Inaccurate\": \"The response presents significantly wrong information about the product, with claims that contradict the actual product details.\",\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Generation Workflow\n",
                "\n",
                "Now we'll configure the data generation workflow to create product information, questions, and answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define product category options\n",
                "aidd.add_column(\n",
                "    name=\"category\",\n",
                "    type=\"category\",\n",
                "    params={\"values\": ['Electronics', 'Clothing', 'Home Appliances', 'Groceries', 'Toiletries', \n",
                "                       'Sports Equipment', 'Toys', 'Books', 'Pet Supplies', 'Tools & Home Improvement', \n",
                "                       'Beauty', 'Health & Wellness', 'Outdoor Gear', 'Automotive', 'Jewelry', \n",
                "                       'Watches', 'Office Supplies', 'Gifts', 'Arts & Crafts', 'Baby & Kids', \n",
                "                       'Music', 'Video Games', 'Movies', 'Software', 'Tech Devices']}\n",
                ")\n",
                "\n",
                "# Define price range to seed realistic product types\n",
                "aidd.add_column(\n",
                "    name=\"price_tens_of_dollars\",\n",
                "    type=\"uniform\",\n",
                "    params={\"low\": 1, \"high\": 200},\n",
                "    convert_to=\"int\"\n",
                ")\n",
                "\n",
                "aidd.add_column(\n",
                "    name=\"product_price\",\n",
                "    type=\"expression\",\n",
                "    expr=\"{{ (price_tens_of_dollars * 10) - 0.01 | round(2) }}\",\n",
                "    dtype=\"float\"\n",
                ")\n",
                "\n",
                "# Generate first letter for product name to ensure diversity\n",
                "aidd.add_column(\n",
                "    name=\"first_letter\",\n",
                "    type=\"category\",\n",
                "    params={\"values\": list(string.ascii_uppercase)}\n",
                ")\n",
                "\n",
                "# Generate product information\n",
                "aidd.add_column(\n",
                "    name=\"product_info\",\n",
                "    type=\"llm-structured\",\n",
                "    prompt=\"\"\"\\\n",
                "Generate a realistic product description for a product in the {{ category }} category that costs {{ product_price }}.\n",
                "The name of the product MUST start with the letter {{ first_letter }}.\\\n",
                "\"\"\",\n",
                "    output_format=ProductInfo\n",
                ")\n",
                "\n",
                "# Generate user questions about the product\n",
                "aidd.add_column(\n",
                "    name=\"question\",\n",
                "    prompt=\"Ask a question about the following product:\\n\\n {{ product_info }}\",\n",
                ")\n",
                "\n",
                "# Determine if this example will include hallucination\n",
                "aidd.add_column(\n",
                "  name=\"is_hallucination\",\n",
                "  type=\"bernoulli\",\n",
                "  params={\"p\": 0.5}\n",
                ")\n",
                "\n",
                "# Generate answers to the questions\n",
                "aidd.add_column(\n",
                "    name=\"answer\",\n",
                "    prompt=\"\"\"\\\n",
                "{%- if is_hallucination == 0 -%}\n",
                "<product_info>\n",
                "{{ product_info }}\n",
                "</product_info>\n",
                "\n",
                "{%- endif -%}\n",
                "User Question: {{ question }}\n",
                "\n",
                "Directly and succinctly answer the user's question.\\\n",
                "{%- if is_hallucination == 1 -%}\n",
                " Make up whatever information you need to in order to answer the user's request.\\\n",
                "{%- endif -%}\n",
                "\"\"\"\n",
                ")\n",
                "\n",
                "# Evaluate answer quality\n",
                "aidd.add_column(\n",
                "    name=\"llm_answer_metrics\",\n",
                "    type=\"llm-judge\",\n",
                "    prompt=\"\"\"\\\n",
                "<product_info>\n",
                "{{ product_info }}\n",
                "</product_info>\n",
                "\n",
                "User Question: {{question }}\n",
                "AI Assistant Answer: {{ answer }}\n",
                "\n",
                "Judge the AI assistant's response to the user's question about the product described in <product_info>.\\\n",
                "\"\"\",\n",
                "    rubrics=[CompletenessRubric, AccuracyRubric]\n",
                ")\n",
                "\n",
                "# Extract metric scores for easier analysis\n",
                "aidd.add_column(\n",
                "    name=\"completeness_result\",\n",
                "    type=\"expression\",\n",
                "    expr=\"{{ llm_answer_metrics.Completeness.score }}\"\n",
                ")\n",
                "\n",
                "aidd.add_column(\n",
                "    name=\"accuracy_result\",\n",
                "    type=\"expression\",\n",
                "    expr=\"{{ llm_answer_metrics.Accuracy.score }}\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate the Preview\n",
                "\n",
                "Let's examine a sample record to understand the generated data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview the generated data\n",
                "outs = aidd.preview()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "outs.display_sample_record()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Viewing the Dataset\n",
                "\n",
                "We can view the entire preview dataset to understand the variety of products, questions, and answers generated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "outs.dataset.df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generating the Full Dataset\n",
                "\n",
                "Now that we've verified our data model looks good, let's generate a full dataset with 1,000 records."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "HrRvPXoyTFLn"
            },
            "outputs": [],
            "source": [
                "# Run the job\n",
                "workflow_run = aidd.create(num_records=1_000, name=\"product_qa_dataset\")\n",
                "\n",
                "workflow_run.wait_until_done()"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "base_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
