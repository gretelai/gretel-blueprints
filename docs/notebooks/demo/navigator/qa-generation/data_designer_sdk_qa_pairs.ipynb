{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a776ccf",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/navigator/qa-generation/data_designer_sdk_qa_pairs.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Qw4dkKAPvhT"
   },
   "source": [
    "# ðŸ“š Data Designer SDK: Generate Diverse Q&A Pairs\n",
    "\n",
    "This notebook demonstrates a general approach for extracting Q&A pairs from any source document\n",
    "(e.g. text, markdown, or PDF files). The generated Q&A pairs can be used for:\n",
    "- **Instruction Tuning:** Training models with clear, self-contained examples.\n",
    "- **Retrieval-Augmented Generation (RAG):** Enhancing retrieval systems with precise and context-supported Q&A pairs.\n",
    "- **Search and FAQ Systems:** Powering natural language query systems and documentation.\n",
    "\n",
    "> **Note:** The [Data Designer](https://docs.gretel.ai/create-synthetic-data/gretel-data-designer-beta) functionality demonstrated in this notebook is currently in **Early Preview**. To access these features and run this notebook, please [join the waitlist](https://gretel.ai/navigator/data-designer#waitlist).\n",
    "\n",
    "# ðŸ“˜ Getting Started\n",
    "\n",
    "First, let's install and import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yELZ-vcSE67b"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "\n",
    "%%capture\n",
    "!pip install -qq langchain unstructured[pdf] smart_open git+https://github.com/gretelai/gretel-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ml6WiAdCYaN"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# -------------\n",
    "# Define your source document(s) and the number of Q&A pairs to generate.\n",
    "# You can replace this with your own documents in PDF, markdown, or text formats.\n",
    "\n",
    "DOCUMENT_LIST = [\"https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/rag_evals/databricks-state-of-data-ai-report.pdf\"]\n",
    "NUM_QA_PAIRS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBmeZnTgFjFU"
   },
   "outputs": [],
   "source": [
    "# Document Processing\n",
    "# ------------------\n",
    "# The DocumentProcessor class handles loading and chunking source documents for RAG evaluation.\n",
    "# We use langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing.\n",
    "\n",
    "from typing import List, Union\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.auto import partition\n",
    "from smart_open import open\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def parse_document(self, uri: str) -> str:\n",
    "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
    "        with open(uri, 'rb') as file:\n",
    "            content = file.read()\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                temp_file.write(content)\n",
    "                temp_file.flush()\n",
    "                elements = partition(temp_file.name)\n",
    "\n",
    "        os.unlink(temp_file.name)\n",
    "        return \"\\n\\n\".join([str(element) for element in elements])\n",
    "\n",
    "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
    "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
    "        if isinstance(uris, str):\n",
    "            uris = [uris]\n",
    "\n",
    "        all_chunks = []\n",
    "        for uri in uris:\n",
    "            text = self.parse_document(uri)\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vu8Qx1WXJWiW"
   },
   "outputs": [],
   "source": [
    "# Data Models\n",
    "# -----------\n",
    "# Define Pydantic models for structured output generation:\n",
    "# 1. QAPair: Schema for question-answer pairs\n",
    "# 2. EvalMetrics: Schema for scoring generation quality\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(..., description=\"A clear and concise question derived from the context.\")\n",
    "    answer: str = Field(..., description=\"A detailed and accurate answer fully supported by the context.\")\n",
    "\n",
    "\n",
    "class EvalMetrics(BaseModel):\n",
    "    clarity: int = Field(\n",
    "        ...,\n",
    "        description=\"How clear and understandable is the question? (1=vague/confusing, 5=perfectly clear and well-structured)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    factual_accuracy: int = Field(\n",
    "        ...,\n",
    "        description=\"Is the answer fully supported by the context? (1=contains errors/unsupported claims, 5=completely accurate and supported)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    comprehensiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"Does the answer cover all necessary details? (1=missing crucial information, 5=complete coverage of all relevant points)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    answer_relevance: int = Field(\n",
    "        ...,\n",
    "        description=\"Does the answer directly address what was asked in the question? (1=misaligned/tangential, 5=perfectly addresses the question)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k009Ko0EJZas"
   },
   "outputs": [],
   "source": [
    "# Setup & Configure Data Designer\n",
    "# --------------------------------\n",
    "# Initialize the Data Designer with a custom system message to ensure that the generated\n",
    "# Q&A pairs are high-quality, context-supported, and tailored for a variety of downstream applications.\n",
    "\n",
    "\n",
    "from gretel_client.navigator import DataDesigner\n",
    "\n",
    "# Process document chunks\n",
    "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
    "chunks = processor.process_documents(DOCUMENT_LIST)\n",
    "\n",
    "# Initialize Data Designer\n",
    "designer = DataDesigner(\n",
    "    api_key=\"prompt\",\n",
    "    model_suite=\"llama-3.x\",  # or \"apache-2.0\" as needed\n",
    "    endpoint=\"https://api.gretel.cloud\",\n",
    "    special_system_instructions=\"\"\"\\\n",
    "You are an expert at generating high-quality, context-supported Q&A pairs.\n",
    "Your output should be clear, concise, and factually correct.\n",
    "Ensure that every question is self-contained and every answer is comprehensive and derived solely from the provided context.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Add Seed Columns for Controlled Diversity\n",
    "\n",
    "# Context: Document chunks.\n",
    "designer.add_categorical_seed_column(\n",
    "    name=\"context\",\n",
    "    description=\"A chunk of text extracted from the source document.\",\n",
    "    values=chunks\n",
    ")\n",
    "\n",
    "# Difficulty with nested Sophistication.\n",
    "designer.add_categorical_seed_column(\n",
    "    name=\"difficulty\",\n",
    "    description=\"The overall difficulty level of the question.\",\n",
    "    values=[\"easy\", \"medium\", \"hard\"],\n",
    "    subcategories=[\n",
    "        {\n",
    "            \"name\": \"sophistication\",\n",
    "            \"values\": {\n",
    "                \"easy\": [\"basic\", \"straightforward\"],\n",
    "                \"medium\": [\"intermediate\", \"moderately complex\"],\n",
    "                \"hard\": [\"advanced\", \"sophisticated\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Question Style: Tone and approach.\n",
    "designer.add_categorical_seed_column(\n",
    "    name=\"question_style\",\n",
    "    description=\"The style or tone of the question.\",\n",
    "    values=[\"factual\", \"exploratory\", \"analytical\", \"comparative\"]\n",
    ")\n",
    "\n",
    "# Target Audience: Language complexity.\n",
    "designer.add_categorical_seed_column(\n",
    "    name=\"target_audience\",\n",
    "    description=\"The intended audience for the Q&A pair.\",\n",
    "    values=[\"novice\", \"intermediate\", \"expert\"]\n",
    ")\n",
    "\n",
    "# Response Format: Expected answer style.\n",
    "designer.add_categorical_seed_column(\n",
    "    name=\"response_format\",\n",
    "    description=\"The format of the answer expected (e.g., short, detailed, step-by-step).\",\n",
    "    values=[\"short\", \"detailed\", \"step-by-step\", \"list\"]\n",
    ")\n",
    "\n",
    "# Define Generation Template for Q&A Pairs\n",
    "designer.add_generated_data_column(\n",
    "    name=\"qa_pair\",\n",
    "    generation_prompt=(\n",
    "        \"\\n{context}\\n\\n\"\n",
    "        \"Based on the above context, generate a high-quality Q&A pair. The question should be clear and concise, \"\n",
    "        \"and tailored for an audience at the '{target_audience}' level. The overall difficulty should be '{difficulty}', \"\n",
    "        \"with a corresponding sophistication level (e.g., {sophistication}). The question style should be '{question_style}', \"\n",
    "        \"and the answer should be provided in a '{response_format}' format. \"\n",
    "        \"Ensure that the answer is factually accurate, fully derived from the context, and comprehensive.\\n\"\n",
    "        \"Put your thoughts within <think>...</think> before providing the JSON.\"\n",
    "    ),\n",
    "    data_config={\n",
    "        \"type\": \"structured\",\n",
    "        \"params\": {\"model\": QAPair}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optional: Define Evaluation Template for Q&A Pairs\n",
    "designer.add_generated_data_column(\n",
    "    name=\"eval_metrics\",\n",
    "    llm_type=\"judge\",\n",
    "    generation_prompt=(\n",
    "        \"\\n{context}\\n\\n\"\n",
    "        \"For the above Q&A pair:\\n\"\n",
    "        \"{qa_pair}\\n\\n\"\n",
    "        \"Rate each criterion on a scale of 1-5:\\n\"\n",
    "        \"- Clarity\\n\"\n",
    "        \"- Factual Accuracy\\n\"\n",
    "        \"- Comprehensiveness\\n\"\n",
    "        \"- Answer Relevance\\n\"\n",
    "        \"Put your thoughts within <think>...</think> before providing the JSON.\"\n",
    "    ),\n",
    "    data_config={\n",
    "        \"type\": \"structured\",\n",
    "        \"params\": {\"model\": EvalMetrics}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Preview a Sample of 10 Generated Records\n",
    "preview = designer.generate_dataset_preview()\n",
    "preview.display_sample_record()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNNiHIIhjli8"
   },
   "outputs": [],
   "source": [
    "# Generate a report analyzing RAG coverage\n",
    "# ---------------------\n",
    "\n",
    "import json\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def generate_qa_report(df):\n",
    "\n",
    "    qa_df = pd.json_normalize(\n",
    "        df.assign(eval_metrics=lambda _df: _df[\"eval_metrics\"].apply(json.loads)).to_dict(orient=\"records\")\n",
    "    )\n",
    "    console = Console()\n",
    "    categories = ['difficulty', 'sophistication', 'question_style', 'target_audience', 'response_format']\n",
    "\n",
    "    console.print(\"\\n[bold blue]ðŸ§  QA Pair Generation Report[/bold blue]\", justify=\"center\")\n",
    "    console.print(\"=\" * 80, justify=\"center\")\n",
    "    console.print(f\"\\n[bold]Total QA Pairs:[/bold] {len(qa_df)}\")\n",
    "\n",
    "    for category in categories:\n",
    "        if category in qa_df.columns:\n",
    "            # Only count non-empty values\n",
    "            counts = Counter(x for x in qa_df[category] if pd.notna(x) and x != '')\n",
    "            if not counts:\n",
    "                continue\n",
    "\n",
    "            table = Table(title=f\"\\n{category.title()} Distribution\")\n",
    "            table.add_column(\"Category\", style=\"cyan\")\n",
    "            table.add_column(\"Count\", justify=\"right\")\n",
    "            table.add_column(\"Percentage\", justify=\"right\")\n",
    "\n",
    "            total = sum(counts.values())\n",
    "            for value, count in sorted(counts.items()):\n",
    "                percentage = (count / total) * 100\n",
    "                table.add_row(str(value), str(count), f\"{percentage:.1f}%\")\n",
    "\n",
    "            console.print(table)\n",
    "\n",
    "    if any(col.startswith('eval_metrics.') for col in qa_df.columns):\n",
    "        metrics_table = Table(title=\"\\nQuality Metrics Summary\")\n",
    "        metrics_table.add_column(\"Metric\")\n",
    "        metrics_table.add_column(\"Average Score\", justify=\"right\")\n",
    "\n",
    "        metrics = ['clarity', 'factual_accuracy', 'comprehensiveness', \"answer_relevance\"]\n",
    "        for metric in metrics:\n",
    "            col = f'eval_metrics.{metric}'\n",
    "            if col in qa_df.columns:\n",
    "                scores = qa_df[col].dropna()\n",
    "                if len(scores) > 0:\n",
    "                    avg_score = scores.mean()\n",
    "                    metrics_table.add_row(\n",
    "                        metric.replace('_', ' ').title(),\n",
    "                        f\"{avg_score:.2f}/5.00\"\n",
    "                    )\n",
    "\n",
    "        console.print(metrics_table)\n",
    "\n",
    "generate_qa_report(preview.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8REiGAvaPm-"
   },
   "outputs": [],
   "source": [
    "# Explore the generated preview as a Pandas DataFrame\n",
    "# ---------------------------\n",
    "\n",
    "preview.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCAOD8Y4J819"
   },
   "outputs": [],
   "source": [
    "# Generate and Analyze Dataset\n",
    "# ---------------------------\n",
    "# Uncomment these lines to generate evaluation pairs, save them to JSONL,\n",
    "# and analyze the coverage and quality of the generated dataset.\n",
    "\n",
    "#batch_job = designer.submit_batch_workflow(num_records=NUM_QA_PAIRS)\n",
    "#dataset = batch_job.fetch_dataset(wait_for_completion=True)\n",
    "\n",
    "#generate_qa_report(dataset)\n",
    "\n",
    "#dataset.to_json('qa_pairs.jsonl', orient='records', lines=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
