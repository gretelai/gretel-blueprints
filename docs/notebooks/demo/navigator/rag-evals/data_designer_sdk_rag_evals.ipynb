{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Designer SDK: Generate Diverse RAG Evaluations\n",
        "\n",
        "Generate comprehensive evaluation datasets for your RAG systems, customized to your content and use cases. This blueprint helps you create diverse question-answer pairs at scale, testing both answerable and unanswerable scenarios across different difficulty levels and reasoning types that form a critical step in deploying a production grade RAG system.\n",
        "\n",
        "> **Note:** The [Data Designer](https://docs.gretel.ai/create-synthetic-data/gretel-data-designer-beta) functionality demonstrated in this notebook is currently in **Early Preview**. To access these features and run this notebook, please [join the waitlist](https://gretel.ai/navigator/data-designer#waitlist).\n",
        "\n",
        "# ðŸ“˜ Getting Started\n",
        "\n",
        "First, let's install and import the required packages:"
      ],
      "metadata": {
        "id": "_Qw4dkKAPvhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yELZ-vcSE67b"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "\n",
        "%%capture\n",
        "!pip install -qq langchain unstructured[pdf] smart_open git+https://github.com/gretelai/gretel-python-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "# -------------\n",
        "# Define source documents and total number of evaluation pairs to generate.\n",
        "# You can replace this with your own documents.\n",
        "\n",
        "DOCUMENT_LIST = [\"https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/rag_evals/databricks-state-of-data-ai-report.pdf\"]\n",
        "NUM_EVALS = 100"
      ],
      "metadata": {
        "id": "2ml6WiAdCYaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Processing\n",
        "# ------------------\n",
        "# The DocumentProcessor class handles loading and chunking source documents for RAG evaluation.\n",
        "# We use langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing.\n",
        "\n",
        "from typing import List, Union\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from unstructured.partition.auto import partition\n",
        "from smart_open import open\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "    def parse_document(self, uri: str) -> str:\n",
        "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
        "        with open(uri, 'rb') as file:\n",
        "            content = file.read()\n",
        "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                temp_file.write(content)\n",
        "                temp_file.flush()\n",
        "                elements = partition(temp_file.name)\n",
        "\n",
        "        os.unlink(temp_file.name)\n",
        "        return \"\\n\\n\".join([str(element) for element in elements])\n",
        "\n",
        "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
        "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
        "        if isinstance(uris, str):\n",
        "            uris = [uris]\n",
        "\n",
        "        all_chunks = []\n",
        "        for uri in uris:\n",
        "            text = self.parse_document(uri)\n",
        "            chunks = self.text_splitter.split_text(text)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        return all_chunks"
      ],
      "metadata": {
        "id": "JBmeZnTgFjFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Models\n",
        "# -----------\n",
        "# Define Pydantic models for structured output generation:\n",
        "# 1. QAPair: Schema for question-answer evaluation pairs\n",
        "# 2. EvalMetrics: Schema for scoring generation quality\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, Literal\n",
        "\n",
        "class QAPair(BaseModel):\n",
        "    question: str = Field(..., description=\"A specific question related to the domain of the context\")\n",
        "    answer: str = Field(..., description=\"Either a context-supported answer or explanation of why the question cannot be answered\")\n",
        "    reasoning: str = Field(..., description=\"Explanation of why this Q&A pair is valuable for evaluation\")\n",
        "    question_type: Literal[\"answerable\", \"unanswerable\"] = Field(..., description=\"Whether the question should be answerable from the given context\")\n",
        "    expected_behavior: str = Field(..., description=\"Expected RAG system behavior when encountering this question\")\n",
        "\n",
        "class EvalMetrics(BaseModel):\n",
        "   context_relevance: int = Field(..., description=\"How relevant the retrieved context is (1=irrelevant, 5=perfectly relevant)\", ge=1, le=5)\n",
        "   answer_precision: int = Field(..., description=\"Answer accuracy or appropriateness (1=incorrect/inappropriate, 5=perfect)\", ge=1, le=5)\n",
        "   answer_completeness: int = Field(..., description=\"Information completeness (1=missing critical info, 5=fully complete)\", ge=1, le=5)\n",
        "   hallucination_avoidance: int = Field(..., description=\"Adherence to facts (1=complete fabrication, 5=no hallucination)\", ge=1, le=5)"
      ],
      "metadata": {
        "id": "Vu8Qx1WXJWiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Data Designer\n",
        "# ---------------------\n",
        "# Set up the data generation pipeline with:\n",
        "# 1. Process source document into chunks\n",
        "# 2. Initialize Data Designer\n",
        "# 3. Configure seed columns for controlled diversity\n",
        "# 4. Define generation templates for Q&A pairs and eval metrics\n",
        "\n",
        "from gretel_client.navigator import DataDesigner\n",
        "\n",
        "# Process document chunks\n",
        "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
        "chunks = processor.process_documents(DOCUMENT_LIST)\n",
        "\n",
        "# Initialize Data Designer\n",
        "designer = DataDesigner(\n",
        "    api_key=\"prompt\",\n",
        "    cache=\"yes\",\n",
        "    model_suite=\"llama-3.x\"\n",
        "    )\n",
        "\n",
        "# Add seed columns\n",
        "designer.add_categorical_seed_column(\n",
        "    name=\"context\",\n",
        "    values=chunks\n",
        ")\n",
        "\n",
        "designer.add_categorical_seed_column(\n",
        "    name=\"difficulty\",\n",
        "    values=[\"easy\", \"medium\", \"hard\"]\n",
        ")\n",
        "\n",
        "designer.add_categorical_seed_column(\n",
        "    name=\"reasoning_type\",\n",
        "    values=[\n",
        "        \"factual recall\",\n",
        "        \"inferential reasoning\",\n",
        "        \"comparative analysis\",\n",
        "        \"procedural understanding\",\n",
        "        \"cause and effect\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "designer.add_categorical_seed_column(\n",
        "    name=\"question_type\",\n",
        "    values=[\n",
        "        \"answerable\",\n",
        "        \"unanswerable\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generation template for Q&A pairs\n",
        "designer.add_generated_data_column(\n",
        "    name=\"qa_pair\",\n",
        "    generation_prompt=(\n",
        "        \"<context>\\n{context}\\n</context>\\n\\n\"\n",
        "        \"Generate a {difficulty} {reasoning_type} question-answer pair.\\n\"\n",
        "        \"The question should be {question_type} using the provided context.\\n\\n\"\n",
        "        \"For answerable questions:\\n\"\n",
        "        \"- Ensure the answer is fully supported by the context\\n\"\n",
        "        \"- Make the reasoning clear and traceable\\n\\n\"\n",
        "        \"For unanswerable questions:\\n\"\n",
        "        \"- Keep the question topically relevant\\n\"\n",
        "        \"- Make it clearly beyond the context's scope\\n\"\n",
        "        \"- Explain why it cannot be answered\\n\\n\"\n",
        "        \"Put your thoughts within <think>...</think> before providing the JSON.\"\n",
        "    ),\n",
        "    data_config={\n",
        "        \"type\": \"structured\",\n",
        "        \"params\": {\"model\": QAPair}\n",
        "    }\n",
        ")\n",
        "\n",
        "# Eval template for Q&A pairs\n",
        "designer.add_generated_data_column(\n",
        "   name=\"eval_metrics\",\n",
        "   llm_type=\"judge\",\n",
        "   generation_prompt=(\n",
        "       \"<context>\\n{context}\\n</context>\\n\\n\"\n",
        "       \"For this {difficulty} {reasoning_type} Q&A pair:\\n\"\n",
        "       \"{qa_pair}\\n\\n\"\n",
        "       \"Score each metric from 1-5 (5 is best):\\n\"\n",
        "       \"1. Context relevance (1=irrelevant, 5=perfectly relevant)\\n\"\n",
        "       \"2. Answer precision (1=incorrect/inappropriate, 5=perfect)\\n\"\n",
        "       \"3. Answer completeness (1=missing critical info, 5=fully complete)\\n\"\n",
        "       \"4. Hallucination avoidance (1=complete fabrication, 5=no hallucination)\\n\\n\"\n",
        "       \"Put your thoughts within <think>...</think> before providing the JSON.\"\n",
        "   ),\n",
        "   data_config={\n",
        "       \"type\": \"structured\",\n",
        "       \"params\": {\"model\": EvalMetrics}\n",
        "   }\n",
        ")\n",
        "\n",
        "# Preview sample of 10 records\n",
        "preview = designer.generate_dataset_preview()\n",
        "preview.display_sample_record()\n"
      ],
      "metadata": {
        "id": "k009Ko0EJZas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze Dataset Coverage\n",
        "# -----------------------\n",
        "# Analyze the generated evaluation dataset to ensure:\n",
        "# - Good distribution across question types and difficulty levels\n",
        "# - Strong evaluation metrics\n",
        "# - Sufficient coverage of different reasoning types\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.panel import Panel\n",
        "from collections import Counter\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "def analyze_rag_coverage(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Analyze the coverage of RAG evaluation examples.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the RAG evaluation data\n",
        "\n",
        "    Displays:\n",
        "        Rich console tables showing:\n",
        "        - Distribution of question types, difficulty levels, and reasoning types\n",
        "        - Statistics for evaluation metrics\n",
        "        - Summary of total examples\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "\n",
        "    # Create main table for category coverage\n",
        "    category_table = Table(title=\"Category Coverage Analysis\", show_header=True, header_style=\"bold magenta\")\n",
        "    category_table.add_column(\"Category\", style=\"blue\")\n",
        "    category_table.add_column(\"Distribution\", style=\"green\")\n",
        "\n",
        "    # Analyze categorical distributions\n",
        "    categories = {\n",
        "        \"Question Type\": \"question_type\",\n",
        "        \"Difficulty\": \"difficulty\",\n",
        "        \"Reasoning Type\": \"reasoning_type\"\n",
        "    }\n",
        "\n",
        "    for category_name, column in categories.items():\n",
        "        value_counts = df[column].value_counts()\n",
        "        distribution = \"\\n\".join([\n",
        "            f\"{k}: {v} ({v/len(df)*100:.1f}%)\"\n",
        "            for k, v in value_counts.items()\n",
        "        ])\n",
        "        category_table.add_row(category_name, distribution)\n",
        "\n",
        "    # Create metrics table\n",
        "    metrics_table = Table(title=\"Evaluation Metrics Analysis\", show_header=True, header_style=\"bold magenta\")\n",
        "    metrics_table.add_column(\"Metric\", style=\"blue\")\n",
        "    metrics_table.add_column(\"Statistics\", style=\"green\")\n",
        "    metrics_table.add_column(\"Distribution\", style=\"yellow\")\n",
        "\n",
        "    # Analyze metrics distributions\n",
        "    metrics = df['eval_metrics'].apply(json.loads)\n",
        "    for metric in metrics.iloc[0].keys():\n",
        "        values = [m[metric] for m in metrics]\n",
        "        stats = {\n",
        "            'mean': sum(values) / len(values),\n",
        "            'min': min(values),\n",
        "            'max': max(values)\n",
        "        }\n",
        "\n",
        "        # Create distribution buckets\n",
        "        buckets = Counter([round(v * 2) / 2 for v in values])  # Round to nearest 0.5\n",
        "        distribution = \"\\n\".join([f\"{k:.1f}: {v}\" for k, v in sorted(buckets.items())])\n",
        "\n",
        "        stats_str = f\"Mean: {stats['mean']:.2f}\\nMin: {stats['min']:.2f}\\nMax: {stats['max']:.2f}\"\n",
        "        metrics_table.add_row(metric, stats_str, distribution)\n",
        "\n",
        "    # Create summary table\n",
        "    summary_table = Table.grid()\n",
        "    summary_table.add_column(style=\"bold blue\")\n",
        "    summary_table.add_column(style=\"green\")\n",
        "\n",
        "    total_examples = len(df)\n",
        "    summary_table.add_row(\"Total Examples:\", str(total_examples))\n",
        "\n",
        "    # Display all tables\n",
        "    console.print(\"\\n\")\n",
        "    console.print(Panel(category_table, title=\"Category Coverage Analysis\", expand=False))\n",
        "    console.print(Panel(metrics_table, title=\"Metrics Analysis\", expand=False))\n",
        "    console.print(Panel(summary_table, title=\"Summary\", expand=False))\n",
        "\n",
        "analyze_rag_coverage(preview.dataset)"
      ],
      "metadata": {
        "id": "9uFtq1kFJpG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and Analyze Dataset\n",
        "# ---------------------------\n",
        "# Uncomment these lines to generate evaluation pairs, save them to JSONL,\n",
        "# and analyze the coverage and quality of the generated dataset.\n",
        "\n",
        "#batch_job = designer.submit_batch_workflow(num_records=NUM_EVALS)\n",
        "#dataset = batch_job.fetch_dataset(wait_for_completion=True)\n",
        "\n",
        "#analyze_rag_coverage(dataset)\n",
        "\n",
        "#dataset.to_json('rag_evals.jsonl', orient='records', lines=True)\n"
      ],
      "metadata": {
        "id": "uCAOD8Y4J819"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}