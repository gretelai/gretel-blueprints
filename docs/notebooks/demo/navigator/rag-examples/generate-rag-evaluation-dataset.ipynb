{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d069ca41",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/navigator/rag-examples/generate-rag-evaluation-dataset.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Qw4dkKAPvhT"
   },
   "source": [
    "# ðŸŽ¨ Navigator Data Designer SDK: Generate Diverse RAG Evaluations\n",
    "\n",
    "Generate comprehensive evaluation datasets for your RAG systems, customized to your content and use cases. This blueprint helps you create diverse question-answer pairs at scale, testing both answerable and unanswerable scenarios across different difficulty levels and reasoning types that form a critical step in deploying a production grade RAG system.\n",
    "\n",
    "> **Note:** The [Data Designer](https://docs.gretel.ai/create-synthetic-data/gretel-data-designer-beta) functionality demonstrated in this notebook is currently in **Early Preview**. To access these features and run this notebook, please [join the waitlist](https://gretel.ai/navigator/data-designer#waitlist).\n",
    "\n",
    "# ðŸ“˜ Getting Started\n",
    "\n",
    "First, let's install and import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yELZ-vcSE67b"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "\n",
    "!pip install -qq langchain smart_open git+https://github.com/gretelai/gretel-python-client\n",
    "!pip install 'unstructured[pdf]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2ml6WiAdCYaN"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# -------------\n",
    "# Define source documents and total number of evaluation pairs to generate.\n",
    "# You can replace this with your own documents.\n",
    "\n",
    "DOCUMENT_LIST = [\"https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/rag_evals/databricks-state-of-data-ai-report.pdf\"]\n",
    "NUM_EVALS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JBmeZnTgFjFU"
   },
   "outputs": [],
   "source": [
    "# Document Processing\n",
    "# ------------------\n",
    "# The DocumentProcessor class handles loading and chunking source documents for RAG evaluation.\n",
    "# We use langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing.\n",
    "\n",
    "from typing import List, Union\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.auto import partition\n",
    "from smart_open import open\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def parse_document(self, uri: str) -> str:\n",
    "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
    "        with open(uri, 'rb') as file:\n",
    "            content = file.read()\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                temp_file.write(content)\n",
    "                temp_file.flush()\n",
    "                elements = partition(temp_file.name)\n",
    "\n",
    "        os.unlink(temp_file.name)\n",
    "        return \"\\n\\n\".join([str(element) for element in elements])\n",
    "\n",
    "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
    "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
    "        if isinstance(uris, str):\n",
    "            uris = [uris]\n",
    "\n",
    "        all_chunks = []\n",
    "        for uri in uris:\n",
    "            text = self.parse_document(uri)\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vu8Qx1WXJWiW"
   },
   "outputs": [],
   "source": [
    "# Data Models\n",
    "# -----------\n",
    "# Define Pydantic models for structured output generation:\n",
    "# 1. QAPair: Schema for question-answer evaluation pairs\n",
    "# 2. EvalMetrics: Schema for scoring generation quality\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(..., description=\"A specific question related to the domain of the context\")\n",
    "    answer: str = Field(..., description=\"Either a context-supported answer or explanation of why the question cannot be answered\")\n",
    "\n",
    "\n",
    "class EvalMetrics(BaseModel):\n",
    "   context_relevance: int = Field(..., description=\"How relevant the retrieved context is (1=irrelevant, 5=perfectly relevant)\", ge=1, le=5)\n",
    "   answer_precision: int = Field(..., description=\"Answer accuracy or appropriateness (1=incorrect/inappropriate, 5=perfect)\", ge=1, le=5)\n",
    "   answer_completeness: int = Field(..., description=\"Information completeness (1=missing critical info, 5=fully complete)\", ge=1, le=5)\n",
    "   hallucination_avoidance: int = Field(..., description=\"Adherence to facts (1=complete fabrication, 5=no hallucination)\", ge=1, le=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k009Ko0EJZas"
   },
   "outputs": [],
   "source": [
    "# Setup & Configure Data Designer using the new SDK\n",
    "# --------------------------------\n",
    "# Process document chunks\n",
    "\n",
    "import pandas as pd\n",
    "from gretel_client.navigator_client import Gretel\n",
    "\n",
    "# Process document chunks\n",
    "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
    "chunks = processor.process_documents(DOCUMENT_LIST)\n",
    "\n",
    "# Create a seed DataFrame with the document chunks\n",
    "seed_df = pd.DataFrame({\"context\": chunks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gretel client and Data Designer\n",
    "gretel = Gretel(api_key=\"prompt\", endpoint=\"https://api.dev.gretel.ai\")\n",
    "aidd = gretel.data_designer.new(model_suite=\"llama-3.x\")\n",
    "\n",
    "# Upload the seed dataset with document chunks\n",
    "aidd.with_seed_dataset(seed_df, sampling_strategy=\"shuffle\", with_replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categorical columns for controlled diversity\n",
    "aidd.add_column(\n",
    "    name=\"difficulty\",\n",
    "    type=\"category\",\n",
    "    params={\"values\": [\"easy\", \"medium\", \"hard\"]}\n",
    ")\n",
    "\n",
    "# Add reasoning_type as a subcategory of difficulty\n",
    "aidd.add_column(\n",
    "    name=\"reasoning_type\",\n",
    "    type=\"category\",\n",
    "    params={\"values\": [\n",
    "        \"factual recall\",\n",
    "        \"inferential reasoning\",\n",
    "        \"comparative analysis\",\n",
    "        \"procedural understanding\",\n",
    "        \"cause and effect\"\n",
    "    ]}\n",
    ")\n",
    "\n",
    "# Add question_type with weights to generate more answerable questions than unanswerable\n",
    "aidd.add_column(\n",
    "    name=\"question_type\",\n",
    "    type=\"category\",\n",
    "    params={\"values\": [\n",
    "        \"answerable\",\n",
    "        \"unanswerable\"\n",
    "    ], \"weights\": [10, 1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add Q&A pair generation column\n",
    "aidd.add_column(\n",
    "    name=\"qa_pair\",\n",
    "    type=\"llm-structured\",\n",
    "    system_prompt=\"\"\"You are an expert at generating high-quality RAG evaluation pairs.\n",
    "Your output should include both answerable and unanswerable questions to properly test RAG systems.\"\"\",\n",
    "    prompt=\"\"\"\\n{{context}}\\n\\n\n",
    "Generate a {{difficulty}} {{reasoning_type}} question-answer pair.\n",
    "The question should be {{question_type}} using the provided context.\n",
    "\n",
    "For answerable questions:\n",
    "- Ensure the answer is fully supported by the context\n",
    "- Make the reasoning clear and traceable\n",
    "\n",
    "For unanswerable questions:\n",
    "- Keep the question topically relevant\n",
    "- Make it clearly beyond the context's scope\n",
    "- Explain why it cannot be answered\n",
    "\n",
    "Put your thoughts within <think>...</think> before providing the JSON.\"\"\",\n",
    "    output_format=QAPair\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client.data_designer.params import Rubric\n",
    "\n",
    "# Define evaluation rubrics\n",
    "context_relevance_rubric = Rubric(\n",
    "    name=\"Context Relevance\",\n",
    "    description=\"Evaluates how relevant the answer is to the provided context\",\n",
    "    scoring={\n",
    "        \"5\": \"Perfect relevance to context with no extraneous information\",\n",
    "        \"4\": \"Highly relevant with minor deviations from context\",\n",
    "        \"3\": \"Moderately relevant but includes some unrelated information\",\n",
    "        \"2\": \"Minimally relevant with significant departure from context\",\n",
    "        \"1\": \"Almost entirely irrelevant to the provided context\"\n",
    "    }\n",
    ")\n",
    "\n",
    "answer_precision_rubric = Rubric(\n",
    "    name=\"Answer Precision\",\n",
    "    description=\"Evaluates the accuracy and specificity of the answer\",\n",
    "    scoring={\n",
    "        \"5\": \"Extremely precise with exact, specific information\",\n",
    "        \"4\": \"Very precise with minor imprecisions\",\n",
    "        \"3\": \"Adequately precise but could be more specific\",\n",
    "        \"2\": \"Imprecise with vague or ambiguous information\",\n",
    "        \"1\": \"Completely imprecise or inaccurate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "answer_completeness_rubric = Rubric(\n",
    "    name=\"Answer Completeness\",\n",
    "    description=\"Evaluates how thoroughly the answer addresses all aspects of the question\",\n",
    "    scoring={\n",
    "        \"5\": \"Fully complete, addressing all aspects of the question\",\n",
    "        \"4\": \"Mostly complete with minor omissions\",\n",
    "        \"3\": \"Adequately complete but missing some details\",\n",
    "        \"2\": \"Substantially incomplete, missing important aspects\",\n",
    "        \"1\": \"Severely incomplete, barely addresses the question\"\n",
    "    }\n",
    ")\n",
    "\n",
    "hallucination_avoidance_rubric = Rubric(\n",
    "    name=\"Hallucination Avoidance\",\n",
    "    description=\"Evaluates the absence of made-up or incorrect information\",\n",
    "    scoring={\n",
    "        \"5\": \"No hallucinations, all information is factual and verifiable\",\n",
    "        \"4\": \"Minimal hallucinations that don't impact the core answer\",\n",
    "        \"3\": \"Some hallucinations that partially affect the answer quality\",\n",
    "        \"2\": \"Significant hallucinations that undermine the answer\",\n",
    "        \"1\": \"Severe hallucinations making the answer entirely unreliable\"\n",
    "    }\n",
    ")\n",
    "\n",
    "EVAL_METRICS_PROMPT_TEMPLATE = \"\"\"\\\n",
    "You are an expert evaluator of question-answer pairs. Analyze the following Q&A pair and evaluate it objectively.\n",
    "\n",
    "For this {{difficulty}} {{reasoning_type}} Q&A pair:\n",
    "{{qa_pair}}\n",
    "\n",
    "Take a deep breath and carefully evaluate each criterion based on the provided rubrics, considering the difficulty level and reasoning type indicated.\n",
    "\"\"\"\n",
    "\n",
    "# Add evaluation metrics column\n",
    "aidd.add_column(\n",
    "    name=\"eval_metrics\",\n",
    "    type=\"llm-judge\",\n",
    "    prompt=EVAL_METRICS_PROMPT_TEMPLATE,\n",
    "    rubrics=[context_relevance_rubric, answer_precision_rubric, answer_completeness_rubric, hallucination_avoidance_rubric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a sample of generated records\n",
    "preview = aidd.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explore the generated preview as a Pandas DataFrame\n",
    "# # ---------------------------\n",
    "preview.dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uFtq1kFJpG_"
   },
   "outputs": [],
   "source": [
    "# Generate a report analyzing RAG coverage\n",
    "# ---------------------\n",
    "\n",
    "import json\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_rag_coverage(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze the coverage of RAG evaluation examples with consistent formatting.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing RAG evaluation data\n",
    "    \"\"\"\n",
    "    # Normalize the DataFrame\n",
    "    qa_df = pd.json_normalize(\n",
    "        df.assign(eval_metrics=lambda _df: _df[\"eval_metrics\"].apply(\n",
    "            lambda x: json.loads(x) if isinstance(x, str) else x\n",
    "        )).to_dict(orient=\"records\")\n",
    "    )\n",
    "\n",
    "    console = Console()\n",
    "    categories = ['question_type', 'difficulty', 'reasoning_type']\n",
    "\n",
    "    # Print header\n",
    "    console.print(\"\\n[bold blue]ðŸ“Š RAG Evaluation Report[/bold blue]\", justify=\"center\")\n",
    "    console.print(\"=\" * 80, justify=\"center\")\n",
    "    console.print(f\"\\n[bold]Total Examples:[/bold] {len(qa_df)}\")\n",
    "\n",
    "    # Category distributions\n",
    "    for category in categories:\n",
    "        if category in qa_df.columns:\n",
    "            # Count non-empty values\n",
    "            counts = Counter(x for x in qa_df[category] if pd.notna(x) and x != '')\n",
    "            if not counts:\n",
    "                continue\n",
    "\n",
    "            table = Table(title=f\"\\n{category.title()} Distribution\")\n",
    "            table.add_column(\"Category\", style=\"cyan\")\n",
    "            table.add_column(\"Count\", justify=\"right\")\n",
    "            table.add_column(\"Percentage\", justify=\"right\")\n",
    "\n",
    "            total = sum(counts.values())\n",
    "            for value, count in sorted(counts.items()):\n",
    "                percentage = (count / total) * 100\n",
    "                table.add_row(str(value), str(count), f\"{percentage:.1f}%\")\n",
    "\n",
    "            console.print(table)\n",
    "\n",
    "    # Extract evaluation metrics from the dataframe\n",
    "    if 'eval_metrics' in df.columns:\n",
    "        # Create a table for metrics\n",
    "        metrics_table = Table(title=\"\\nQuality Metrics Summary\")\n",
    "        metrics_table.add_column(\"Metric\")\n",
    "        metrics_table.add_column(\"Average Score\", justify=\"right\")\n",
    "        \n",
    "        # Get all metrics from the first row to understand structure\n",
    "        metrics_dict = df['eval_metrics'][0]\n",
    "        \n",
    "        # Process each metric category\n",
    "        for category, details in metrics_dict.items():\n",
    "            if 'score' in details:\n",
    "                # Convert score to float if it's stored as string\n",
    "                try:\n",
    "                    score_value = float(details['score'])\n",
    "                    metrics_table.add_row(\n",
    "                        category,\n",
    "                        f\"{score_value:.2f}/5.00\"\n",
    "                    )\n",
    "                except (ValueError, TypeError):\n",
    "                    # Handle case where score might not be a number\n",
    "                    metrics_table.add_row(\n",
    "                        category,\n",
    "                        details['score']\n",
    "                    )\n",
    "        \n",
    "        # Print the metrics table\n",
    "        console.print(metrics_table)\n",
    "\n",
    "# Usage\n",
    "analyze_rag_coverage(preview.dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uCAOD8Y4J819"
   },
   "outputs": [],
   "source": [
    "# Generate and Analyze Full Dataset\n",
    "# ---------------------------\n",
    "# Uncomment these lines to generate evaluation pairs, save them to JSONL,\n",
    "# and analyze the coverage and quality of the generated dataset.\n",
    "\n",
    "workflow_run = aidd.create(\n",
    "   num_records=100,\n",
    "   name=\"rag_eval_generation\",\n",
    "   wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerated dataset shape:\", workflow_run.dataset.df.shape)\n",
    "\n",
    "analyze_rag_coverage(workflow_run.dataset.df)\n",
    "\n",
    "workflow_run.dataset.df.to_json('rag_evals.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
