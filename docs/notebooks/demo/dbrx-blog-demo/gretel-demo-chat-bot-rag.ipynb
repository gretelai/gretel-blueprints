{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a762da09",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/demo/dbrx-blog-demo/gretel-demo-chat-bot-rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41dc7aa3-3c4e-4abd-8adf-f44ad988148f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Gretel x Databricks LLM Chatbot with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b640cc-def8-489c-9597-8da8f8e05fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook can be used in tandem with the workflow notebook to work through the demo. \n",
    "[Original Source](https://notebooks.databricks.com/demos/llm-rag-chatbot/index.html#) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fbc66d-d17c-4e60-b9e2-4fa881c005d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet databricks-sdk==0.28.0 databricks-agents mlflow-skinny mlflow mlflow[gateway] databricks-vectorsearch langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eabaeec8-f6f9-4924-be5a-88fde92da781",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import time\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = input(\"Enter the name of the Vector Search endpoint to use:\")\n",
    "\n",
    "def endpoint_exists(client, endpoint_name):\n",
    "    endpoints = client.list_endpoints()\n",
    "    if 'endpoints' in endpoints:\n",
    "        return any(endpoint['name'] == endpoint_name for endpoint in endpoints['endpoints'])\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(client, endpoint_name, timeout=1200, interval=10):\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        endpoint = client.get_endpoint(endpoint_name)\n",
    "        if 'endpoint_status' in endpoint and endpoint['endpoint_status']['state']== 'ONLINE':\n",
    "            return True\n",
    "        time.sleep(interval)\n",
    "    raise TimeoutError(f\"Endpoint {endpoint_name} did not become ready within {timeout} seconds.\")\n",
    "\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6eaa90-2449-45c3-ac08-2022ddef0f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "## Should be where the synthetic data was written to in the previous notebook\n",
    "catalog = input('Catalog to read from:')\n",
    "db = input('Schema to read from:')\n",
    "\n",
    "def index_exists(client, endpoint_name, index_name):\n",
    "    indices = client.list_indexes(endpoint_name)\n",
    "    if 'vector_indexes' in indices:\n",
    "        return any(index['name'] == index_name for index in indices['vector_indexes'])\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def wait_for_index_to_be_ready(client, endpoint_name, index_name, timeout=1200, interval=10):\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        index = client.get_index(endpoint_name, index_name)\n",
    "        if index.describe().get('status').get('detailed_state').startswith('ONLINE'):\n",
    "            return True\n",
    "        time.sleep(interval)\n",
    "    raise TimeoutError(f\"Endpoint {endpoint_name} did not become ready within {timeout} seconds.\")\n",
    "\n",
    "\n",
    "table_name = input('Table to index:')\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.{table_name}\"\n",
    "index_name = input('Index Name:')\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.{index_name}\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"customer_id\",\n",
    "    embedding_source_column='customer_query', #The column containing our text\n",
    "    embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    "  )\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd448da8-608f-4e51-a778-749389b1751b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "question = \"How can I close my account\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"interaction_type\", \"intent\", \"customer_query\", \"response\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5224b414-8c5c-4f36-b831-070b0f65807e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "rag_chain_config = {\n",
    "    \"databricks_resources\": {\n",
    "        \"llm_endpoint_name\": \"databricks-dbrx-instruct\",\n",
    "        \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    },\n",
    "    \"input_example\": {\n",
    "        \"messages\": [{\"content\": \"Sample user question\", \"role\": \"user\"}]\n",
    "    },\n",
    "    \"llm_config\": {\n",
    "        \"llm_parameters\": {\"max_tokens\": 1500, \"temperature\": 0.01},\n",
    "        \"llm_prompt_template\": \"You are a trusted AI assistant that helps answer questions based only on the provided information. If you do not know the answer to a question, you truthfully say you do not know. Here is the history of the current conversation you are having with your user: {chat_history}. And here is some context which may or may not help you answer the following question: {context}.  Answer directly, do not repeat the question, do not start with something like: the answer to the question, do not add AI in front of your answer, do not say: here is the answer, do not mention the context or the question. Based on this context, answer this question: {question}\",\n",
    "        \"llm_prompt_template_variables\": [\"context\", \"chat_history\", \"question\"],\n",
    "    },\n",
    "    \"retriever_config\": {\n",
    "        \"chunk_template\": \"Passage: {chunk_text}\\n\",\n",
    "        \"data_pipeline_tag\": \"poc\",\n",
    "        \"parameters\": {\"k\": 5, \"query_type\": \"ann\"},\n",
    "        \"schema\": {\"chunk_text\": \"customer_query\", \"primary_key\": \"customer_id\"},\n",
    "        \"vector_search_index\": f\"{vs_index_fullname}\",\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    with open('rag_chain_config.yaml', 'w') as f:\n",
    "        yaml.dump(rag_chain_config, f)\n",
    "except:\n",
    "    print('pass to work on build job')\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9611fec-6d18-47f9-b039-3d935264ecd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain.py\n",
    "import os\n",
    "import mlflow\n",
    "from operator import itemgetter\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def extract_previous_messages(chat_messages_array):\n",
    "    messages = \"\\n\"\n",
    "    for msg in chat_messages_array[:-1]:\n",
    "        messages += (msg[\"role\"] + \": \" + msg[\"content\"] + \"\\n\")\n",
    "    return messages\n",
    "\n",
    "def combine_all_messages_for_vector_search(chat_messages_array):\n",
    "    return extract_previous_messages(chat_messages_array) + extract_user_query_string(chat_messages_array)\n",
    "\n",
    "#Get the conf from the local conf file\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')\n",
    "\n",
    "databricks_resources = model_config.get(\"databricks_resources\")\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "llm_config = model_config.get(\"llm_config\")\n",
    "\n",
    "# Connect to the Vector Search Index\n",
    "vs_client = VectorSearchClient(disable_notice=True)\n",
    "vs_index = vs_client.get_index(\n",
    "    endpoint_name=databricks_resources.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=retriever_config.get(\"vector_search_index\"),\n",
    ")\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    columns=[\n",
    "        vector_search_schema.get(\"primary_key\"),\n",
    "        vector_search_schema.get(\"chunk_text\"),\n",
    "        vector_search_schema.get(\"document_uri\"),\n",
    "    ],\n",
    ").as_retriever(search_kwargs=retriever_config.get(\"parameters\"))\n",
    "\n",
    "# Required to:\n",
    "# 1. Enable the RAG Studio Review App to properly display retrieved chunks\n",
    "# 2. Enable evaluation suite to measure the retriever\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=vector_search_schema.get(\"primary_key\"),\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    doc_uri=vector_search_schema.get(\"document_uri\")\n",
    ")\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt\n",
    "def format_context(docs):\n",
    "    chunk_template = retriever_config.get(\"chunk_template\")\n",
    "    chunk_contents = [\n",
    "        chunk_template.format(\n",
    "            chunk_text=d.page_content,\n",
    "        )\n",
    "        for d in docs\n",
    "    ]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "# Prompt Template for generation\n",
    "prompt = PromptTemplate(\n",
    "    template=llm_config.get(\"llm_prompt_template\"),\n",
    "    input_variables=llm_config.get(\"llm_prompt_template_variables\"),\n",
    ")\n",
    "\n",
    "# FM for generation\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"context\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(combine_all_messages_for_vector_search)\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_previous_messages)\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# chain.invoke(model_config.get(\"input_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ae7c43-81c0-405b-88d5-e189fa4f95a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "with mlflow.start_run(run_name=f\"dbdemos_rag_quickstart\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "        model_config='rag_chain_config.yaml',  # Chain configuration \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=model_config.get(\"input_example\"),  # Save the chain's input schema.  MLflow will execute the chain before logging & capture it's output schema.\n",
    "    )\n",
    "\n",
    "# Test the chain locally\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(model_config.get(\"input_example\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42d7642-2f86-4443-a2ad-3a10bc211818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "MODEL_NAME = input(\"Enter the name of the model to be reviewed (e.g., dbdemos_rag_demo):\")\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28c9303-d548-4c76-8e14-1de038c45861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "instructions_to_reviewer = f\"\"\"### Instructions for Testing the our Databricks Documentation Chatbot assistant\n",
    "\n",
    "Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.\n",
    "\n",
    "1. **Variety of Questions**:\n",
    "   - Please try a wide range of questions that you anticipate the end users of the application will ask. This helps us ensure the application can handle the expected queries effectively.\n",
    "\n",
    "2. **Feedback on Answers**:\n",
    "   - After asking each question, use the feedback widgets provided to review the answer given by the application.\n",
    "   - If you think the answer is incorrect or could be improved, please use \"Edit Answer\" to correct it. Your corrections will enable our team to refine the application's accuracy.\n",
    "\n",
    "3. **Review of Returned Documents**:\n",
    "   - Carefully review each document that the system returns in response to your question.\n",
    "   - Use the thumbs up/down feature to indicate whether the document was relevant to the question asked. A thumbs up signifies relevance, while a thumbs down indicates the document was not useful.\n",
    "\n",
    "Thank you for your time and effort in testing our assistant. Your contributions are essential to delivering a high-quality product to our end users.\"\"\"\n",
    "\n",
    "def wait_for_model_serving_endpoint_to_be_ready(endpoint_name, timeout=1200, interval=10):\n",
    "    client = WorkspaceClient()\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        endpoint = client.serving_endpoints.get(name=endpoint_name)\n",
    "        if('ready' in dir(endpoint.state)):\n",
    "            return True\n",
    "        time.sleep(interval)\n",
    "    raise TimeoutError(f\"Endpoint {endpoint_name} did not become ready within {timeout} seconds.\")\n",
    "\n",
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=MODEL_NAME_FQN, model_version=uc_registered_model_info.version, scale_to_zero=True)\n",
    "\n",
    "# Add the user-facing instructions to the Review App\n",
    "agents.set_review_instructions(MODEL_NAME_FQN, instructions_to_reviewer)\n",
    "\n",
    "wait_for_model_serving_endpoint_to_be_ready(deployment_info.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "327f6f08-d14f-4583-b8ed-203e7a87e591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DBRX Rag use Case",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
