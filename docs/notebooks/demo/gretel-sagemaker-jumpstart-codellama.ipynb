{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-Tuning CodeLlama on Gretel's Synthetic Text-to-SQL Dataset and AWS JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251624f9-1eb6-4051-a774-0a4ba83cabf5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "This notebook demonstrates how to use the SageMaker Python SDK to fine-tune the pre-trained CodeLlama-13B model on [Gretel's synthetic text-to-sql](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql) dataset. \n",
    "\n",
    "The notebook requires a `ml.g5.24xlarge` instance for training job usage. If you encounter an error message that you've exceeded your quota, use the Service Quotas console to request an increase. For instructions on how to request a quota increase, see [Requesting a quota increase](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Install Necessary Packages\n",
    "Please restart the kernel after executing the cell below for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9f032-3eeb-4d4d-b4b2-1b150fa64cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet datasets transformers func_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61aeb0a-0bfd-42d9-9277-fdfe57441752",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eea7db-d79d-4390-a51a-28442478953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from sagemaker import Session\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d21e27-a147-4268-b326-4a758e31c148",
   "metadata": {},
   "source": [
    "### Select Model\n",
    "Select your desired model ID. You can search for available models in the [Built-in Algorithms with pre-trained Model Table](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e01401-82db-4d49-9457-f930f4138618",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-codellama-13b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "### Load Dataset\n",
    "We will use the [synthetic text-to-SQL dataset](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql) provided by Gretel. The dataset is a rich dataset of high-quality synthetic Text-to-SQL samples, designed and generated using Gretel Navigator, and released under Apache 2.0. \n",
    "\n",
    "The dataset contains a total of 105,851 records partitioned into 100,000 train and 5,851 test records. From the ~23M tokens, there are ~12M SQL tokens. The SQL queries have coverage across 100 distinct domains/verticals, various SQL tasks, such as data definition, retrieval, manipulation, analytics & reporting, at a wide range of SQL complexity levels, including subqueries, single joins, multiple joins, aggregations, window functions, set operations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gretel_text_to_sql = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "gretel_text_to_sql[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "Create a template for using the data in an instruction format for the training job. This template will also be used during model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"prompt\": (\n",
    "        \"[INST] Write a SQL query that answers the following question based on the given database schema and any additional information provided. Use SQLite syntax.\\n\\n\"\n",
    "        \"[SCHEMA] {sql_context}\\n\\n\"\n",
    "        \"[KNOWLEDGE] This is an '{sql_task_type}' task, commonly used for {sql_task_type_description}. In the domain of {domain}, which involves {domain_description}.\\n\\n\"\n",
    "        \"[QUESTION] {sql_prompt}\\n\\n\"\n",
    "        \"[/INST]\"\n",
    "    ),\n",
    "    \"completion\": \"```{sql}```\\n\\n\\n{sql_explanation}\\n\",\n",
    "}\n",
    "\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)\n",
    "\n",
    "display(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload Training Data and Template to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = Session()\n",
    "output_bucket = session.default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/gretel_text_to_sql\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "\n",
    "### Set Hyperparameters\n",
    "Define the hyperparameters for fine-tuning the model. By default, the models will train via domain adaptation, so you must indicate instruction tuning through the `instruction_tuned` hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"instruction_tuned\": \"True\",\n",
    "    \"epoch\": \"1\",\n",
    "    \"learning_rate\": \"0.0002\",\n",
    "    \"lora_r\": \"8\",\n",
    "    \"lora_alpha\": \"32\",\n",
    "    \"lora_dropout\": \"0.05\",\n",
    "    \"int8_quantization\": \"False\",\n",
    "    \"enable_fsdp\": \"True\",\n",
    "    \"per_device_train_batch_size\": \"4\",\n",
    "    \"per_device_eval_batch_size\": \"2\",\n",
    "    \"max_train_samples\": \"-1\",\n",
    "    \"max_val_samples\": \"-1\",\n",
    "    \"max_input_length\": 512,\n",
    "    \"validation_split_ratio\": \"0.2\",\n",
    "    \"train_data_split_seed\": \"0\",\n",
    "}\n",
    "\n",
    "# Setup the estimator with the generated model name\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    environment={\"accept_eula\": \"true\"},  # Accept EULA for gated models\n",
    "    disable_output_compression=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14614415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bdf74-a06d-49ee-b267-719f2c693f9b",
   "metadata": {},
   "source": [
    "Due to the potential complexity and duration of the training job, it may take several hours to complete. During this period, it is possible that the Python kernel might time out and disconnect. However, the training process will continue to run in SageMaker uninterrupted.\n",
    "\n",
    "If you encounter a disconnection, you can still proceed with deploying your trained model by using the training job name. Follow these steps to locate and use your training job name:\n",
    "\n",
    "1. Navigate to the AWS Management Console.\n",
    "2. Select SageMaker.\n",
    "3. Go to Training Jobs under the Training section.\n",
    "4. Locate your specific training job and copy the training job name.\n",
    "\n",
    "Once you have the training job name, you can use the following Python code to attach to the existing training job, monitor the logs, and deploy your model:\n",
    "\n",
    "```python\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# Replace '<<training_job_name>>' with your actual training job name\n",
    "training_job_name = '<<training_job_name>>'\n",
    "\n",
    "# Attach to the existing training job\n",
    "attached_estimator = JumpStartEstimator.attach(training_job_name)\n",
    "\n",
    "# Optional: View logs\n",
    "attached_estimator.logs()\n",
    "\n",
    "# Deploy the trained model\n",
    "attached_estimator.deploy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment and Invocation\n",
    "\n",
    "Deploy the fine-tuned model to an endpoint directly from the estimator and invoke the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564a1dc-40cf-4f2a-b680-5000b09656fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_and_predict(prompt_input, parameters):\n",
    "    \"\"\"\n",
    "    Generates a SQL query based on the given prompt input using a language model.\n",
    "\n",
    "    Args:\n",
    "        prompt_input (dict): A dictionary containing the prompt input with the following keys:\n",
    "            - schema (str): The database schema.\n",
    "            - question (str): The question to be answered.\n",
    "            - knowledge (str): Additional knowledge or context.\n",
    "            - database (str): Name of the database.\n",
    "        parameters (dict): Additional parameters for model prediction.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated SQL query along with the database name, separated by '----- bird -----'.\n",
    "    \"\"\"\n",
    "    # Extract inputs from the prompt_input dictionary\n",
    "    sql_context = prompt_input[\"schema\"]  # Extract schema\n",
    "    sql_prompt = prompt_input[\"question\"]  # Extract question\n",
    "    knowledge = prompt_input[\"knowledge\"]  # Extract knowledge\n",
    "    database = prompt_input[\"database\"]  # Extract database\n",
    "\n",
    "    # Construct the prompt string with schema, knowledge, and question\n",
    "    prompt = (\n",
    "        f\"[INST] Write a SQL query that answers the following question based on the given database schema and any additional information provided. Use SQLite syntax.\\n\\n\"\n",
    "        f\"[SCHEMA] {sql_context}\\n\\n\"\n",
    "        f\"[KNOWLEDGE] {knowledge}\\n\\n\"\n",
    "        f\"[QUESTION] {sql_prompt}\\n\\n\"\n",
    "        f\"[/INST]\"\n",
    "    )\n",
    "\n",
    "    # Prepare payload for prediction with prompt and parameters\n",
    "    payload = {\"inputs\": prompt, \"parameters\": parameters}\n",
    "\n",
    "    # Get prediction from the model\n",
    "    response = predictor.predict(payload)\n",
    "    response = response[0] if isinstance(response, list) else response\n",
    "\n",
    "    # Split the generated text into code blocks\n",
    "    code_blocks = response[\"generated_text\"].strip().split(\"```\")\n",
    "\n",
    "    # Find the SQL query code block\n",
    "    sql_query = \"\"\n",
    "    for code_block in code_blocks:\n",
    "        if \"SELECT\" in code_block:\n",
    "            sql_query = code_block\n",
    "\n",
    "    # Clean up the SQL query\n",
    "    sql_query = sql_query.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Construct the output string with SQL query and database\n",
    "    output = f\"{sql_query}\\t----- bird -----\\t{database}\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00a204-e638-4f28-859f-82fd689386c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for model prediction\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 256,  # Maximum number of tokens to generate\n",
    "    \"top_p\": 0.9,  # Top-p sampling probability\n",
    "    \"temperature\": 0.1,  # Sampling temperature\n",
    "    \"decoder_input_details\": True,  # Include decoder input details\n",
    "    \"details\": True,  # Include additional details\n",
    "}\n",
    "\n",
    "# Load prompts from a JSON file\n",
    "list_of_prompts = pd.read_json(\n",
    "    \"https://gretel-public-website.s3.us-west-2.amazonaws.com/bird-bench/prompts-dev.json\"\n",
    ")\n",
    "\n",
    "# Iterate over each prompt and predict a response\n",
    "responses = []\n",
    "for question in tqdm(list_of_prompts.columns, desc=f\"Prompting model\"):\n",
    "    prompt_values = list_of_prompts[question].to_dict()\n",
    "    response = prompt_and_predict(prompt_values, parameters)\n",
    "    responses.append(response)\n",
    "\n",
    "# Convert responses to DataFrame\n",
    "df = pd.DataFrame(responses, columns=[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb51d-769b-4805-b14e-3b3990059639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path for saving predicted SQL responses\n",
    "predicted_sql_dir = f\"{model_id}-FT-gretel-text-to-sql\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(predicted_sql_dir):\n",
    "    os.makedirs(predicted_sql_dir)\n",
    "\n",
    "# Define the file path for saving the JSON file\n",
    "predicted_sql_path = f\"{predicted_sql_dir}/predict_dev.json\"\n",
    "\n",
    "# Save the responses DataFrame to a JSON file\n",
    "df[\"response\"].to_json(predicted_sql_path, orient=\"index\", indent=4)\n",
    "\n",
    "# Print the path where the output is saved\n",
    "print(f\"Output saved to {predicted_sql_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee5b65-5b02-4302-a2f2-980e694df3ba",
   "metadata": {},
   "source": [
    "## Run Bird-Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad5f52-e290-4c33-a76f-2a46a0c9bce8",
   "metadata": {},
   "source": [
    "To run the Bird-Benchmark tool on the generated SQL queries you can follow the steps [here](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird).\n",
    "\n",
    "1. Clone the repo https://github.com/AlibabaResearch/DAMO-ConvAI.git\n",
    "2. Download the DEV dataset from: https://bird-bench.github.io/\n",
    "3. Run the eval script\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6acf6-7a63-488d-8a49-d0efc26674eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone the repository if the directory does not exist\n",
    "if not os.path.isdir(\"DAMO-ConvAI\"):\n",
    "    !git clone https://github.com/AlibabaResearch/DAMO-ConvAI.git\n",
    "\n",
    "base_bird_dir = \"DAMO-ConvAI/bird/llm\"\n",
    "\n",
    "if not os.path.exists(f\"{base_bird_dir}/data/dev.zip\"):\n",
    "    !wget -P {base_bird_dir}/data https://bird-bench.oss-cn-beijing.aliyuncs.com/dev.zip --no-check-certificate\n",
    "    # Unzip the downloaded file in the same directory\n",
    "    !unzip {base_bird_dir}/data/dev.zip -d {base_bird_dir}/data\n",
    "    !unzip {base_bird_dir}/data/dev/dev_databases.zip -d {base_bird_dir}/data/dev\n",
    "    # Rename ground truth sql\n",
    "    !cp {base_bird_dir}/data/dev/dev.sql {base_bird_dir}/data/dev/dev_gold.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b0b28-9efa-4167-87a7-2e087c87979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Specify the source file name\n",
    "source_file = \"codellama-13B-FT-Gretel.json\"\n",
    "\n",
    "# Specify the target directory and the new file name\n",
    "predicted_sql_dir, ext = os.path.splitext(source_file)\n",
    "target_file = os.path.join(predicted_sql_dir, \"predict_dev.json\")\n",
    "\n",
    "# Ensure the target directory exists, create if it doesn't\n",
    "os.makedirs(predicted_sql_dir, exist_ok=True)\n",
    "\n",
    "# Copy and rename the source file to the new location\n",
    "shutil.copy(source_file, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5ac7f-6bdf-4fc1-bc13-52d42c0c4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "# Define the common arguments\n",
    "eval_args = [\n",
    "    \"--db_root_path\",\n",
    "    f\"{base_bird_dir}/data/dev/dev_databases/\",\n",
    "    \"--predicted_sql_path\",\n",
    "    predicted_sql_dir + \"/\",\n",
    "    \"--data_mode\",\n",
    "    \"dev\",\n",
    "    \"--ground_truth_path\",\n",
    "    f\"{base_bird_dir}/data/dev/\",\n",
    "    \"--num_cpus\",\n",
    "    \"4\",\n",
    "    \"--diff_json_path\",\n",
    "    f\"{base_bird_dir}/data/dev/dev.json\",\n",
    "]\n",
    "\n",
    "print(\"Execution Accuracy (EX) metric on Dev with Knowledge\")\n",
    "command = [\"python3\", \"-u\", f\"{base_bird_dir}/src/evaluation.py\"] + eval_args\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "print(output.stdout)\n",
    "\n",
    "print(\"Valid Efficiency Score (VES) metric on Dev with Knowledge\")\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"-u\",\n",
    "    f\"{base_bird_dir}/src/evaluation_ves.py\",\n",
    "] + eval_args\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "print(output.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d035b3-71c5-4833-991a-7d53af35e0b6",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Make sure to clean up resources to avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_predictor()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
