{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/zredlined/75a14557b5a288551131d432a2c2f249/gemma-lora-finetune-synthetic-data-ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDsqIWyN_fWI"
      },
      "source": [
        "# How to Fine-Tune Gemma to Label PII in Financial Data\n",
        "\n",
        "Recently, Google released [Gemma](https://huggingface.co/blog/gemma), a new family of state-of-the-art open LLMs. Gemma comes in two sizes: 7B parameters, for efficient deployment and development on consumer-size GPU and TPU and 2B versions for CPU and on-device applications. Both come in base and instruction-tuned variants.\n",
        "\n",
        "This blog post is derived Phil Schmid's excellent [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl) blog. We will use Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) & [datasets](https://huggingface.co/docs/datasets/index), along with synthetic training data generated by [Gretel Navigator](https://gretel.ai/navigator).\n",
        "\n",
        "1. Setup development environment\n",
        "2. Create and prepare the synthetic dataset that we generated earlier\n",
        "3. Fine-tune LLM using `trl` and the `SFTTrainer`\n",
        "4. Test and evaluate the LLM - Visualize results, identify gaps, iterate on synthetic training data if necessary\n",
        "\n",
        "Note: This blog was created to run on an NVIDIA A100 40GB GPU, but can also run on recent consumer grade GPUs running Ampere architecture or later (A10G, RTX3090/4090) with 24GB or more memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8y9houA_fWJ"
      },
      "source": [
        "## 1. Setup development environment\n",
        "\n",
        "Our first step is to install Hugging Face Libraries and Pytorch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs\n",
        "\n",
        "If you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgJW0yl8_fWJ"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install -Uqqr requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFgcYceO_fWK"
      },
      "outputs": [],
      "source": [
        "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDENvgsZ_fWK"
      },
      "source": [
        "We will also need to login into our [Hugging Face](https://huggingface.co) account to be able to access Gemma. To use Gemma you first need to agree to the terms of use. You can do this by visiting the [Gemma page](https://huggingface.co/google/gemma-7b) following the gate mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHdK93bu_fWK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = input(\"Please enter your Hugging Face token: \")\n",
        "\n",
        "login(token=hf_token, add_to_git_credential=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54C385Iu_fWK"
      },
      "source": [
        "## 2. Create and prepare the dataset\n",
        "\n",
        "In this section, we will convert our synthetically generated training set into Huggingface Dataset to support batching and streaming during training, and the ChatML format matching the example format below.\n",
        "\n",
        "```json\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "```\n",
        "\n",
        "The latest release of `trl` supports the conversation dataset formats. This means don't need to do any additional formatting of the dataset. We can use the dataset as is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HkrzsgK9xwk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from smart_open import open\n",
        "\n",
        "# Path to the synthetic training data\n",
        "dataset_path = \"https://storage.googleapis.com/gretel-public-data/gretel-datasets/synthetic-pii-training-data/generated_results_with_spans.jsonl\"\n",
        "\n",
        "# Convert dataset to OAI messages\n",
        "system_message = \"\"\"You are an expert named entity recognition system for financial documents. Users will send you text and you will return only a labeled version of the exact same text, without any additional content. Enclose the recognized entities in curly braces and square brackets, like this: {[ENTITY_TYPE]text}. Do not provide any explanations, summaries, or other additional text.\"\"\"\n",
        "\n",
        "def create_conversation(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": sample[\"document_text\"]},\n",
        "            {\"role\": \"assistant\", \"content\": sample[\"text_markup\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "        for item in data:\n",
        "            if 'pii_spans' in item:\n",
        "                item['pii_spans'] = json.dumps(item['pii_spans']) \n",
        "    return data\n",
        "\n",
        "def pretty_print_conversation(messages):\n",
        "    print(\"Conversation:\")\n",
        "    for msg in messages:\n",
        "        print(f\"\\nRole: {msg['role']}\")\n",
        "        print(f\"Content: {msg['content']}\\n\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Load dataset from JSONL file\n",
        "data = load_jsonl(dataset_path)\n",
        "\n",
        "# Convert data into a dictionary with column names as keys\n",
        "column_names = list(data[0].keys())\n",
        "columns = {column: [sample[column] for sample in data] for column in column_names}\n",
        "\n",
        "# Convert to Dataset object\n",
        "dataset = Dataset.from_dict(columns)\n",
        "\n",
        "# Convert dataset to OAI messages\n",
        "dataset = dataset.map(create_conversation, batched=False)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "test_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "pretty_print_conversation(train_dataset[42][\"messages\"])\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Save datasets to disk\n",
        "train_dataset.to_json(\"train_dataset.json\", orient=\"records\")\n",
        "test_dataset.to_json(\"test_dataset.json\", orient=\"records\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lff80RAc_fWK"
      },
      "source": [
        "## 3. Fine-tune LLM using `trl` and the `SFTTrainer`\n",
        "\n",
        "We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including:\n",
        "* Dataset formatting, including conversational and instruction format\n",
        "* Training on completions only, ignoring prompts\n",
        "* Packing datasets for more efficient training\n",
        "* PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
        "* Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
        "\n",
        "We will use the dataset formatting, packing and PEFT features in our example. As peft method we will use [QLoRA](https://arxiv.org/abs/2305.14314) a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR3gh5Zp_fWL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "\n",
        "# Model and tokenizer IDs\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "tokenizer_id = \"philschmid/gemma-tokenizer-chatml\"\n",
        "output_dir = \"gemma-2b-it-label-chatml\"\n",
        "\n",
        "# BitsAndBytesConfig for int-4 quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, \n",
        "    bnb_4bit_use_double_quant=True, \n",
        "    bnb_4bit_quant_type=\"nf4\", \n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Configure environment to use the first GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "print(\"CUDA_VISIBLE_DEVICES set to:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
        "\n",
        "# Load model with specified device_map and quantization configurations\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map={'': torch.cuda.current_device()},\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# Load and configure tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "tokenizer.padding_side = 'right'  # Adjust to prevent warnings\n",
        "\n",
        "# LoRA configuration for model parameter-efficient tuning\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.05,\n",
        "    r=6, \n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Training arguments for model training\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    learning_rate=2e-4,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    push_to_hub=False,\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iOTZjyz_fWL"
      },
      "source": [
        "Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w01c_QR0_fWL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 1512 # max sequence length for model and packing of the dataset\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False, # We template with special tokens\n",
        "        \"append_concat_token\": False, # No need to add additional separator token\n",
        "    }\n",
        ")\n",
        "\n",
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9QiX7k8_fWL"
      },
      "source": [
        "The training with Flash Attention for 8 epochs with a synthetic dataset of 3k samples (780 steps) took 1:5:12 on a `a2-highgpu-1g` with a single 40GB NVIDIA A100 GPU. The instance costs `1.61$/h` which brings us to a total cost of only ~`$2`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qprwLDJm_fWL"
      },
      "source": [
        "### Optional: Merge LoRA adapter in to the original model\n",
        "\n",
        "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the save_pretrained method.\n",
        "\n",
        "Check out the [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#optional-merge-lora-adapter-in-to-the-original-model) blog post on how to do it .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_WWgGCu_fWL"
      },
      "source": [
        "## 3. Test Model and run Inference\n",
        "\n",
        "After the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\n",
        "\n",
        "_Note: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post._\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmFpq868_fWL"
      },
      "outputs": [],
      "source": [
        "# Clear out GPU memory, and reload the saved model from disk\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDvvnzP2_fWL"
      },
      "source": [
        "We load the adapted model and the tokenize into the `pipeline` to easily test it and extract the token id of `<|im_end|>` to use it in the `generate` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsm4yAiJ_fWL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import  AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"gemma-2b-it-label-chatml\"\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsS1JEFd_fWL"
      },
      "source": [
        "Lets test some prompt samples and see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg_rWnMq_fWM"
      },
      "outputs": [],
      "source": [
        "from finetune_utils import test_inference_batch\n",
        "\n",
        "# Define prompts\n",
        "prompts = [\n",
        "    \"The project site is located at 33.35283, -111.78903, where the new system will be rolled out first, showcasing our commitment to innovation in this region.\",\n",
        "    \"This agreement is executed on this 15th day of October, 2023, at New York, NY. Passport numbers: 635510778, 365520581, and 603951954 are provided for verification purposes.\",\n",
        "    \"We received a support ticket from a customer reporting slow network connectivity at 046 Malone Neck.\",\n",
        "]\n",
        "\n",
        "# Repeat prompts\n",
        "prompts *= 1\n",
        "\n",
        "# Process prompts in batch\n",
        "generated_texts = test_inference_batch(prompts, pipe, eos_token_id=eos_token)\n",
        "\n",
        "# Print results\n",
        "for i, generated_text in enumerate(generated_texts):\n",
        "    prompt = prompts[i]\n",
        "    total_chars_prompt = len(prompt)\n",
        "    total_chars_response = len(generated_text)\n",
        "    print(f\"Prompt:\\n{prompt}\")\n",
        "    print(f\"Response:\\n{generated_text}\")\n",
        "    print(f\"Total characters in prompt: {total_chars_prompt}\")\n",
        "    print(f\"Total characters in response: {total_chars_response}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating the fine-tuned model\n",
        "\n",
        "Now, we are raedy to evaluate our fine-tuned model against the `test_dataset`. Below is an evaluation routine that measures detection accuracy overall (across all document and PII types), as well as per-type and document. This granularity helps us understand where the model is performing well, and where we might need to tune the synthetic data or provide additional examples.\n",
        "\n",
        "We will also add a timer in, to keep track of how long inference takes. Note: This is for illustrative purposes only, we are not doing any optimizations here to speed up inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from finetune_utils import evaluate_pii_labeling_accuracy\n",
        "\n",
        "MAX_EXAMPLES = 500\n",
        "\n",
        "overall_accuracy, pii_type_counters, doc_type_pii_type_counters, detection_percentages, missed_detections = evaluate_pii_labeling_accuracy(test_dataset.select(range(MAX_EXAMPLES)), pipe)\n",
        "\n",
        "print(\"=\" * 40)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.2f}% (Found: {sum(c['found'] for c in pii_type_counters.values())}, Missed: {sum(c['missed'] for c in pii_type_counters.values())})\")\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring evaluation results\n",
        "\n",
        "Let's use some visualizations to dive in and understand where our model is working well, and where it's underperforming. We can use this to tune the synthetic data generation prompt to provide more examples of a particular document, PII type, or both to improve task performance, or to create more synthetic data to improve a class that is under-represented in training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from finetune_utils import plot_found_vs_missed\n",
        "\n",
        "plot_found_vs_missed(pii_type_counters, peft_model_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Heatmap: Visualizing PII Detection Across Document Types\n",
        "\n",
        "We'll create a heatmap using Plotly to visualize our model's performance in detecting PII across different document types. While Large Language Models (LLMs) and today's Small Language Models (SLMs) are more adaptable than traditional NER models (even BERT!), they still require examples specific to the unique data businesses work with.\n",
        "\n",
        "By analyzing the heatmap, we can identify areas where the model performs well and where it needs improvement. This insights will guide us in collecting additional training data or refining examples for specific document types, ultimately enhancing the model's accuracy and robustness in handling diverse data schemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from finetune_utils import plot_detection_percentages_heatmap\n",
        "\n",
        "plot_detection_percentages_heatmap(detection_percentages, peft_model_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Directly investigate any missed detections\n",
        "missed_detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating Throughput\n",
        "\n",
        "As part of an optimized pipeline on an A100 GPU in a batched mode with an input token size of 500 tokens and an accelerated back end such as vLLM or TensorRT-LLM, the Gemma 7B parameter model can average a sustained throughput of 550 tokens/sec, and Gemma 2B at an estimated 2,000 tokens/sec. With Gemma's expanded vocabulary and efficiency, we'll estimate 4 characters per token on average. \n",
        "\n",
        "550 tokens/sec * 4 characters/token * 3600 seconds/hour = 7,920,000 characters per hour that can be anonymized on an A100 GPU, at $1.6 per hour. 1mb of text (1 million characters) with Gemma 7b can be anonymized for about $0.20 in compute. The smaller Gemma2 model can accomplish the same task at approximately $0.05 in compute.\n",
        "\n",
        "In comparison, NER frameworks such as Flair and Spacy `en_core_web_trf` achieve between 1,184 and 3,768 words per second on a GPU (about 2-5x more efficient than Gemma 2B in compute) and ~89% accuracy on NER tasks with industry benchmarks, but with less support for multiple languages and both lack the ability to customize detections via prompt tuning.\n",
        "\n",
        "Coming soon: Cutting edge GPUs like NVIDIA's H200 can lead to potential orders of magnitude gains with Gemma models- where NVIDIA quotes a single H200 GPU delivering 79,000 tokens per second on the Gemma 2B model, and 19,000 tokens per second on the larger 7B model. With Gemma 2b and assuming a compute cost around $8/hr for an H200, this will reduce the cost of anonymizing a 1mb text corpus from $0.05 to about $0.0012."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
