{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcleI72QuN_y"
   },
   "source": [
    "# How to create customized Synthetic Training Data with Gretel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLAxfE00v-Kh"
   },
   "source": [
    "## 1. Define the use case\n",
    "When creating synthetic data, it's important to know the use case and the task you want to use the data to solve. For this example, we are going to use the following use case:\n",
    "\n",
    "> Our goal is to create a wide variety of diverse synthetic examples containing known PII values that can be used to train a language model or NER to detect and label domain-specific PII.\n",
    "\n",
    "## 2. Specify requirements\n",
    "As this model will be used to label PII in a production environment, we will need to simulate a wide variety of examples including:\n",
    "\n",
    "* Multiple languages\n",
    "* Standard PII types (e.g. valid credit card numbers)\n",
    "* Customized PII types\n",
    "* A wide variety of different financial document types and schemas\n",
    "* Generate 10k+ synthetic examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efab8780-c080-4aa3-a088-fd5e84bfa824",
     "showTitle": false,
     "title": ""
    },
    "id": "dGNPnovUVrbE"
   },
   "source": [
    "## Setup development environment\n",
    "\n",
    "Our first step is to install the Gretel client. You'll need a free API key from https://console.gretel.ai. If you're using Colab, we recommend storing it using Colab Secrets, under the name `gretel_api_key`.\n",
    "\n",
    "Also, we'll use Google Drive as a simple way to store the synthetic data as it is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf79225-3795-4cb6-a42f-f3daa6215cdc",
     "showTitle": false,
     "title": ""
    },
    "id": "yjjd6aDAVrbF"
   },
   "outputs": [],
   "source": [
    "!pip install -qq \"git+https://github.com/gretelai/gretel-python-client.git@main\"\n",
    "!pip install -qq faker tqdm jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxPk0kx91L35"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "# from google.colab import drive, userdata\n",
    "from gretel_client import Gretel\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6dyvRdAKnjs"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to store our synthetic data\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# output_file_path = '/content/drive/My Drive/generated_results.jsonl'\n",
    "output_file_path = \"generated_results.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71ugkMmrYXuw"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Gretel Client\n",
    "\n",
    "gretel = Gretel(api_key=\"prompt\")\n",
    "\n",
    "navigator = gretel.factories.initialize_inference_api(backend_model=\"gretelai/tabular-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ac9643-7621-44ee-a7f5-b423bd106641",
     "showTitle": false,
     "title": ""
    },
    "id": "iHMCa0BgVrbG"
   },
   "source": [
    "# Create Contextual Tags\n",
    "\n",
    "## 1: Create domain-specific document types and descriptions\n",
    "\n",
    "Our goal is to create synthetic examples across a wide variety of document types. This is a case where we can leverage an LLM's inherent knowledge of different industry verticals and data types to generate these document types and schemas, without having to go to the trouble of thinking of all possibilities, and then crawling the web to find examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58efbae-7ec1-429c-80d4-55915d19b43b",
     "showTitle": false,
     "title": ""
    },
    "id": "G9oc6SNUVrbG"
   },
   "outputs": [],
   "source": [
    "NUM_DOCUMENT_TYPES = 10\n",
    "\n",
    "DOCUMENT_TYPE_PROMPT = f\"\"\"\n",
    "You are a data expert across the financial services, insurance, and banking verticals. Generate a diverse dataset of domains and detailed descriptions for various document types, including specific formats and schemas, as they relate to the customer journey within a Finance, Insurance, Fintech, or Banking company.\n",
    "\n",
    "Columns:\n",
    "* document_type: Examples include Email, Customer support conversation, Financial Statement, Insurance Policy, Loan Application, Bill of Lading, Safety Data Sheet, Policyholder's Report, XBRL, EDI, SWIFT Messages, FIX Protocol, FpML, ISDA Definitions, BAI Format, MT940.\n",
    "* document_description: A one-sentence detailed description of the kind of documents found in this domain, including specifics about format, common fields, and content type where applicable. Describe the schema, structure, and length of the data format that could be used as instructions to create a document from scratch.\n",
    "\n",
    "Remember to customize fields and formats based on the specific requirements of each domain to accurately reflect the variety and complexity of documents in a SaaS company environment.\"\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists(\"document_types.csv\"):\n",
    "    df = navigator.generate(prompt=DOCUMENT_TYPE_PROMPT, num_records=NUM_DOCUMENT_TYPES)\n",
    "    df.to_csv(\"document_types.csv\", index=False)\n",
    "else:\n",
    "    df = pd.read_csv(\"document_types.csv\")\n",
    "\n",
    "# Display the DataFrame\n",
    "document_type_dict = dict(zip(df[\"document_type\"], df[\"document_description\"]))\n",
    "navigator.display_dataframe_in_notebook(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGggKaD8fjK4"
   },
   "source": [
    "# Build PII Generator\n",
    "\n",
    "We can ask Gretel to synthesize PII types for us, but often different kinds of PII have very specific attributes we want to detect- for example, a valid credit card number must pass a Luhn check. In this case, rather than synthesizing data using the LLM, we'll build a wrapper for the popular Python `Faker` library, as well as allowing users to provide their own lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7JYmdvYfh1L"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "\n",
    "class PIIGenerator:\n",
    "    def __init__(self, locales=[\"en_US\"]):\n",
    "        self.faker = Faker(locales)\n",
    "        self.locales = locales\n",
    "        self.pii_types = {}\n",
    "\n",
    "    def add_faker_generator(self, name, method, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adds a Faker-based generator for a specific PII type.\n",
    "        \"\"\"\n",
    "        self.pii_types[name] = (\n",
    "            self._generate_faker_data,\n",
    "            (method, args, kwargs),\n",
    "            \"generator\",\n",
    "        )\n",
    "\n",
    "    def add_custom_list(self, name, custom_list):\n",
    "        \"\"\"\n",
    "        Adds a custom list of values for a specific PII type.\n",
    "        \"\"\"\n",
    "        self.pii_types[name] = (itertools.cycle, (custom_list,), \"list\")\n",
    "\n",
    "    def _generate_faker_data(self, method, args, kwargs):\n",
    "        \"\"\"\n",
    "        Internal method to generate data using Faker.\n",
    "        \"\"\"\n",
    "        result = getattr(self.faker, method)(*args, **kwargs)\n",
    "        if isinstance(result, tuple):\n",
    "            # Concatenate tuple elements into a single string\n",
    "            return \" \".join(map(str, result))\n",
    "        else:\n",
    "            return str(result)\n",
    "\n",
    "    def get_pii_generator(self, name, count=1):\n",
    "        \"\"\"\n",
    "        Retrieves a generator for the specified PII type.\n",
    "        \"\"\"\n",
    "        if name in self.pii_types:\n",
    "            func, args, _ = self.pii_types[name]\n",
    "            for _ in range(count):\n",
    "                yield func(*args)\n",
    "        else:\n",
    "            raise ValueError(f\"PII type '{name}' not defined.\")\n",
    "\n",
    "    def sample(self, name, sample_size=1):\n",
    "        \"\"\"\n",
    "        Samples data for the specified PII type without exhausting the generator.\n",
    "        \"\"\"\n",
    "        if name not in self.pii_types:\n",
    "            raise ValueError(f\"PII type '{name}' not defined.\")\n",
    "\n",
    "        _, args, type = self.pii_types[name]\n",
    "\n",
    "        if type == \"generator\":\n",
    "            # For generators, generate a larger pool then sample, as direct sampling is not possible\n",
    "            pool_size = max(\n",
    "                10, sample_size\n",
    "            )  # Ensure at least 10 or the requested sample size\n",
    "            pool = [\n",
    "                next(self.get_pii_generator(name, 1)).replace(\"\\n\", \" \")\n",
    "                for _ in range(pool_size)\n",
    "            ]\n",
    "            return random.sample(pool, k=sample_size)\n",
    "        elif type == \"list\":\n",
    "            # Directly sample from the list\n",
    "            return random.sample(args[0], k=sample_size)\n",
    "\n",
    "    def get_all_pii_generators(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of all PII types with their corresponding generators.\n",
    "        \"\"\"\n",
    "        return {name: self.get_pii_generator(name) for name in self.pii_types}\n",
    "\n",
    "    def print_examples(self):\n",
    "        \"\"\"\n",
    "        Prints two examples of each PII type.\n",
    "        \"\"\"\n",
    "        print(\"Current Locales:\", self.locales)\n",
    "\n",
    "        for name, _ in self.pii_types.items():\n",
    "            examples = list(self.sample(name, sample_size=2))\n",
    "            print(f\"Examples of {name}: {examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOY8k9dLFHB8"
   },
   "source": [
    "## Instantiate the PII generator\n",
    "\n",
    "Now, we will instantiate the PII generator with a list of PII that we wish to interleave into our synthetic data. As many types of PII are locale specific, we also define the desired languages/character sets/locales in the `locale_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr5wdw9dfrm5"
   },
   "outputs": [],
   "source": [
    "# Specify a list of locales to use with the Faker library\n",
    "locale_list = [\"en_US\", \"ja_JP\"]\n",
    "\n",
    "# Instantiate the PII generator, and add the data types that we wish to train on.\n",
    "pii_generator = PIIGenerator(locales=locale_list)\n",
    "pii_generator.add_faker_generator(\"Name\", \"name\")\n",
    "pii_generator.add_faker_generator(\"Email\", \"email\")\n",
    "pii_generator.add_faker_generator(\"Phone number\", \"phone_number\")\n",
    "pii_generator.add_faker_generator(\"Full address\", \"address\")\n",
    "pii_generator.add_faker_generator(\"Street address\", \"street_address\")\n",
    "pii_generator.add_faker_generator(\"Credit card\", \"credit_card_number\")\n",
    "pii_generator.add_faker_generator(\"Org or Company Name\", \"company\")\n",
    "pii_generator.add_faker_generator(\"Date of birth\", \"date_of_birth\")\n",
    "pii_generator.add_faker_generator(\"Zip code\", \"zipcode\")\n",
    "pii_generator.add_faker_generator(\"IBAN number\", \"iban\")\n",
    "pii_generator.add_faker_generator(\"IPv4 address\", \"ipv4\")\n",
    "pii_generator.add_faker_generator(\"IPv6 address\", \"ipv6\")\n",
    "pii_generator.add_faker_generator(\"US bank number\", \"bban\")\n",
    "pii_generator.add_faker_generator(\"US passport number\", \"passport_number\")\n",
    "pii_generator.add_faker_generator(\"US social security number\", \"ssn\")\n",
    "pii_generator.add_custom_list(\n",
    "    \"GPS latitude and longitude coordinates\",\n",
    "    [\n",
    "        \"40.56754, -89.64066\",\n",
    "        \"25.13915, 73.06784\",\n",
    "        \"-7.60361, 37.00438\",\n",
    "        \"33.35283, -111.78903\",\n",
    "        \"17.54907, 82.85749\",\n",
    "    ],\n",
    ")\n",
    "pii_generator.add_custom_list(\n",
    "    \"Customer ID\", [\"ID-001\", \"ID-002\", \"ID-003\", \"ID-004\", \"ID-005\"]\n",
    ")\n",
    "\n",
    "# Build a dictionary to store all generators\n",
    "pii_type_dict = pii_generator.get_all_pii_generators()\n",
    "\n",
    "# Sample PII types\n",
    "pii_generator.print_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y2ncqZ4Fetc"
   },
   "source": [
    "## Supporting multiple languages\n",
    "\n",
    "Named entity recognition can be very sensitive to different contexts, schemas, and languages. We want our model to be as adaptable as possible to different languages and dialects that may exist in a production environment for financial data. In the section below, we guide the LLM to create synthetic examples matching the desired language and dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e64d666-3a17-452c-95fa-92bce8ce1369",
     "showTitle": false,
     "title": ""
    },
    "id": "r00PuBhVVrbH"
   },
   "outputs": [],
   "source": [
    "# Create contextual tags for all of the data types we wish to generate\n",
    "\n",
    "language_dict = {\n",
    "    \"english_us\": \"Content in English as spoken and written in the United States\",\n",
    "    #'spanish_spain': 'Content in Spanish as spoken and written in Spain',\n",
    "    #'french_france': 'Content in French as spoken and written in France',\n",
    "    #'german_germany': 'Content in German as spoken and written in Germany',\n",
    "    #'italian_italy': 'Content in Italian as spoken and written in Italy',\n",
    "    #'japanese_japan': 'Content in Japanese as spoken and written in Japan',\n",
    "    #'dutch_netherlands': 'Content in Dutch as spoken and written in the Netherlands',\n",
    "    #'swedish_sweden': 'Content in Swedish as spoken and written in Sweden',\n",
    "    #'english_uk': 'Content in English as spoken and written in the United Kingdom',\n",
    "    #'spanish_mexico': 'Content in Spanish as spoken and written in Mexico',\n",
    "    #'portuguese_brazil': 'Content in Portuguese as spoken and written in Brazil'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49c9834c-0b8d-4b67-8349-193225746e22",
     "showTitle": false,
     "title": ""
    },
    "id": "d_7lkjnjVrbH"
   },
   "source": [
    "# Generate permutations of contextual tags\n",
    "\n",
    "This is the final stage of data preparation before using Gretel to generate data at scale. In this step, we compile all of the following tags to create a \"recipe\" that can guide the LLM to generate highly diverse synthetic data at scale.\n",
    "\n",
    "For this dataset, we will guide each LLM generation with the following properties:\n",
    "* document type (synthetic)\n",
    "* document description (synthetic)\n",
    "* language (synthetic)\n",
    "* pii type (sampled from Faker)\n",
    "* pii values (sampled from Faker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded25a26-8cfa-4ecb-ba56-5df59f001788",
     "showTitle": false,
     "title": ""
    },
    "id": "IkZYsmvGVrbH"
   },
   "outputs": [],
   "source": [
    "N_ROWS = 10  # 1000  # Total number of contextual tags to generate\n",
    "MAX_PII_TYPES = 3\n",
    "PII_VALUES_COUNT = 3  # Number of PII values to generate for each PII type\n",
    "MIN_TEXT_LENGTH = 200  # Minimum length\n",
    "\n",
    "sampled_contextual_tag_data = []\n",
    "for _ in range(N_ROWS):\n",
    "    document_type = random.choice(list(document_type_dict.keys()))\n",
    "    locale = random.choice(list(language_dict.keys()))\n",
    "    # Select a random number of PII types between 1 and 3\n",
    "    num_pii_types = random.randint(1, MAX_PII_TYPES)\n",
    "    selected_pii_types = random.sample(list(pii_type_dict.keys()), num_pii_types)\n",
    "\n",
    "    # Initialize lists to hold the selected PII types and their corresponding values\n",
    "    selected_pii_types_list = []\n",
    "    pii_values_list = []\n",
    "\n",
    "    for pii_type in selected_pii_types:\n",
    "        # Sample the PII values for each selected PII type\n",
    "        pii_values = pii_generator.sample(pii_type, sample_size=PII_VALUES_COUNT)\n",
    "        selected_pii_types_list.append(pii_type)\n",
    "        pii_values_list.append(pii_values)\n",
    "\n",
    "    # Create a single data entry with lists of PII types and their values\n",
    "    data_entry = (\n",
    "        document_type,\n",
    "        document_type_dict[document_type],\n",
    "        selected_pii_types_list,  # This now contains a list of selected PII types\n",
    "        locale,\n",
    "        pii_values_list,  # This now contains a list of lists of PII values\n",
    "    )\n",
    "    sampled_contextual_tag_data.append(data_entry)\n",
    "\n",
    "# Convert sampled data to a DataFrame\n",
    "contextual_tags_df = pd.DataFrame(\n",
    "    sampled_contextual_tag_data,\n",
    "    columns=[\n",
    "        \"document_type\",\n",
    "        \"document_description\",\n",
    "        \"pii_type\",\n",
    "        \"language\",\n",
    "        \"pii_values\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Created {len(contextual_tags_df)} contextual tag permutations\")\n",
    "navigator.display_dataframe_in_notebook(contextual_tags_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36b98e5d-26ce-4d84-a86e-d33f9ce2bc45",
     "showTitle": false,
     "title": ""
    },
    "id": "qDvWP_wUVrbI"
   },
   "source": [
    "# Creating the Synthetic Dataset\n",
    "\n",
    "We have completed the contextual tags to guide our LLM with synthetic data generation, and now we are ready to generate synthetic data at scale. To do this, we'll prompt Gretel to create a new dataset of synthetic records matching the desired `document_type`, `language`, and sampling `PII` attributes from our generator.\n",
    "\n",
    "For this example, we'll use create mode, iterating over each row of the contextual tag dataframe and generating `NUM_RECORDS_PER_CONTEXT` synthetic records for each row.\n",
    "\n",
    "With Gretel, there are two ways to do this:\n",
    "\n",
    "1. Simply passing the contextual tag dataframe with a prompt to Gretel Navigator using \"edit\" mode. This is the simplest method.\n",
    "2. Formatting the contextual tags using a prompt template, asking Gretel to create a synthetic dataset. This offers a bit more customization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xnLS5ZGYhWY"
   },
   "outputs": [],
   "source": [
    "NUM_DOCUMENTS_PER_CONTEXT = 3\n",
    "\n",
    "\n",
    "def add_markup_to_text(text, pii_types_dict):\n",
    "    for pii_type, pii_value_list in pii_types_dict.items():\n",
    "        for pii_value in pii_value_list:\n",
    "            marked_up_pii = f\"{{[{pii_type}]{pii_value}}}\"\n",
    "            text = text.replace(pii_value, marked_up_pii)\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_text2pii_data(row, verbose=False):\n",
    "    document_type = row[\"document_type\"]\n",
    "    document_description = row[\"document_description\"]\n",
    "\n",
    "    pii_types_dict = {}\n",
    "    pii_type = row[\"pii_type\"]\n",
    "    for k in range(len(pii_type)):\n",
    "        pii_types_dict[pii_type[k]] = row[\"pii_values\"][k]\n",
    "\n",
    "    pii_values_markdown = \", or \".join(\n",
    "        [\n",
    "            f\"'{item}'\"\n",
    "            for key, values_list in pii_types_dict.items()\n",
    "            for item in values_list\n",
    "        ]\n",
    "    )\n",
    "    language = row[\"language\"]\n",
    "\n",
    "    generated_records = []\n",
    "    failed_count = 0\n",
    "\n",
    "    create_prompt = f\"\"\"\n",
    "Create a unique, comprehensive dataset entry as described below. Each entry should differ substantially in content, style, and perspective.\n",
    "\n",
    "Dataset format: Two columns - 'document_type' and 'document_text'\n",
    "\n",
    "Entry specifications:\n",
    "\n",
    "'document_type': \"{document_type}\"\n",
    "'document_text': A complete, coherent, and distinct synthetic {document_description} in {language}, formatted as a detailed {document_type}\n",
    "  * Incorporate varied themes, styles, viewpoints, and structures\n",
    "  * Use vivid descriptions, examples, and elaborations\n",
    "  * Avoid repetition; ensure each entry stands out\n",
    "  * Maintain coherence and logical flow\n",
    "  * Seamlessly integrate the following {pii_type} values exactly as provided into the text: {pii_values_markdown}\n",
    "  * Identify appropriate locations within the document to naturally incorporate these values\n",
    "  * Provide context for each {pii_type}, explaining its relevance to the {document_type}\n",
    "  * Ensure the {pii_type} values fit grammatically and contextually within the surrounding text\n",
    "  * Maintain the overall structure and coherence of the {document_type}\n",
    "Aim to create a rich, detailed, and engaging {document_type} that showcases creativity and diversity while seamlessly incorporating the provided {pii_type} values.\n",
    "\"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(create_prompt)\n",
    "\n",
    "    while len(generated_records) < NUM_DOCUMENTS_PER_CONTEXT:\n",
    "        # Generate initial documents\n",
    "        results = navigator.generate(\n",
    "            prompt=create_prompt, num_records=NUM_DOCUMENTS_PER_CONTEXT\n",
    "        )\n",
    "\n",
    "        # Add 'markup' column by applying the markup helper function\n",
    "        results[\"text_markup\"] = results[\"document_text\"].apply(\n",
    "            lambda text: add_markup_to_text(text, pii_types_dict)\n",
    "        )\n",
    "\n",
    "        # Filter out rows where the marked-up text is not different from the provided text\n",
    "        failed_results = results[\n",
    "            (results[\"text_markup\"] == results[\"document_text\"])\n",
    "            | (results[\"document_text\"].str.len() < MIN_TEXT_LENGTH)\n",
    "        ]\n",
    "\n",
    "        # Store the successfully generated records\n",
    "        generated_records.extend(\n",
    "            results[~results.index.isin(failed_results.index)][\n",
    "                [\"document_type\", \"document_text\", \"text_markup\"]\n",
    "            ].values.tolist()\n",
    "        )\n",
    "        failed_count += len(failed_results)\n",
    "\n",
    "    if verbose:\n",
    "        # Print status update\n",
    "        print(\n",
    "            f\"Batch Update: Successfully generated {len(generated_records)} records so far. Failed: {failed_count}.\"\n",
    "        )\n",
    "        # Display an example of the latest successful record\n",
    "        if generated_records:\n",
    "            latest_record = generated_records[-1]\n",
    "            print(f\"Latest Example:\\n{textwrap.fill(str(latest_record), width=80)}\\n\")\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        generated_records, columns=[\"document_type\", \"document_text\", \"text_markup\"]\n",
    "    )\n",
    "\n",
    "\n",
    "navigator.display_dataframe_in_notebook(\n",
    "    generate_text2pii_data(contextual_tags_df.iloc[0], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5268076e-f39a-4543-a0af-cb02188d5c59",
     "showTitle": false,
     "title": ""
    },
    "id": "8OqvwlKiVrbI"
   },
   "source": [
    "## Generate synthetic data at scale\n",
    "\n",
    "After prompt tuning, we are now ready to start generating synthetic data at scale. The code below iterates over each row in the contextual tags dataframe, creating `NUM_DOCUMENTS_PER_CONTEXT` synthetic documents for each given combination of contextual tags. To ensure that all data is saved at each generation, we will append results from each generation to the `output_file_path` file in Google Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkQqwtNY1cF_"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in tqdm(contextual_tags_df.iterrows(), total=len(contextual_tags_df)):\n",
    "    result_df = generate_text2pii_data(row, verbose=False)\n",
    "    results.append(result_df)\n",
    "\n",
    "    # Display the latest result with a scrollbar, if needed\n",
    "    display(\n",
    "        result_df.tail(1)\n",
    "        .style.set_table_attributes(\"style='display:inline'\")\n",
    "        .set_caption(\"Latest Record\")\n",
    "    )\n",
    "\n",
    "    # Append the result to the JSONL file\n",
    "    with jsonlines.open(output_file_path, mode=\"a\") as writer:\n",
    "        for _, result_row in result_df.iterrows():\n",
    "            writer.write(result_row.to_dict())\n",
    "\n",
    "# Concatenate all the DataFrames in the list into a single DataFrame\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame with all results\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0CL5yd-H2FV"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Synthetic Data Generation provides a cost effective, and most importantly, iterative way to build and customize data for AI projects. Synthetic data can significantly enhance task performance and open new opportunities for innovation and feature development where you need data. With the increasing accessibility and cost effectiveness, there has never been a better time to start working with synthetic data."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_generate_text2sql_create_mode_single_generation",
   "widgets": {}
  },
  "colab": {
   "provenance": [
    {
     "file_id": "https://gist.github.com/zredlined/4a4fee95eccf5cceff21d013ac9697e1#file-generate-synthetic-financial-pii-training-data-ipynb",
     "timestamp": 1711561552463
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
