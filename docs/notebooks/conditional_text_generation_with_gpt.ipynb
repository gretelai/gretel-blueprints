{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/conditional_text_generation_with_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTRxpSlaczHY"
   },
   "source": [
    "# Generating Synthetic Text\n",
    "\n",
    "This notebook will walk you through generating realistic but synthetic text examples using an open-source implementation of OpenAI's GPT-3 architecture. \n",
    "\n",
    "In this example, we will generate new annotated text utterances that can be used to augment a real world financial dataset called `banking77`. This augmented dataset will have additional annotated examples that can help downstream ML models better understand and respond to new customer queries. To run this notebook, you will need an API key from the Gretel console,  at https://console.gretel.cloud. \n",
    "<br>\n",
    "\n",
    "** **Limitations and Biases** **\n",
    "Large-scale language models such as GPT-X may produce untrue and/or offensive content without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results. For more information and examples please see [OpenAI](https://huggingface.co/gpt2#limitations-and-bias) and [EleutherAI](https://huggingface.co/EleutherAI/gpt-neo-125M#limitations-and-biases)'s docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEM6kjRsczHd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhBCe4PfrTWW"
   },
   "source": [
    "## Set up your project\n",
    "* `DATASET_PATH`: Specify a dataset to run on.\n",
    "* `INTENT`: Select an intent from the training data to boost examples for.\n",
    "* `SEPARATOR`: Specify a separator character (default=`,`) to combine intents and texts with into a single column.\n",
    "* `PROJECT`: Specify a project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ-TmAdwczHd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from gretel_client import configure_session\n",
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects import create_or_get_unique_project, get_project\n",
    "\n",
    "\n",
    "DATASET_PATH = 'https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/banking77.csv'\n",
    "INTENT = \"card arrival\"\n",
    "SEPARATOR = ','\n",
    "PROJECT = 'banking77'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOygU-1MrTWY",
    "outputId": "4fc3ff59-dd7a-49a9-9c36-2faec13f5d91"
   },
   "outputs": [],
   "source": [
    "# Log into Gretel and configure project\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", endpoint=\"https://api.gretel.cloud\", validate=True, clear=True)\n",
    "\n",
    "project = create_or_get_unique_project(name=PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PD5B0U06ALs"
   },
   "source": [
    "## Create the model configuration\n",
    "\n",
    "In this notebook we will use GPT-Neo, a transformer model designed using EleutherAI's replication of OpenAI's GPT-3 Architecture. This model has been pre-trained on the Pile, a large-scale dataset using 300 billion tokens over 572,300 steps. In this introductory example, we will fine-tune GPT-Neo to generate synthetic text utterances for a given intent that could be used to train a chat-bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3n0npt-_rTWa"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"schema_version\": 1,\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"gpt_x\": {\n",
    "        \"data_source\": \"__\",\n",
    "        \"pretrained_model\": \"EleutherAI/gpt-neo-125M\",\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 1,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"lr_scheduler\": \"cosine\",\n",
    "        \"learning_rate\": 1e-6\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9LTh7GO6VIu"
   },
   "source": [
    "## Load and preview the training dataset\n",
    "Create single-column CSV training set by combining `intent` + `SEPARATOR` + `text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "YMg9nX6SczHe",
    "outputId": "f318c986-7ee8-4c36-cccb-4b30e5f05deb"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine intents and text into a single string to pass to GPT-X.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    max_tokens = 0\n",
    "    \n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df['intent_and_text'] = df['intent'] + SEPARATOR + df['text']\n",
    "    return df\n",
    "    \n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "df = create_dataset(DATASET_PATH)\n",
    "df[['intent_and_text']].to_csv('finetune.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxnH8th-65Dh"
   },
   "source": [
    "## Train the synthetic model\n",
    "In this step, we will task the worker running in the Gretel cloud, or locally, to fine-tune the GPT language model on the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4-E_F0qczHe",
    "outputId": "d10f75c7-f21d-42a5-b6a7-35164b5e2f6b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "model = project.create_model_obj(model_config=config)\n",
    "model.data_source = \"finetune.csv\"\n",
    "model.name = f\"{PROJECT}-gpt\"\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IkWOnVQ7oo1"
   },
   "source": [
    "## Generate synthetic text data\n",
    "The next cells walk through sampling data from the fine-tuned model using a prompt (conditional data generation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8gD9Q2XSFyv",
    "outputId": "5e83f53e-495f-408e-91c7-a1c26d623471"
   },
   "outputs": [],
   "source": [
    "# Generate new text examples for a given intent by seeding\n",
    "# model generation with examples from the class. \n",
    "\n",
    "# NOTE: We have found prompting the model with \n",
    "# ~25 examples for the class you wish to \n",
    "# generate to work well in practice.\n",
    "\n",
    "def create_prompt(df: pd.DataFrame, intent: str = \"\", recs: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Seed GPT-X text generation with an intent from the training data.\n",
    "    \"\"\"\n",
    "    # NOTE: When creating a DataFrame for prompts, it must be a 1-column DataFrame!\n",
    "    sample = df.query(f'intent == \"{intent}\"').head(recs)\n",
    "    prompt = \"\\n\".join([x[0] for x in sample[['intent_and_text']].values])\n",
    "    \n",
    "    # NOTE: the column name provide here does not matter, the returned\n",
    "    # synthetic DataFrame will have the original column name that\n",
    "    # was used to train the model, in this case it will be \"intent_and_text\"\n",
    "    return pd.DataFrame([prompt], columns=[\"prompt_text\"])\n",
    "\n",
    "\n",
    "prompt_df = create_prompt(df=df, intent=INTENT, recs=25)\n",
    "\n",
    "record_handler = model.create_record_handler_obj(\n",
    "    params={\"maximum_text_length\": 1000},\n",
    "    data_source=prompt_df\n",
    ")\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy8q3f2dTAHv"
   },
   "source": [
    "# Creating synthetic intents\n",
    "\n",
    "In the cell below, we process the raw texts generated by GPT-X into a structured dataframe format, by splitting each row based on the intent prefix (`card_arrival`) that was used to prompt generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "8Fx4aeMOSFyw",
    "outputId": "d8416537-56e2-400e-bda5-933685f4b3ac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_intents(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract new intents generated by the GPT-X model.\n",
    "    \"\"\"\n",
    "    MIN_LENGTH = 20\n",
    "    texts = []\n",
    "    \n",
    "    for idx, row in gptx_df.iterrows(): \n",
    "        for text in row[0].split(f\"{INTENT}{SEPARATOR}\"):\n",
    "            text = text.strip()\n",
    "            if len(text) > MIN_LENGTH:\n",
    "                texts.append([INTENT, text])\n",
    "\n",
    "    intents = pd.DataFrame(texts, columns=['intent', 'synthetic_text'])\n",
    "    return intents\n",
    "\n",
    "\n",
    "gptx_df = pd.read_csv(record_handler.get_artifact_link(\"data\"), compression='gzip')\n",
    "gptx_df\n",
    "syn = get_intents(df=gptx_df)\n",
    "syn.head(15)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Conditional Text Generation with Gretel GPT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
