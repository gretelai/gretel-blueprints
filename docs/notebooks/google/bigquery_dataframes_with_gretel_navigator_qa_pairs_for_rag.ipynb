{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b1d7fa",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/google/bigquery_dataframes_with_gretel_navigator_qa_pairs_for_rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Gf-AxN1kPOL"
   },
   "source": [
    "# ðŸ¤– Generate High-Quality Q&A Pairs from Unstructured Data for AI Knowledge Bases\n",
    "\n",
    "Transform unstructured data into valuable, privacy-safe Q&A pairs using [Gretel Navigator](https://gretel.ai/navigator), [Google BigQuery](https://cloud.google.com/bigquery), and the [BigFrames SDK](https://cloud.google.com/python/docs/reference/bigframes/latest).\n",
    "\n",
    "## ðŸ” In this Notebook:\n",
    "\n",
    "1. Retrieve example IT-security podcast transcripts from BigQuery\n",
    "2. Generate synthetic Q&A pairs, filtering out irrelevant information and PII\n",
    "3. Evaluate data quality using LLM-based scoring (toxicity, safety, accuracy, relevance, coherence)\n",
    "4. Store AI-ready, privacy-safe Q&A pairs in BigQuery\n",
    "\n",
    "## ðŸ’ª Why It Matters:\n",
    "\n",
    "- Extract insights from various unstructured data sources\n",
    "- Automatically filter out PII and sensitive information\n",
    "- Customize knowledge extraction for specific topics\n",
    "- Power knowledge bases, FAQs, chatbots, or LLM training data\n",
    "- Ensure high-quality, relevant synthetic data\n",
    "- Optimize for RAG systems or generate diverse test examples\n",
    "- Process large volumes of unstructured data efficiently\n",
    "\n",
    "Combine Gretel's AI models with BigQuery's processing power to create customized AI experiences while maintaining data privacy and quality.\n",
    "\n",
    "[Explore Gretel Navigator](https://gretel.ai/navigator) | [Learn about BigQuery](https://cloud.google.com/bigquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yszAdfe5YH_F"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -Uqq \"gretel-client>=0.22.0\" langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn2J-SDdQbv4"
   },
   "outputs": [],
   "source": [
    "# Install bigframes if it's not already installed in the environment.\n",
    "\n",
    "# %%capture\n",
    "# !pip install bigframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U7nNaBNpAXN"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "import bigframes.pandas as bpd\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure your GCP Project here\n",
    "BIGQUERY_PROJECT = \"gretel-vertex-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66PktR8uYNPn"
   },
   "outputs": [],
   "source": [
    "# Initialize Gretel for synthetic data generation and evaluation\n",
    "from gretel_client import Gretel\n",
    "from gretel_client.bigquery import BigFrames\n",
    "\n",
    "gretel = Gretel(api_key=\"prompt\", validate=True, project_name=\"bigframes-rag\")\n",
    "\n",
    "gretel_bigframes = BigFrames(gretel)\n",
    "\n",
    "# Initialize two separate Navigator models for generation and evaluation\n",
    "gretel_bigframes.init_navigator(\"generator\", backend_model=\"gretelai/auto\")\n",
    "gretel_bigframes.init_navigator(\"evaluator\", backend_model=\"gretelai-google/gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SaQfhYUQxey"
   },
   "outputs": [],
   "source": [
    "# Load the unstructured chat transcripts from BigQuery using BigFrames\n",
    "\n",
    "# Set BigFrames options\n",
    "bpd.options.display.progress_bar = None\n",
    "bpd.options.bigquery.project = BIGQUERY_PROJECT\n",
    "\n",
    "# Define the source project and dataset\n",
    "project_id = \"gretel-public\"\n",
    "dataset_id = \"public\"\n",
    "table_id = \"sample-security-podcasts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1B0ABgEQbv5"
   },
   "outputs": [],
   "source": [
    "# Construct the table path\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Read the table into a DataFrame\n",
    "df = bpd.read_gbq_table(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72ZRdXFj5ZKY"
   },
   "outputs": [],
   "source": [
    "# Visualize an example\n",
    "\n",
    "def print_dataset_statistics(data_source):\n",
    "    \"\"\"Print high level dataset statistics\"\"\"\n",
    "    num_rows = data_source.shape[0]\n",
    "    num_chars = data_source['text'].str.len().sum()\n",
    "\n",
    "    print(f\"\\nNumber of rows: {num_rows}\")\n",
    "    print(f\"Number of characters: {num_chars}\")\n",
    "\n",
    "def print_wrapped_text(text, width=128):\n",
    "    \"\"\"Print text wrapped to a specified width\"\"\"\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    print(wrapped_text)\n",
    "\n",
    "print(\"Sample Security Podcast Transcript:\\n\")\n",
    "print_wrapped_text(df.iloc[0]['text'])\n",
    "print_dataset_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ltrdPJxkXJl"
   },
   "source": [
    "## ðŸ§  Generating Synthetic Q&A Pairs with Gretel AI\n",
    "\n",
    "Gretel's AI models offer powerful capabilities for generating high-quality, domain-specific synthetic Q&A pairs. Key features include:\n",
    "\n",
    "- Utilizes advanced language models to understand and generate context-appropriate content\n",
    "- Creates thought-provoking questions that encourage critical thinking\n",
    "- Generates comprehensive, textbook-quality answers\n",
    "- Maintains topical relevance to security and cloud environments\n",
    "- Scales efficiently to process large volumes of podcast transcripts\n",
    "\n",
    "This approach enables the creation of synthetic Q&A pairs that capture the nuances of security topics while providing valuable training data for chatbots and other AI applications.\n",
    "\n",
    "[Learn more about Gretel's AI Models](https://docs.gretel.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2Xn4le8Yyr8"
   },
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "\n",
    "topics = \"How to protect the security of your Google cloud environment, and techniques used by hackers and advanced threat actors\"\n",
    "\n",
    "prompt_template = \"\"\"\\\n",
    "Given the following text extracted from a podcast, create a dataset with these columns:\n",
    "\n",
    "* `question`: Generate unique, thought-provoking questions that require critical thinking and detailed answers. Focus on the following topics: {topics}. Ensure that each question:\n",
    "  - Is complex enough to necessitate a multi-step or in-depth answer\n",
    "  - Encourages the application of knowledge rather than mere recall\n",
    "  - Addresses potential knowledge gaps or underrepresented aspects of the topic\n",
    "  - Includes sufficient context to be understood without additional information\n",
    "  - Do not reference 'the text', questions must be self contained and introduce the topic and context.\n",
    "\n",
    "* `answer`: Provide comprehensive, textbook-quality answers that thoroughly address the question. Each answer should:\n",
    "  - Present a step-by-step explanation of the concept or solution\n",
    "  - Include all relevant details from the source text, as well as logical extensions or implications\n",
    "  - Explain the reasoning process, not just the final conclusion\n",
    "  - Be self-contained, assuming the reader has no access to the original context\n",
    "  - Aim for 3-5 sentences of rich, educational content\n",
    "\n",
    "Source Text:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co7mHryr5pWv"
   },
   "outputs": [],
   "source": [
    "# Synthesize examples from data\n",
    "\n",
    "def chunk_text(text, max_tokens=6000):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_tokens,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "all_qa_pairs = [] # List[bpd.DataFrame], stores intermediate QA pairs generated from Navigator\n",
    "num_records = 2  # Number of Q&A pairs per chunk\n",
    "\n",
    "total_chunks = sum(len(chunk_text(text)) for text in df.to_pandas()['text'])\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "with tqdm(total=total_chunks * num_records, desc=\"Generating Synthetic Q&A pairs\") as pbar:\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        chunks = chunk_text(text)\n",
    "        for chunk in chunks:\n",
    "            prompt = prompt_template.format(text=chunk, topics=topics)\n",
    "\n",
    "            # Generate the synthetic\n",
    "            chunk_df = gretel_bigframes.navigator_generate(\"generator\", prompt, num_records=num_records, disable_progress_bar=True)\n",
    "\n",
    "            all_qa_pairs.append(chunk_df)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(num_records)\n",
    "\n",
    "print(f\"\\nGenerated Synthetic Q&A pairs for {len(all_qa_pairs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPqaROp1Qbv7"
   },
   "outputs": [],
   "source": [
    "df_synth = bpd.concat(all_qa_pairs, ignore_index=True)\n",
    "gretel_bigframes.display_dataframe_in_notebook(df_synth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5bN8g8Xkcg5"
   },
   "source": [
    "## ðŸŽ¯ Evaluating Synthetic Q&A Pairs\n",
    "\n",
    "To ensure the quality of our generated Q&A pairs, we'll use Gretel's AI evaluation capabilities. This process is crucial for maintaining the integrity and usefulness of our synthetic data, especially when dealing with sensitive or complex information sources.\n",
    "\n",
    "ðŸ’ª With Gretel Navigator, you can pass in the tabular data from BQ and simply add new fields with a prompt, so augmenting data is **only 2 lines of code!**\n",
    "\n",
    "Key benefits of this evaluation step:\n",
    "\n",
    "- Assesses multiple aspects of each Q&A pair, including relevance, coherence, and factual accuracy\n",
    "- Provides numerical scores for easy filtering and quality control\n",
    "- Helps identify and remove low-quality, irrelevant, or potentially sensitive content\n",
    "- Ensures the final dataset meets high standards for use in AI applications\n",
    "- Enables creation of high-quality training data or knowledge bases from diverse, unstructured sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_-RXWvtb1D8"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please evaluate the following dataset of synthetically generated question-answer pairs using these five metrics. For each entry, provide a score between 1 and 100 for each metric:\n",
    "\n",
    "Columns to add to dataset:\n",
    "1. `relevance_score`: int\n",
    "   Measures how well the question and answer relate to the source content and each other.\n",
    "   1 (completely irrelevant) to 100 (highly relevant and on-topic)\n",
    "   Consider: Does the Q&A pair address key points from the source? Is the answer directly related to the question?\n",
    "\n",
    "2. `coherence_score`: int\n",
    "   Assesses the logical flow, clarity, and internal consistency of both the question and the answer.\n",
    "   1 (incoherent or confusing) to 100 (perfectly clear and logically structured)\n",
    "   Consider: Is the language clear? Do ideas flow logically? Is there a consistent narrative?\n",
    "\n",
    "3. `factual_accuracy_score`: int\n",
    "   Measures the factual correctness and informativeness of the answer, based on the source content.\n",
    "   1 (contains major errors or lacks depth) to 100 (completely accurate and informative)\n",
    "   Consider: Are all stated facts correct? Does the answer provide substantial, useful information?\n",
    "\n",
    "4. `bias_score`: int\n",
    "   Evaluates the presence of unfair prejudice, stereotypes, or favoritism in the Q&A pair.\n",
    "   1 (heavily biased) to 100 (neutral and balanced)\n",
    "   Consider: Does the Q&A pair show unfair preference or discrimination? Are diverse perspectives represented fairly when relevant?\n",
    "\n",
    "5. `safety_score`: int\n",
    "   Assesses the degree to which the Q&A pair is free from harmful, toxic, or inappropriate content.\n",
    "   1 (contains harmful or inappropriate content) to 100 (completely safe and appropriate)\n",
    "   Consider: Is the language respectful and non-toxic? Are sensitive topics handled appropriately? Is the content suitable for a general audience?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_scored = gretel_bigframes.navigator_edit(\"evaluator\", prompt, seed_data=df_synth)\n",
    "\n",
    "# For local evaluation of the scores, we use a pandas DataFrame\n",
    "df_eval = df_scored.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_v5grY8Qbv8"
   },
   "outputs": [],
   "source": [
    "# Peak at the QA pairs with LLM-as-a-judge scores\n",
    "\n",
    "gretel_bigframes.display_dataframe_in_notebook(df_scored.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V9e0P8VkVW5"
   },
   "outputs": [],
   "source": [
    "# Helper functions to visualize and filter evaluation results\n",
    "\n",
    "def plot_score_distribution(df: pd.DataFrame) -> None:\n",
    "    score_columns = [col for col in df.columns if col.endswith('_score')]\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, metric in enumerate(score_columns):\n",
    "        scores = df[metric]\n",
    "        fig.add_trace(go.Box(\n",
    "            y=scores,\n",
    "            name=metric.replace('_score', '').capitalize(),\n",
    "            boxpoints='all',\n",
    "            jitter=0.3,\n",
    "            pointpos=-1.8,\n",
    "            marker_color=colors[i % len(colors)]\n",
    "        ))\n",
    "        print(f\"{metric}: Average = {np.mean(scores):.2f}, Std Dev = {np.std(scores):.2f}\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Distribution of LLM Judge Scores',\n",
    "        xaxis_title='Evaluation Metrics',\n",
    "        yaxis_title='Score (1-100)',\n",
    "        xaxis_tickangle=-45,\n",
    "        showlegend=False,\n",
    "        margin=dict(l=40, r=40, t=40, b=80),\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        xaxis=dict(automargin=True, title_standoff=25),\n",
    "        yaxis=dict(automargin=True, title_standoff=15, range=[0, 100])\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def filter_and_summarize(df: pd.DataFrame, threshold: int = 80) -> Tuple[pd.DataFrame, str]:\n",
    "    score_columns = [col for col in df.columns if col.endswith('_score')]\n",
    "    total_records = len(df)\n",
    "\n",
    "    # Filter records\n",
    "    df_filtered = df[df[score_columns].min(axis=1) >= threshold]\n",
    "    filtered_records = total_records - len(df_filtered)\n",
    "\n",
    "    # Create summary\n",
    "    summary = f\"\"\"\n",
    "    âœ¨ Summary of Filtering Process âœ¨\n",
    "    --------------------------------\n",
    "    Total examples processed: {total_records}\n",
    "    Examples filtered out: {filtered_records}\n",
    "    Remaining examples: {len(df_filtered)}\n",
    "    --------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    return df_filtered, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyo5xBwILahq"
   },
   "outputs": [],
   "source": [
    "# Visualize and Filter evaluations\n",
    "score_columns = [col for col in df_eval.columns if col.endswith('_score')]\n",
    "df_eval[score_columns] = df_eval[score_columns].astype(float)\n",
    "\n",
    "# Plot original distribution\n",
    "print(\"Original Distribution:\")\n",
    "plot_score_distribution(df_eval)\n",
    "\n",
    "# Filter and summarize\n",
    "df_filtered, summary = filter_and_summarize(df_eval, threshold=75)\n",
    "\n",
    "# Print summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyacDwobp1Bn"
   },
   "outputs": [],
   "source": [
    "# Write the synthetically generated data to your table in BQ\n",
    "# NOTE: The BQ Dataset must already exist!\n",
    "\n",
    "project_id = BIGQUERY_PROJECT\n",
    "dataset_id = \"syntheticdata\"\n",
    "table_id = \"security-chatbot-qa\"\n",
    "\n",
    "# Construct the table path\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Write to the destination table in BQ, un-comment to actually write to BQ.\n",
    "# df_synth.to_gbq(table_path, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo8rMW0Um3EM"
   },
   "source": [
    "## ðŸš€ Conclusion: Unlocking the Power of Unstructured Data\n",
    "\n",
    "This notebook demonstrates how Gretel's synthetic data capabilities can transform raw, unstructured data into valuable, AI-ready knowledge:\n",
    "\n",
    "1. **Data Transformation**: We've taken complex, potentially sensitive podcast transcripts and extracted focused, high-quality Q&A pairs.\n",
    "2. **Quality Assurance**: By using LLM-based evaluation, we ensure that only the most relevant and accurate information is retained.\n",
    "3. **Versatility**: The resulting dataset can power knowledge bases, chatbots, or serve as training data for LLMs, adapting to your specific needs.\n",
    "4. **Scalability**: This process can be applied to various data sources and scaled to handle large volumes of information.\n",
    "\n",
    "By leveraging these techniques, organizations can unlock the full potential of their diverse data assets, creating customized AI experiences while maintaining data quality and privacy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
