{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "notebook",
     "evaluate_synthetic_data_classification_models.ipynb"
    ]
   },
   "source": [
    "# Evaluate synthetic vs. real data on classification models\n",
    "\n",
    "### How to use this notebook\n",
    "Use this notebook to analyze the performance of your synthetic data vs. real data, where both are trained and evaluated on machine learning classifiers. \n",
    "\n",
    "This notebook gives you 2 options: use a Gretel model to generate synthetic data or BYO synthetic data. In either case, you'll use a Gretel Evaluate task to perform the training and evaluation. After the task completes, you'll see a Gretel Synthetic Data Utility Report. This report provides you the model(s) metrics and synthetic vs. real data comparison. \n",
    "\n",
    "Interested in evaluation on regression models? Check out [the regression notebook](https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/downstream_machine_learning_regression_evaluation.ipynb).\n",
    "\n",
    "\n",
    "### A low-code alternative\n",
    "You can also try the `Synthesize data + evaluate ML performance` flow in the [Gretel Console](https://console.gretel.ai/use_cases/cards/use-case-downstream-accuracy/projects). This is a low-code alternative that will walk you step-by-step through the evaluation. You can find the Synthetic Data Utility Report at the end of the process in [your Projects list](https://console.gretel.ai/projects).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Installation\n",
    "Install the Gretel Client to use Gretel's synthetic models as well as the Gretel Evaluate Regression model. You'll have to get your API key from the [Gretel console](https://www.console.gretel.ai) to configure your session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest Gretel Client\n",
    "%pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Gretel session - enter your API key when prompted\n",
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(endpoint=\"https://api.gretel.cloud\", api_key=\"prompt\", cache=\"yes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Try: Generate synthetic data, then evaluate the synthetic data on classifiers against real-world data\n",
    " First, we'll generate synthetic data using a smaller-sized version of the publicly available [bank marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing), which predicts whether a client will subscribe a term deposit (prediction: yes/no in column \"y\"). We'll use Gretel's LSTM model to train on the real-world and generate the synthetic data.\n",
    " \n",
    " To use the Gretel Evaluate model, you must indicate the target column. Optionally, you can change the test-holdout amount, which is a float indicating the amount of real-world data you want to use as a holdout for testing the downstream classifiers. You can also optionally select which classifiers to use and which metric to optimize for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SUPPORTED MODELS AND METRICS ####\n",
    "## If you want to only use some classification models, you can also indicate which models you want the autoML library to use, by indicating from the list below. \n",
    "## By default, all models will be used in the autoML training. \n",
    "## If you want to change the metric that the classifiers will use to optimize for, you can select one metric from classification_metrics below. The default metric is acc (accuracy).\n",
    "\n",
    "classification_models = [\n",
    "    \"lr\", \n",
    "    \"knn\", \n",
    "    \"nb\", \n",
    "    \"dt\", \n",
    "    \"svm\", \n",
    "    \"rbfsvm\", \n",
    "    \"gpc\", \n",
    "    \"mlp\", \n",
    "    \"ridge\", \n",
    "    \"rf\", \n",
    "    \"qda\", \n",
    "    \"ada\", \n",
    "    \"gbc\", \n",
    "    \"lda\", \n",
    "    \"et\", \n",
    "    \"xgboost\", \n",
    "    \"lightgbm\", \n",
    "    \"dummy\"\n",
    "]\n",
    "\n",
    "shorter_list_classification_models = [\n",
    "    \"nb\", \n",
    "    \"ridge\",\n",
    "    \"rbfsvm\",\n",
    "    \"knn\",\n",
    "    \"xgboost\", \n",
    "    \"ada\", \n",
    "    \"gbc\", \n",
    "    \"mlp\",\n",
    "    \"dummy\"\n",
    "]\n",
    "\n",
    "classification_metrics = [\n",
    "    \"acc\",\n",
    "    \"auc\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "    \"f1\",\n",
    "    \"kappa\",\n",
    "    \"mcc\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a project on Gretel Cloud using the following example project name. Then, notice that the config includes both the synthetic data model and evaluation model. Note we're using the Gretel LSTM model configuration in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project with a name that describes this use case\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "project = create_or_get_unique_project(name=\"bank-marketing-classification-notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects.models import read_model_config\n",
    "\n",
    "# We'll import the bank_marketing_small dataset from Gretel's public S3 bucket\n",
    "# You can modify this to select a dataset of your choice\n",
    "dataset_path = \"https://gretel-datasets.s3.amazonaws.com/bank_marketing_small.csv\"\n",
    "\n",
    "# We will modify the config for Gretel synthetic models to add an extra downstream Evaluate model and task\n",
    "# Uncomment the additional params to change from defaults.\n",
    "config = read_model_config(\"synthetics/tabular-actgan\")\n",
    "\n",
    "config[\"models\"][0][\"actgan\"][\"evaluate\"] = {\n",
    "    # Available downstream tasks are \"classification\" or \"regression\"\n",
    "    \"task\": \"classification\",\n",
    "    # Set to the target you wish to predict -- Change this if you try a different data set!\n",
    "    \"target\": \"y\",  # yes/no to subscriptions, use a categorical column for classification\n",
    "    # \"holdout\": 0.2,  # default holdout value\n",
    "    # \"models\": classification_models,  # default set of models\n",
    "    # \"metric\": \"acc\",  # default metric used for sorting results, choose one\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we'll train and run the model. At the end when the job completes, you can find the Gretel Synthetic Data Utility Report in your local `/tmp` folder OR go to the `bank-marketing-classification-notebook` project by logging into [the Gretel Console](https://console.gretel.ai/projects) for all the downloads and to see more about the model you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and run the model\n",
    "## Note: this will both train and run the model to generate synthetic data as well as \n",
    "## run the downstream metrics evaluation immediately after\n",
    "\n",
    "model = project.create_model_obj(\n",
    "    model_config=config, \n",
    "    data_source=dataset_path\n",
    ")\n",
    "\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model)\n",
    "\n",
    "# Save all artifacts\n",
    "model.download_artifacts(\"/tmp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or: BYO synthetic or augmented data to evaluate downstream classification against real-world data\n",
    "Already have your synthetic or augmented data? You can use your own CSV or JSON(L) data files in the Gretel Evaluate Classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Evaluate SDK using your custom config\n",
    "from gretel_client.evaluation.downstream_classification_report import DownstreamClassificationReport\n",
    "\n",
    "# Create a project with a name that describes this use case\n",
    "# When you go to your Gretel Console, you can find this project and also download the report after the evaluation finishes\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "project = create_or_get_unique_project(name=\"evaluate-bank-classification-notebook-2\") \n",
    "\n",
    "# Params\n",
    "# This is the synthetic data, REQUIRED for evaluate model\n",
    "# Download this sample bank marketing synthetic dataset: https://drive.google.com/uc?export=download&id=1s9nT7be3NFC1HrpEIgIj2tKib8ftoAC_\n",
    "# And make sure your file path is correct\n",
    "data_source = \"/Users/[MY_USERNAME]/Downloads/bank-marketing-synthetic.csv\"\n",
    "\n",
    "# This is the real-world data, REQUIRED for evaluate model\n",
    "ref_data = \"https://gretel-datasets.s3.amazonaws.com/bank_marketing_small.csv\"\n",
    "\n",
    "# Target to predict, REQUIRED for evaluate model\n",
    "target = 'y'  # prediction field for whether a user will opt in\n",
    "\n",
    "# Default holdout value\n",
    "# test_holdout = 0.2\n",
    "\n",
    "# Supply a subset if you do not want all of these, default is to use all of them\n",
    "# models = classification_models\n",
    "\n",
    "# Metric to use for ordering results, default is \"acc\" (Accuracy) for classification\n",
    "# metric = \"acc\"\n",
    "\n",
    "# Create a downstream classification report\n",
    "evaluate = DownstreamClassificationReport(\n",
    "    project=project,\n",
    "    target=target, \n",
    "    data_source=data_source, \n",
    "    ref_data=ref_data,\n",
    "    # holdout=test_holdout,\n",
    "    # models=models,\n",
    "    # metric=metric,\n",
    "    # output_dir = '/tmp',\n",
    "    # runner_mode=\"cloud\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run and view the Evaluate Classification Report\n",
    "\n",
    "evaluate.run() # this will wait for the job to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the full HTML contents of the report.\n",
    "evaluate.as_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the full report JSON details.\n",
    "evaluate.as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary representation of how well the top 3 models trained on synthetic data performed against the \n",
    "# top 3 models trained on real-world data. 'Value' is the synthetic or augmented data's performance against real-world data (averaged)\n",
    "evaluate.peek()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "To see the Gretel Data Utility Report and the results of your evaluation, go to your [Projects list](https://console.gretel.ai/projects) and look for the projects titled `bank-marketing-classification-notebook` or `evaluate-bank-classification-notebook-2`. You can download the Gretel Synthetic Data Quality Report and the Synthetic Data Utility Report.  \n",
    "\n",
    "You can also check out more model details like the configuration and model stats, or keep synthesizing or augmenting your data to get the best results for you. \n",
    "Happy synthesizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1264641a2296bed54b65447ff0d3f452674f070f0748798274bc429fe6ce8efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
