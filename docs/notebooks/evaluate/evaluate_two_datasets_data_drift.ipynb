{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "notebook",
     "evaluate-monitor-two-datasets"
    ]
   },
   "source": [
    "# Evaluate two datasets and monitor data drift  \n",
    "\n",
    "### How to use this notebook\n",
    "Customers often ask how to understand and evaluate the quality of their data as it changes over time. Even if you don't create synthetic data, you can use Gretel Evaluate to compare any two datasets, like monitoring data drift in the same database over time. This could be relevant if you collect more data (e.g. from user growth), implement a new data policy (like GPDR compliance), or need to monitor data quality for maintaining the accuracy of machine learning models. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Installation\n",
    " You'll have to get your API key from the [Gretel console](https://www.console.gretel.ai) to configure your session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest Gretel Client\n",
    "%%capture\n",
    "%pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Gretel session - enter your API key when prompted\n",
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(endpoint=\"https://api.gretel.cloud\", api_key=\"prompt\", cache=\"yes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gretel_client.evaluation.quality_report import QualityReport\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "\n",
    "project = create_or_get_unique_project(name=\"evaluate-datasets-monitor-data\") \n",
    "\n",
    "data_path = 'https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/creditcard_kaggle_25k.csv.zip'\n",
    "data_source = pd.read_csv(data_path)\n",
    "\n",
    "ref_data_path = 'https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/creditcard_kaggle_25k.csv.zip'\n",
    "ref_data = pd.read_csv(ref_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Quality Report LOCALLY, using the specified project\n",
    "local_report = QualityReport(project=project, data_source=data_source, ref_data=ref_data, output_dir='report_results')\n",
    "local_report.run()\n",
    "local_report.peek()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the full HTML contents of the report.\n",
    "\n",
    "import IPython\n",
    "from smart_open import open\n",
    "\n",
    "IPython.display.HTML(data=local_report.as_html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: evaluate data on machine learning classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client.evaluation.downstream_classification_report import DownstreamClassificationReport\n",
    "\n",
    "# Target to predict, required field -- enter the header name of the label or target\n",
    "target = \"Class\" \n",
    "\n",
    "test_holdout = 0.05\n",
    "\n",
    "# Supply a subset if you do not want all of these, default is to use all of them\n",
    "# models = classification_models\n",
    "\n",
    "# Metric to use for ordering results, default is \"acc\" (Accuracy) for classification\n",
    "# metric = \"acc\"\n",
    "\n",
    "# Evaluate classification\n",
    "evaluate = DownstreamClassificationReport(\n",
    "    project=project,\n",
    "    target=target, \n",
    "    data_source=data_source, \n",
    "    ref_data=ref_data,\n",
    "    holdout=test_holdout,\n",
    "    # models=models,\n",
    "    # metric=metric,\n",
    "    # output_dir = '/tmp',\n",
    "    # runner_mode=\"cloud\",\n",
    ")\n",
    "\n",
    "evaluate.run() # this will wait for the job to finish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data utility report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the full HTML contents of the report.\n",
    "\n",
    "import IPython\n",
    "from smart_open import open\n",
    "\n",
    "IPython.display.HTML(data=evaluate.as_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1264641a2296bed54b65447ff0d3f452674f070f0748798274bc429fe6ce8efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
