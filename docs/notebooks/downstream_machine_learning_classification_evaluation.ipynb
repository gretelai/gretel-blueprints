{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "Use this notebook to understand the quality and performance of your synthetic or augmented data on downstream machine learning classification tasks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Install Gretel Client to use both Gretel's synthetic models as well as the Gretel Evaluate Classification model. You'll have to get your API key from the Gretel console dashboard to configure your session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(endpoint=\"https://api-dev.gretel.cloud\", api_key=\"prompt\", cache=\"yes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Option 1: Generate synthetic data, then evaluate the synthetic data on classifiers against real-world data\n",
    " First, we'll generate synthetic data using a publicly available bank marketing dataset, which predicts whether a client will subscribe a term deposite (prediction: yes/no in column \"y\"). We'll use Gretel's ACTGAN model to train on the real-world and generate the synthetic data.\n",
    " \n",
    " To use the Gretel Evaluate Classification model, you must indicate the target column. Optionally, you can change the test-holdout amount, which is a float indicating the amount of real-world data you want to use as a holdout for testing the downstream classifiers. Youc an also optionally select which classifiers to use and which metric to optimize for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SUPPORTED MODELS AND METRICS ####\n",
    "\n",
    "classification_models = [\n",
    "    \"lr\", \n",
    "    \"knn\", \n",
    "    \"nb\", \n",
    "    \"dt\", \n",
    "    \"svm\", \n",
    "    \"rbfsvm\", \n",
    "    \"gpc\", \n",
    "    \"mlp\", \n",
    "    \"ridge\", \n",
    "    \"rf\", \n",
    "    \"qda\", \n",
    "    \"ada\", \n",
    "    \"gbc\", \n",
    "    \"lda\", \n",
    "    \"et\", \n",
    "    \"xgboost\", \n",
    "    \"lightgbm\", \n",
    "    \"dummy\"\n",
    "]\n",
    "\n",
    "classification_metrics = [\n",
    "    \"acc\",\n",
    "    \"auc\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "    \"f1\",\n",
    "    \"kappa\",\n",
    "    \"mcc\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project with a name that describes this use case\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "project = create_or_get_unique_project(name=\"bank-marketing-synthetic-data-downstream-classification-evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects.models import read_model_config\n",
    "\n",
    "# We'll import the bank_marketing_small dataset from Gretel's public S3 bucket\n",
    "# You can modify this to select a dataset of your choice\n",
    "dataset_path = \"\"\n",
    "\n",
    "# We will modify the config for Gretel synthetic models to add an extra downstream Evaluate model and task\n",
    "# Uncomment the additional params to change from defaults.\n",
    "config = read_model_config(\"synthetics/tabular-actgan\")\n",
    "\n",
    "config[\"models\"][0][\"synthetics\"][\"evaluate\"] = {\n",
    "    # Available downstream tasks are \"classification\" or \"regression\"\n",
    "    \"task\": \"classification\",\n",
    "    # Set to the target you wish to predict -- Change this if you try a different data set!\n",
    "    \"target\": \"V3\",  # marital status, use a categorical column for classification\n",
    "    # \"holdout\": 0.2,  # default holdout value\n",
    "    # \"models\": classification_models,  # default set of models\n",
    "    # \"metric\": \"acc\",  # default metric used for sorting results, choose one\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and run the model\n",
    "model = project.create_model_obj(\n",
    "    model_config=config, \n",
    "    data_source=dataset_path\n",
    ")\n",
    "\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model)\n",
    "\n",
    "# Save all artifacts\n",
    "model.download_artifacts(\"/tmp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: BYO sythetic or augmented data to evaluate downstream classification against real-world data\n",
    "Already have your synthetic or augmented data? You can use your own CSV or JSONL data files in the Gretel Evaluate Classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Evaluate SDK using your custom config\n",
    "from gretel_client.evaluation.downstream_classification_report import DownstreamClassificationReport\n",
    "\n",
    "# Params\n",
    "# This is the synthetic data, REQUIRED for evaluate model\n",
    "data_source = \"bank_marketing_synthetic_data.csv\"\n",
    "\n",
    "# This is the real-world data, REQUIRED for evaluate model\n",
    "ref_data = \"bank_marketing_small.csv\"\n",
    "\n",
    "# Target to predict, REQUIRED for evaluate model\n",
    "target = 'y'  # prediction field for whether client will subscribe to a bank term deposit\n",
    "\n",
    "# Default holdout value\n",
    "# test_holdout = 0.2\n",
    "\n",
    "# Supply a subset if you do not want all of these, default is to use all of them\n",
    "# models = classification_models\n",
    "\n",
    "# Metric to use for ordering results, defaults are \"acc\" (Accuracy) for classification, \"r2\" (R2) for regression.\n",
    "# metric = \"acc\"\n",
    "\n",
    "# Create a downstream classification report\n",
    "evaluate = DownstreamClassificationReport(\n",
    "    # project=None,  # Create a temp project\n",
    "    target=target, \n",
    "    data_source=data_source, \n",
    "    ref_data=ref_data,\n",
    "    # holdout=test_holdout,\n",
    "    # models=models,\n",
    "    # metric=metric,\n",
    "    # output_dir=None,\n",
    "    # runner_mode=\"cloud\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate.run() # this will wait for the job to finish\n",
    "\n",
    "# This will return the full report JSON details.\n",
    "evaluate.as_dict\n",
    "\n",
    "# This will return the full HTML contents of the report.\n",
    "evaluate.as_html\n",
    "\n",
    "# Returns a dictionary representation of the top level report scores.\n",
    "evaluate.peek()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0 (default, Sep  8 2022, 12:24:54) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1264641a2296bed54b65447ff0d3f452674f070f0748798274bc429fe6ce8efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
