{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate synthetic data on downstream classification models\n",
    "\n",
    "### How to use this notebook\n",
    "Use this notebook to understand the quality and performance of your synthetic or augmented data on downstream machine learning classification tasks. \n",
    "\n",
    "### Installation\n",
    "Install Gretel Client to use both Gretel's synthetic models as well as the Gretel Evaluate Classification model. You'll have to get your API key from the [Gretel console](https://www.console.gretel.ai) to configure your session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(endpoint=\"https://api.gretel.cloud\", api_key=\"prompt\", cache=\"yes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Try: Generate synthetic data, then evaluate the synthetic data on classifiers against real-world data\n",
    " First, we'll generate synthetic data using a publicly available bank marketing dataset, which predicts whether a client will subscribe a term deposite (prediction: yes/no in column \"y\"). We'll use Gretel's LSTM model to train on the real-world and generate the synthetic data.\n",
    " \n",
    " To use the Gretel Evaluate Classification model, you must indicate the target column. Optionally, you can change the test-holdout amount, which is a float indicating the amount of real-world data you want to use as a holdout for testing the downstream classifiers. Youc an also optionally select which classifiers to use and which metric to optimize for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SUPPORTED MODELS AND METRICS ####\n",
    "## If you want to only use some classification models, you can also indicate which models you want the autoML library to use, by indicating from the list below. \n",
    "## By default, all models will be used in the autoML training. \n",
    "## If you want to change the metric that the classifiers will use to optimize for, you can select one metric from classification_metrics below. The default metric is acc (accuracy).\n",
    "\n",
    "classification_models = [\n",
    "    \"lr\", \n",
    "    \"knn\", \n",
    "    \"nb\", \n",
    "    \"dt\", \n",
    "    \"svm\", \n",
    "    \"rbfsvm\", \n",
    "    \"gpc\", \n",
    "    \"mlp\", \n",
    "    \"ridge\", \n",
    "    \"rf\", \n",
    "    \"qda\", \n",
    "    \"ada\", \n",
    "    \"gbc\", \n",
    "    \"lda\", \n",
    "    \"et\", \n",
    "    \"xgboost\", \n",
    "    \"lightgbm\", \n",
    "    \"dummy\"\n",
    "]\n",
    "\n",
    "shorter_list_classification_models = [\n",
    "    \"nb\", \n",
    "    \"ridge\",\n",
    "    \"rbfsvm\",\n",
    "    \"knn\",\n",
    "    \"xgboost\", \n",
    "    \"ada\", \n",
    "    \"gbc\", \n",
    "    \"mlp\"\n",
    "]\n",
    "\n",
    "classification_metrics = [\n",
    "    \"acc\",\n",
    "    \"auc\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "    \"f1\",\n",
    "    \"kappa\",\n",
    "    \"mcc\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a project on Gretel Cloud using the following example project name. Then, notice that the config includes both the synthetic data model and evaluation model. Note we're using the default Gretel LSTM model configuration in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project with a name that describes this use case\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "project = create_or_get_unique_project(name=\"bank-marketing-classification-notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects.models import read_model_config\n",
    "\n",
    "# We'll import the bank_marketing_small dataset from Gretel's public S3 bucket\n",
    "# You can modify this to select a dataset of your choice\n",
    "dataset_path = \"https://gretel-datasets.s3.amazonaws.com/bank_marketing_small/data.csv\"\n",
    "\n",
    "# We will modify the config for Gretel synthetic models to add an extra downstream Evaluate model and task\n",
    "# Uncomment the additional params to change from defaults.\n",
    "config = read_model_config(\"synthetics/tabular-actgan\")\n",
    "\n",
    "config[\"models\"][0][\"actgan\"][\"evaluate\"] = {\n",
    "    # Available downstream tasks are \"classification\" or \"regression\"\n",
    "    \"task\": \"classification\",\n",
    "    # Set to the target you wish to predict -- Change this if you try a different data set!\n",
    "    \"target\": \"y\",  # yes/no to subscriptions, use a categorical column for classification\n",
    "    # \"holdout\": 0.2,  # default holdout value\n",
    "    # \"models\": classification_models,  # default set of models\n",
    "    # \"metric\": \"acc\",  # default metric used for sorting results, choose one\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now you can train both models, at the end of which you will see all the reports and synthetic data downloaded into the \"tmp\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and run the model\n",
    "## Note: this will both train and run the model to generate synthetic data as well as \n",
    "## run the downstream metrics evaluation immediately after\n",
    "\n",
    "model = project.create_model_obj(\n",
    "    model_config=config, \n",
    "    data_source=dataset_path\n",
    ")\n",
    "\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model)\n",
    "\n",
    "# Save all artifacts\n",
    "model.download_artifacts(\"/tmp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or: BYO synthetic or augmented data to evaluate downstream classification against real-world data\n",
    "Already have your synthetic or augmented data? You can use your own CSV or JSON(L) data files in the Gretel Evaluate Classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Evaluate SDK using your custom config\n",
    "from gretel_client.evaluation.downstream_classification_report import DownstreamClassificationReport\n",
    "\n",
    "# Create a project with a name that describes this use case\n",
    "# When you go to your Gretel Console, you can find this project and also download the report after the evaluation finishes\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "project = create_or_get_unique_project(name=\"evaluate-bank-classification-notebook-2\") \n",
    "\n",
    "# Params\n",
    "# This is the synthetic data, REQUIRED for evaluate model\n",
    "# Download this sample bank marketing synthetic dataset: https://drive.google.com/uc?export=download&id=1s9nT7be3NFC1HrpEIgIj2tKib8ftoAC_\n",
    "# And make sure your file path is correct\n",
    "data_source = \"/Users/[YOUR_USERNAME]/Downloads/bank_marketing_synthetic_data.csv\"\n",
    "\n",
    "# This is the real-world data, REQUIRED for evaluate model\n",
    "ref_data = \"https://gretel-datasets.s3.amazonaws.com/bank_marketing_small/data.csv\"\n",
    "\n",
    "# Target to predict, REQUIRED for evaluate model\n",
    "target = 'y'  # prediction field for whether client will subscribe to a bank term deposit\n",
    "\n",
    "# Default holdout value\n",
    "# test_holdout = 0.2\n",
    "\n",
    "# Supply a subset if you do not want all of these, default is to use all of them\n",
    "# models = classification_models\n",
    "\n",
    "# Metric to use for ordering results, defaults are \"acc\" (Accuracy) for classification, \"r2\" (R2) for regression.\n",
    "# metric = \"acc\"\n",
    "\n",
    "# Create a downstream classification report\n",
    "evaluate = DownstreamClassificationReport(\n",
    "    project=project,\n",
    "    target=target, \n",
    "    data_source=data_source, \n",
    "    ref_data=ref_data,\n",
    "    # holdout=test_holdout,\n",
    "    # models=models,\n",
    "    # metric=metric,\n",
    "    # output_dir = '/tmp', # directory for the report\n",
    "    # runner_mode=\"cloud\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run and view the Evaluate Classification Report\n",
    "\n",
    "evaluate.run() # this will wait for the job to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the full HTML contents of the report.\n",
    "evaluate.as_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the full report JSON details.\n",
    "evaluate.as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary representation of how well the top 3 models trained on synthetic data performed against the \n",
    "# top 3 models trained on real-world data. 'Value' is the synthetic or augmented data's performance against real-world data (averaged)\n",
    "evaluate.peek()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "To see the results of your evaluation in the Gretel Console, go to your [Projects list](https://console-staging.gretel.ai/projects) and look for the projects titled \"bank-marketing-classification-notebook\" or \"evaluate-bank-classification-notebook-2\". You can download the data quality report, regression report, and check out details of the model configuration from there.  \n",
    "\n",
    "Now you can check out the results of the autoML downstream models and keep synthesizing or augmenting your data to get the best results for you. \n",
    "Happy synthesizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1264641a2296bed54b65447ff0d3f452674f070f0748798274bc429fe6ce8efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
