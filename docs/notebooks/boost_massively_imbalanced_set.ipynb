{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f65414e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/boost_massively_imbalanced_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2362aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyyaml numpy pandas sklearn smart_open xgboost\n",
    "!pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ce9e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached Gretel credentials\n",
      "Using endpoint https://api-dev.gretel.cloud\n",
      "Logged in as drew@gretel.ai âœ…\n"
     ]
    }
   ],
   "source": [
    "# Specify your Gretel API key\n",
    "\n",
    "import pandas as pd\n",
    "from gretel_client import configure_session\n",
    "\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31028201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>-1.304168</td>\n",
       "      <td>-1.787931</td>\n",
       "      <td>0.726976</td>\n",
       "      <td>-2.607825</td>\n",
       "      <td>2.234023</td>\n",
       "      <td>-0.193792</td>\n",
       "      <td>-0.620762</td>\n",
       "      <td>-0.497495</td>\n",
       "      <td>0.617784</td>\n",
       "      <td>3.066229</td>\n",
       "      <td>-0.870528</td>\n",
       "      <td>4.237797</td>\n",
       "      <td>-1.840606</td>\n",
       "      <td>1.979937</td>\n",
       "      <td>1.048126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>2.238664</td>\n",
       "      <td>-4.451928</td>\n",
       "      <td>-3.115350</td>\n",
       "      <td>-0.112763</td>\n",
       "      <td>-5.148293</td>\n",
       "      <td>-1.227839</td>\n",
       "      <td>2.607030</td>\n",
       "      <td>1.051266</td>\n",
       "      <td>-1.124752</td>\n",
       "      <td>0.630486</td>\n",
       "      <td>0.719241</td>\n",
       "      <td>-0.239286</td>\n",
       "      <td>2.063776</td>\n",
       "      <td>3.602778</td>\n",
       "      <td>-0.333316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>-1.136602</td>\n",
       "      <td>3.334867</td>\n",
       "      <td>0.136153</td>\n",
       "      <td>-2.736952</td>\n",
       "      <td>1.059928</td>\n",
       "      <td>-0.454829</td>\n",
       "      <td>1.358385</td>\n",
       "      <td>-1.262984</td>\n",
       "      <td>-0.910792</td>\n",
       "      <td>-0.094602</td>\n",
       "      <td>-0.850578</td>\n",
       "      <td>-0.491673</td>\n",
       "      <td>4.966391</td>\n",
       "      <td>-1.601885</td>\n",
       "      <td>-1.390022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>-3.037861</td>\n",
       "      <td>2.146766</td>\n",
       "      <td>0.888396</td>\n",
       "      <td>2.789503</td>\n",
       "      <td>5.806704</td>\n",
       "      <td>-2.132121</td>\n",
       "      <td>0.105169</td>\n",
       "      <td>-0.686753</td>\n",
       "      <td>-2.273966</td>\n",
       "      <td>0.606631</td>\n",
       "      <td>-0.345415</td>\n",
       "      <td>1.056435</td>\n",
       "      <td>1.038290</td>\n",
       "      <td>3.742563</td>\n",
       "      <td>-5.362800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>-0.242303</td>\n",
       "      <td>-1.731569</td>\n",
       "      <td>-1.704046</td>\n",
       "      <td>0.202220</td>\n",
       "      <td>-0.519026</td>\n",
       "      <td>-0.741782</td>\n",
       "      <td>1.579158</td>\n",
       "      <td>-1.430196</td>\n",
       "      <td>2.061192</td>\n",
       "      <td>1.230212</td>\n",
       "      <td>0.227583</td>\n",
       "      <td>0.446696</td>\n",
       "      <td>3.554255</td>\n",
       "      <td>1.822372</td>\n",
       "      <td>-1.342016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "536   -1.304168  -1.787931   0.726976  -2.607825   2.234023  -0.193792   \n",
       "7979   2.238664  -4.451928  -3.115350  -0.112763  -5.148293  -1.227839   \n",
       "3547  -1.136602   3.334867   0.136153  -2.736952   1.059928  -0.454829   \n",
       "4584  -3.037861   2.146766   0.888396   2.789503   5.806704  -2.132121   \n",
       "3947  -0.242303  -1.731569  -1.704046   0.202220  -0.519026  -0.741782   \n",
       "\n",
       "      feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "536   -0.620762  -0.497495   0.617784   3.066229   -0.870528    4.237797   \n",
       "7979   2.607030   1.051266  -1.124752   0.630486    0.719241   -0.239286   \n",
       "3547   1.358385  -1.262984  -0.910792  -0.094602   -0.850578   -0.491673   \n",
       "4584   0.105169  -0.686753  -2.273966   0.606631   -0.345415    1.056435   \n",
       "3947   1.579158  -1.430196   2.061192   1.230212    0.227583    0.446696   \n",
       "\n",
       "      feature_12  feature_13  feature_14  Class  \n",
       "536    -1.840606    1.979937    1.048126      0  \n",
       "7979    2.063776    3.602778   -0.333316      0  \n",
       "3547    4.966391   -1.601885   -1.390022      0  \n",
       "4584    1.038290    3.742563   -5.362800      0  \n",
       "3947    3.554255    1.822372   -1.342016      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create imbalanced train and test data\n",
    "# We will use sklearn's make_classification to create a test dataset.\n",
    "# Or, load your own dataset as a Pandas DataFrame.\n",
    "\n",
    "CLASS_COLUMN = \"Class\"  # the labeled classification column\n",
    "CLASS_VALUE = 1  # the minority classification label to boost\n",
    "MAX_NEIGHBORS = 5  # number of KNN neighbors to use per positive datapoint\n",
    "SYNTHETIC_PERCENT = 10  # generate SYNTHETIC_PERCENT records vs. source data\n",
    "\n",
    "# Create imbalanced test dataset\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "n_features = 15\n",
    "n_recs = 10000\n",
    "\n",
    "\n",
    "def create_dataset(n_features: int) -> pd.DataFrame:\n",
    "    \"\"\"Use sklearn to create a massively imbalanced dataset\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_recs,\n",
    "        n_features=n_features,\n",
    "        n_informative=10,\n",
    "        n_classes=2,\n",
    "        weights=[0.95],\n",
    "        flip_y=0.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(X, columns=[f\"feature_{x}\" for x in range(n_features)])\n",
    "    df = df.round(6)\n",
    "    df[CLASS_COLUMN] = y\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = create_dataset(n_features=n_features)\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69188ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive records shape (rows, columns): (408, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>0.274222</td>\n",
       "      <td>-3.266096</td>\n",
       "      <td>-1.239380</td>\n",
       "      <td>-2.116911</td>\n",
       "      <td>-5.277208</td>\n",
       "      <td>0.009873</td>\n",
       "      <td>-0.152064</td>\n",
       "      <td>3.232532</td>\n",
       "      <td>-0.502239</td>\n",
       "      <td>-0.110357</td>\n",
       "      <td>0.416285</td>\n",
       "      <td>-2.158894</td>\n",
       "      <td>1.299646</td>\n",
       "      <td>1.514053</td>\n",
       "      <td>-0.492109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>-2.141986</td>\n",
       "      <td>-0.677490</td>\n",
       "      <td>-1.193989</td>\n",
       "      <td>-3.292175</td>\n",
       "      <td>3.617636</td>\n",
       "      <td>-0.263354</td>\n",
       "      <td>0.291014</td>\n",
       "      <td>-0.956307</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>-0.111491</td>\n",
       "      <td>2.129525</td>\n",
       "      <td>2.926579</td>\n",
       "      <td>2.170853</td>\n",
       "      <td>1.919613</td>\n",
       "      <td>1.150475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6809</th>\n",
       "      <td>-0.265028</td>\n",
       "      <td>-4.289244</td>\n",
       "      <td>-0.680609</td>\n",
       "      <td>3.735293</td>\n",
       "      <td>-3.600129</td>\n",
       "      <td>0.400657</td>\n",
       "      <td>-0.499392</td>\n",
       "      <td>2.828997</td>\n",
       "      <td>0.401925</td>\n",
       "      <td>-2.642304</td>\n",
       "      <td>-1.984123</td>\n",
       "      <td>-1.341189</td>\n",
       "      <td>-3.645449</td>\n",
       "      <td>0.056322</td>\n",
       "      <td>-0.035453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9073</th>\n",
       "      <td>-1.882966</td>\n",
       "      <td>-1.161521</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>-1.968977</td>\n",
       "      <td>1.851584</td>\n",
       "      <td>-1.284759</td>\n",
       "      <td>0.410516</td>\n",
       "      <td>0.184618</td>\n",
       "      <td>1.199998</td>\n",
       "      <td>-2.685272</td>\n",
       "      <td>-1.070290</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.335742</td>\n",
       "      <td>0.496056</td>\n",
       "      <td>1.905491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>-1.832023</td>\n",
       "      <td>-0.711722</td>\n",
       "      <td>2.798747</td>\n",
       "      <td>-2.534025</td>\n",
       "      <td>2.458227</td>\n",
       "      <td>-2.359642</td>\n",
       "      <td>1.254424</td>\n",
       "      <td>-3.485866</td>\n",
       "      <td>-1.589639</td>\n",
       "      <td>0.316790</td>\n",
       "      <td>1.079321</td>\n",
       "      <td>1.488014</td>\n",
       "      <td>1.667136</td>\n",
       "      <td>-2.201005</td>\n",
       "      <td>1.219179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "2303   0.274222  -3.266096  -1.239380  -2.116911  -5.277208   0.009873   \n",
       "4008  -2.141986  -0.677490  -1.193989  -3.292175   3.617636  -0.263354   \n",
       "6809  -0.265028  -4.289244  -0.680609   3.735293  -3.600129   0.400657   \n",
       "9073  -1.882966  -1.161521   0.274376  -1.968977   1.851584  -1.284759   \n",
       "4774  -1.832023  -0.711722   2.798747  -2.534025   2.458227  -2.359642   \n",
       "\n",
       "      feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "2303  -0.152064   3.232532  -0.502239  -0.110357    0.416285   -2.158894   \n",
       "4008   0.291014  -0.956307   0.333120  -0.111491    2.129525    2.926579   \n",
       "6809  -0.499392   2.828997   0.401925  -2.642304   -1.984123   -1.341189   \n",
       "9073   0.410516   0.184618   1.199998  -2.685272   -1.070290    0.523300   \n",
       "4774   1.254424  -3.485866  -1.589639   0.316790    1.079321    1.488014   \n",
       "\n",
       "      feature_12  feature_13  feature_14  Class  \n",
       "2303    1.299646    1.514053   -0.492109      1  \n",
       "4008    2.170853    1.919613    1.150475      0  \n",
       "6809   -3.645449    0.056322   -0.035453      1  \n",
       "9073    0.335742    0.496056    1.905491      1  \n",
       "4774    1.667136   -2.201005    1.219179      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Split positive and negative datasets\n",
    "positive = train[train[CLASS_COLUMN] == CLASS_VALUE]\n",
    "print(f\"Positive records shape (rows, columns): {positive.shape}\")\n",
    "\n",
    "# Train a nearest neighbor model on the negative dataset\n",
    "neighbors = NearestNeighbors(n_neighbors=MAX_NEIGHBORS, algorithm=\"ball_tree\")\n",
    "neighbors.fit(train)\n",
    "\n",
    "# Locate the nearest neighbors to the positive (minority) set,\n",
    "# and add to the training set.\n",
    "nn = neighbors.kneighbors(positive, MAX_NEIGHBORS, return_distance=False)\n",
    "nn_idx = list(set([item for sublist in nn for item in sublist]))\n",
    "nearest_neighbors = train.iloc[nn_idx, :]\n",
    "\n",
    "oversample = pd.concat([positive] * 5)\n",
    "training_set = pd.concat([oversample, nearest_neighbors]).sample(frac=1)\n",
    "\n",
    "training_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabde917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO: \u001b[0mStarting poller\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"uid\": \"624e44e30d63343893cdac20\",\n",
      "    \"guid\": \"model_27RxHRBgGnYBsRsRTFUj7Yom1BL\",\n",
      "    \"model_name\": \"amusing-kind-baboon\",\n",
      "    \"runner_mode\": \"cloud\",\n",
      "    \"user_id\": \"5ece8962492fbf5bd66089f1\",\n",
      "    \"user_guid\": \"user_26U3XlP53DKYE0zmCWSimFvVFNV\",\n",
      "    \"billing_domain\": \"gretel.ai\",\n",
      "    \"billing_domain_guid\": null,\n",
      "    \"project_id\": \"624e44d2a784abf6f363ce95\",\n",
      "    \"project_guid\": \"proj_27RxFIh1tLD0mYAqIZRcb76fgcP\",\n",
      "    \"status_history\": {\n",
      "        \"created\": \"2022-04-07T01:56:51.158218Z\"\n",
      "    },\n",
      "    \"last_modified\": \"2022-04-07T01:56:51.386254Z\",\n",
      "    \"status\": \"created\",\n",
      "    \"last_active_hb\": null,\n",
      "    \"duration_minutes\": null,\n",
      "    \"error_msg\": null,\n",
      "    \"error_id\": null,\n",
      "    \"traceback\": null,\n",
      "    \"container_image\": \"074762682575.dkr.ecr.us-east-2.amazonaws.com/gretelai/synthetics@sha256:dadb62d3f5e771b08fca1b17e2503feaf08958ac77c74ae01956d366dc39ae77\",\n",
      "    \"model_type\": \"synthetics\",\n",
      "    \"config\": {\n",
      "        \"schema_version\": \"1.0\",\n",
      "        \"name\": null,\n",
      "        \"models\": [\n",
      "            {\n",
      "                \"synthetics\": {\n",
      "                    \"data_source\": [\n",
      "                        \"gretel_74f1b239e6824fd28d5c5b217960fdc5_train.csv\"\n",
      "                    ],\n",
      "                    \"params\": {\n",
      "                        \"field_delimiter\": null,\n",
      "                        \"epochs\": 100,\n",
      "                        \"batch_size\": 64,\n",
      "                        \"vocab_size\": 20000,\n",
      "                        \"reset_states\": false,\n",
      "                        \"learning_rate\": 0.01,\n",
      "                        \"rnn_units\": 256,\n",
      "                        \"dropout_rate\": 0.2,\n",
      "                        \"overwrite\": true,\n",
      "                        \"early_stopping\": true,\n",
      "                        \"gen_temp\": 1.0,\n",
      "                        \"predict_batch_size\": 64,\n",
      "                        \"validation_split\": false,\n",
      "                        \"dp\": false,\n",
      "                        \"dp_noise_multiplier\": 0.001,\n",
      "                        \"dp_l2_norm_clip\": 5.0,\n",
      "                        \"dp_microbatches\": 1,\n",
      "                        \"early_stopping_min_delta\": null,\n",
      "                        \"max_training_time_seconds\": 28200,\n",
      "                        \"field_cluster_size\": 20,\n",
      "                        \"average_record_length_threshold\": 250.0,\n",
      "                        \"data_upsample_limit\": 10000\n",
      "                    },\n",
      "                    \"validators\": {\n",
      "                        \"in_set_count\": 10,\n",
      "                        \"pattern_count\": 10,\n",
      "                        \"datetime_formats\": \"infer\",\n",
      "                        \"use_numeric_iqr\": false\n",
      "                    },\n",
      "                    \"generate\": {\n",
      "                        \"num_records\": 5000,\n",
      "                        \"max_invalid\": null\n",
      "                    },\n",
      "                    \"task\": null,\n",
      "                    \"privacy_filters\": {\n",
      "                        \"outliers\": \"medium\",\n",
      "                        \"similarity\": \"medium\"\n",
      "                    },\n",
      "                    \"debug\": {\n",
      "                        \"invalid_record_cutoff\": 40\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        ],\n",
      "        \"notifications\": null,\n",
      "        \"label_predictors\": null\n",
      "    },\n",
      "    \"autouse_config\": null,\n",
      "    \"autouse_handler_id\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO: \u001b[0mStatus is created. Model creation has been queued.\n",
      "\u001b[33mWARN: \u001b[0mGot interrupt signal.\n",
      "\u001b[32mINFO: \u001b[0mQuitting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drew/.virtualenvs/g/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open\n",
    "import yaml\n",
    "\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "from gretel_client.helpers import poll\n",
    "\n",
    "# Create a project and model configuration.\n",
    "project = create_or_get_unique_project(name=\"boost-imbalanced-synthetic\")\n",
    "\n",
    "# If you want to use a different config or modify it before creating the model,\n",
    "# try something like this (yes, we have other stock configs in that repo)\n",
    "#   from gretel_client.projects.models import read_model_config\n",
    "#   config = read_model_config(\"synthetics/default\")\n",
    "\n",
    "# Get a csv to work with, just dump out the training_set.\n",
    "training_set.to_csv(\"train.csv\", index=False)\n",
    "\n",
    "# Here we just use a shortcut to specify the default synthetics config.\n",
    "# Yes, you can use other shortcuts to point at some of the other stock configs.\n",
    "model = project.create_model_obj(\n",
    "    model_config=\"synthetics/default\", data_source=\"train.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Upload the training data.  Train the model.\n",
    "model.submit_cloud()\n",
    "poll(model)\n",
    "\n",
    "recs_to_generate = int(len(dataset.values) * (SYNTHETIC_PERCENT / 100.0))\n",
    "\n",
    "# Use the model to generate synthetic data.\n",
    "record_handler = model.create_record_handler_obj(\n",
    "    params={\"num_records\": recs_to_generate, \"max_invalid\": recs_to_generate}\n",
    ")\n",
    "record_handler.submit_cloud()\n",
    "\n",
    "poll(record_handler)\n",
    "\n",
    "synthetic_df = pd.read_csv(record_handler.get_artifact_link(\"data\"), compression=\"gzip\")\n",
    "synthetic = synthetic_df[\n",
    "    synthetic_df[CLASS_COLUMN] == CLASS_VALUE\n",
    "]  # Keep only positive examples\n",
    "\n",
    "synthetic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        train.assign(Type=\"train\"),\n",
    "        test.assign(Type=\"test\"),\n",
    "        synthetic.assign(Type=\"synthetic\"),\n",
    "    ]\n",
    ")\n",
    "df.reset_index(inplace=True)\n",
    "df.to_csv(\"combined-boosted-df.csv\")\n",
    "project.upload_artifact(\"combined-boosted-df.csv\")\n",
    "\n",
    "# Save to local CSV\n",
    "synthetic.to_csv(\"boosted-synthetic.csv\", index=False)\n",
    "project.upload_artifact(\"boosted-synthetic.csv\")\n",
    "\n",
    "print(f\"View this project at: {project.get_console_url()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a811d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of positive and negative examples in our \n",
    "# normal vs. boosted datasets\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def visualize_distributions(test: pd.DataFrame, train: pd.DataFrame, synthetic: pd.DataFrame):\n",
    "    \"\"\" Plot the distribution of positive (e.g. fraud) vs negative \n",
    "        e.g. (non-fraud) examples. \n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    fig = plt.figure(1, figsize=(12, 9))\n",
    "\n",
    "    dataframes = {\n",
    "        \"test\": test,\n",
    "        \"train\": train,\n",
    "        \"boosted\": pd.concat([train, synthetic])\n",
    "    }\n",
    "\n",
    "    idx = 0\n",
    "    for name, df in dataframes.items():\n",
    "        df.Class.value_counts().plot.bar(ax=axes[idx], title=name)\n",
    "        idx+=1\n",
    "\n",
    "visualize_distributions(test, train, synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use PCA to visualize highly dimensional data\n",
    "\n",
    "# We will label each data class as:\n",
    "# * Training negative: 0\n",
    "# * Training positive: 1\n",
    "# * Synthetic positive: 2 (our synthetic data points used to boost training data)\n",
    "# * Test positive: 3 (not cheating here, we already trained the classifier)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def create_visualization_dataframe(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Build a new visualization dataframe from our training data\n",
    "    train_vis = train\n",
    "\n",
    "    # Add in positive synthetic results\n",
    "    train_vis = pd.merge(train, synthetic, indicator=True, how=\"outer\")\n",
    "    train_vis.loc[(train_vis._merge == \"right_only\"), \"Class\"] = 2\n",
    "    train_vis = train_vis.drop(columns=[\"_merge\"])\n",
    "\n",
    "    # Add in positive results from the test set\n",
    "    train_vis = pd.merge(\n",
    "        train_vis, test[test[\"Class\"] == 1], indicator=True, how=\"outer\"\n",
    "    )\n",
    "    train_vis.loc[\n",
    "        (train_vis._merge == \"right_only\") | (train_vis._merge == \"both\"), \"Class\"\n",
    "    ] = 3\n",
    "    train_vis = train_vis.drop(columns=[\"_merge\"])\n",
    "    return train_vis\n",
    "\n",
    "\n",
    "def visualize_pca_2d(train_vis: pd.DataFrame):\n",
    "    X = train_vis.iloc[:, :-1]\n",
    "    y = train_vis[\"Class\"]\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 9))\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    x_std = StandardScaler().fit_transform(X)\n",
    "    projected = pca.fit_transform(x_std)\n",
    "\n",
    "    labels = [\"Train Negative\", \"Train Positive\", \"Synthetic Positive\", \"Test Positive\"]\n",
    "    size_map = {0: 25, 1: 50, 2: 75, 3: 50}\n",
    "    sizes = [size_map[x] for x in y]\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        projected[:, 0], projected[:, 1], c=y, s=sizes, cmap=plt.cm.plasma, alpha=0.8\n",
    "    )\n",
    "    plt.title = f\"PCA plot of {n_features}-dimension classification dataset\"\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize PCA distribution in 2D\n",
    "train_vis = create_visualization_dataframe(train)\n",
    "visualize_pca_2d(train_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a020a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA scatter in 3 dimensions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "def visualize_pca_3d(train_vis: pd.DataFrame):\n",
    "    X = train_vis.iloc[:, :-1]\n",
    "    y = train_vis[\"Class\"]\n",
    "\n",
    "    np.random.seed(5)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 9))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "    plt.cla()\n",
    "    pca = decomposition.PCA(n_components=3)\n",
    "    labels = [\"Train Negative\", \"Train Positive\", \"Synthetic Positive\", \"Test Positive\"]\n",
    "    size_map = {0: 25, 1: 50, 2: 75, 3: 50}\n",
    "    sizes = [size_map[x] for x in y]\n",
    "\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        X[:, 0], X[:, 1], X[:, 2], c=y, s=sizes, cmap=plt.cm.plasma, alpha=1.0\n",
    "    )\n",
    "\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize PCA distribution in 3D\n",
    "visualize_pca_3d(train_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an XGBoost model and compare accuracies on the original (normal)\n",
    "# vs. augmented training data (train + synthetic) datasets.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def train_classifier(name: str, train: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"Train our predictor with XGBoost\"\"\"\n",
    "\n",
    "    # Encode labels and categorical variables before training prediction model\n",
    "    X_train = train.iloc[:, :-1]\n",
    "    y_train = train[\"Class\"]\n",
    "    X_test = test.iloc[:, :-1]\n",
    "    y_test = test[\"Class\"]\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"%s : XGBoost Model prediction accuracy: %.2f%%\" % (name, accuracy * 100.0))\n",
    "    return model, y_pred\n",
    "\n",
    "\n",
    "# Train models on normal and augmented data\n",
    "model_normal, y_pred = train_classifier(\"normal\", train, test)\n",
    "model_boosted, y_pred = train_classifier(\"boosted\", pd.concat([train, synthetic]), test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix gives better insight into per-class performance\n",
    "# than overall model accuracy.\n",
    "\n",
    "# As a thought experiment, consider creating a model to predict whether\n",
    "# an account will submit an insurance claim. Our goal is to maximize\n",
    "# accuracy at predicting the minority (positive) set, above those who\n",
    "# will not submit a claim. Try to maximize the diagonal (TP) elements of the\n",
    "# confusion matrix, particularly the bottom right.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "def print_confusion_matrix(name: str, model: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"Plot normalized and non-normalized confusion matrices\"\"\"\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(f\"Plotting confusion matrices for: {name} model\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    fig = plt.figure(1, figsize=(12, 9))\n",
    "    X_test = test.iloc[:, :-1]\n",
    "    y_test = test[\"Class\"]\n",
    "\n",
    "    titles_options = [\n",
    "        (f\"{name} : Confusion matrix, without normalization\", None),\n",
    "        (f\"{name} : Normalized confusion matrix\", \"true\"),\n",
    "    ]\n",
    "\n",
    "    idx = 0\n",
    "    for title, normalize in titles_options:\n",
    "        disp = plot_confusion_matrix(\n",
    "            model,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            display_labels=[\"Negative\", \"Positive\"],\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize,\n",
    "            ax=axes[idx],\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "        idx += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print_confusion_matrix(\"normal\", model_normal, test)\n",
    "print_confusion_matrix(\"boosted\", model_boosted, test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
