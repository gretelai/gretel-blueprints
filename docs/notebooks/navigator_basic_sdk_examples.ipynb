{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/zredlined/542135c75ec9b1ba4fdd54fd71400d28/tabular_llm_basic_sdk_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_CsacPZm9kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💾 Install `gretel-client` and its dependencies"
      ],
      "metadata": {
        "id": "oxzcK8VfciEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -Uqq gretel-client\n",
        "!pip install -qq Jinja2 pandas"
      ],
      "metadata": {
        "id": "JJqUS_rgwl1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛜 Configure your Gretel session\n",
        "\n",
        "- You will be prompted to enter your Gretel API key, which you can retrieve [here](https://console.gretel.ai/users/me/key)."
      ],
      "metadata": {
        "id": "CFgGv53xByt7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRCLZ-hvv-RP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import yaml\n",
        "from IPython.display import display\n",
        "\n",
        "from gretel_client import configure_session, projects\n",
        "from gretel_client.helpers import poll\n",
        "from gretel_client.projects import create_or_get_unique_project\n",
        "\n",
        "# Configure Gretel session\n",
        "configure_session(endpoint=\"https://api.gretel.cloud\", api_key=\"prompt\", cache=\"yes\")\n",
        "\n",
        "# Set Pandas display options (if required)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Create or get a unique Gretel project\n",
        "project = create_or_get_unique_project(name=\"TabularLLM\")\n",
        "\n",
        "print(f\"Project URL: {project.get_console_url()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏗️ Initialize Gretel's Tabular LLM with a custom configuration\n",
        "\n",
        "- Below we initialize the Tabular LLM model in your Gretel Cloud project using a base yaml configuration.\n",
        "\n",
        "- We use JSONL as the output format in this notebook, but CSV can also be used with `output_format: csv`."
      ],
      "metadata": {
        "id": "ZdQSe_Z1CCk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Navigator model\n",
        "model_config = \"\"\"\n",
        "schema_version: 1.0\n",
        "models:\n",
        "  - navigator:\n",
        "        model_id: \"gretelai/tabular-v0\"\n",
        "        output_format: \"jsonl\"\n",
        "\"\"\"\n",
        "model_config = yaml.safe_load(model_config)\n",
        "model = project.create_model_obj(model_config)\n",
        "model.submit_cloud()\n",
        "poll(model, verbose=False)"
      ],
      "metadata": {
        "id": "M8dmK9v4yzmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🧰 Define helper functions\n",
        "# @markdown - Run this cell to define helper functions for\n",
        "# @markdown submitting generation jobs and displaying the results.\n",
        "\n",
        "# Set pandas display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "def clear_project_artifacts(project):\n",
        "    \"\"\"Clear artifacts from the given project.\"\"\"\n",
        "    artifacts = project.artifacts\n",
        "    if artifacts:\n",
        "        print(\"Clearing artifacts\")\n",
        "        for artifact in artifacts:\n",
        "            print(f\" -- {artifact}\")\n",
        "            project.delete_artifact(artifact['key'])\n",
        "\n",
        "def display_all_rows(df):\n",
        "    # Style DataFrame for better visibility and word-wrap\n",
        "    styled = df.style.set_properties(**{\n",
        "        'text-align': 'left',\n",
        "        'white-space': 'normal',\n",
        "        'height': 'auto'\n",
        "    })\n",
        "\n",
        "    # Display the styled DataFrame\n",
        "    display(styled)\n",
        "\n",
        "def submit_generate(model, prompt: str, params: dict, ref_data=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate or augment data from the Tabular LLM model.\n",
        "\n",
        "    Args:\n",
        "    model: The model object that will process the prompt.\n",
        "    prompt (str): The text prompt to generate data from.\n",
        "    params (dict): Parameters for data generation.\n",
        "    ref_data: Optional existing dataset to edit or augment.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The generated data.\n",
        "    \"\"\"\n",
        "    data_processor = model.create_record_handler_obj(\n",
        "        data_source=pd.DataFrame({\"prompt\": [prompt]}),\n",
        "        params=params,\n",
        "        ref_data=ref_data\n",
        "    )\n",
        "    data_processor.submit_cloud()\n",
        "    poll(data_processor, verbose=False)\n",
        "    return pd.read_json(data_processor.get_artifact_link(\"data\"), lines=True, compression=\"gzip\")\n"
      ],
      "metadata": {
        "id": "DtP6afxqy5yM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally clear out previous project artifacts\n",
        "clear_project_artifacts(project)"
      ],
      "metadata": {
        "id": "F44dqc7GamZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Generate synthetic data\n",
        "\n",
        "- Prompt Tabular LLM to create a synthetic dataset.\n"
      ],
      "metadata": {
        "id": "2aqFdfO1cSLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate mock dataset\n",
        "prompt = \"\"\"\\\n",
        "Generate a mock dataset for users from the Foo company based in France.\n",
        "\n",
        "Each user should have the following columns:\n",
        "* first_name: traditional French first names.\n",
        "* last_name: traditional French surnames.\n",
        "* email: formatted as the first letter of their first name followed by their last name @foo.io (e.g., jdupont@foo.io).\n",
        "* gender: Male/Female/Non-binary.\n",
        "* city: a city in France.\n",
        "* country: always 'France'.\n",
        "\"\"\"\n",
        "\n",
        "params = {\n",
        "    \"num_records\": 10,\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 50\n",
        "}\n",
        "df = submit_generate(model=model, prompt=prompt, params=params)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "tbz7D-YEy91f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔧 Augment an existing dataset\n",
        "\n",
        "- Prompt Tabular LLM to add new columns to an existing dataset"
      ],
      "metadata": {
        "id": "CYtHU_ridBei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column to our Pandas Dataframe that is derived from existing values.\n",
        "\n",
        "prompt = \"\"\"Add a new column: initials, which will contain initials of the person.\"\"\"\n",
        "params = {\"num_records\": len(df), \"temperature\": 0.8}\n",
        "ref_data = {\"data\": df}\n",
        "\n",
        "df = submit_generate(model, prompt=prompt, params=params, ref_data=ref_data)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "rynQ1O3hzA5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Generate diverse data with Tabular LLM\n",
        "\n",
        "- Prompt Tabular LLM to answer questions and create new and diverse examples on your domain-specific data."
      ],
      "metadata": {
        "id": "OM_WRRl_d-oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of questions\n",
        "questions = [\n",
        "    \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n",
        "    \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "    \"Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\",\n",
        "    \"Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\",\n",
        "    \"James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\"\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(questions, columns=['question'])\n",
        "\n",
        "prompt = \"\"\"Add a new column: answer, which contains a detailed step-by-step answer to the question in each row.\"\"\"\n",
        "params = {\"num_records\": len(df), \"temperature\": 0.8}\n",
        "ref_data = {\"data\": df}\n",
        "\n",
        "df = submit_generate(model, prompt=prompt, params=params, ref_data=ref_data)\n",
        "\n",
        "display_all_rows(df)"
      ],
      "metadata": {
        "id": "V8ydqwYmJhj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
