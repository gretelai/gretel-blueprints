{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_parent\" href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/synthetics/tab_ft_dp_experiments.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigator Fine Tuning with Differential Privacy: Experiments\n",
    "\n",
    "In this notebook, we generate synthetic tabular data using Navigator Fine Tuning, which allows us to work with both numerical/categorical and free-text columns. We demonstrate how to do that with differential privacy (DP), and compare the performance of the model with and without DP.\n",
    "\n",
    "## Prepare dataset\n",
    "\n",
    "We use an e-commerce dataset that contains free-text reviews as well as other numerical/categorical columns such as rating, product tags, customer age etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"saattrupdan/womens-clothing-ecommerce-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds[\"train\"].to_pandas()\n",
    "\n",
    "# For simplicity, we remove non-standard chars and truncate the review text\n",
    "df_train[\"review_text\"] = df_train[\"review_text\"].str.replace(r'[^A-Za-z0-9 \\.!?\\']+', '', regex=True)\n",
    "df_train[\"review_text\"] = df_train[\"review_text\"].str.slice(0, 128)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit experiment\n",
    "\n",
    "We first set up a generic config that we can use either with or without DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG = lambda dp, epsilon, run_id: f\"\"\"\n",
    "    schema_version: 1.0\n",
    "    name: \"ecommerce-data-{f'dp-eps{epsilon}' if dp else 'nodp'}-run{run_id}\"\n",
    "    models:\n",
    "    - navigator_ft:\n",
    "        group_training_examples_by: null\n",
    "        order_training_examples_by: null\n",
    "\n",
    "        params:\n",
    "            num_input_records_to_sample: auto\n",
    "            batch_size: {8 if dp else 1}\n",
    "\n",
    "        privacy_params:\n",
    "            dp: {'true' if dp else 'false'}\n",
    "            epsilon: {epsilon}\n",
    "\n",
    "        generate:\n",
    "            num_records: 1000\n",
    "            use_structured_generation: {'true' if dp else 'false'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate a Gretel client so that we can submit the experiments, and then launch many of them - both with and without DP, and for multiple values of epsilon. For each, we run it 5 times so that we can have some idea of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client import Gretel\n",
    "\n",
    "PROJECT_NAME = \"navft-dp-experiments\"\n",
    "\n",
    "gretel = Gretel(api_key=\"prompt\", project_name=PROJECT_NAME, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "from gretel_client.gretel.job_results import TrainJobResults\n",
    "models: list[TrainJobResults] = []\n",
    "\n",
    "for i in range(5):\n",
    "    for dp in [False, True]:\n",
    "        if dp:\n",
    "            for epsilon in [2, 4, 8, 16]:\n",
    "                    print(f'Submitting exp w/ DP, epsilon {epsilon}, run {i}')\n",
    "                    model = gretel.submit_train(\n",
    "                        base_config=YAML_CONFIG(dp=True, epsilon=epsilon, run_id=i + 1),\n",
    "                        data_source=df_train,\n",
    "                        wait=False,\n",
    "                    )\n",
    "                    models.append(model)\n",
    "        else:\n",
    "                print(f'Submitting exp w/ no DP, run {i}')\n",
    "                model = gretel.submit_train(\n",
    "                    base_config=YAML_CONFIG(dp=False, epsilon=8, run_id=i + 1),\n",
    "                    data_source=df_train,\n",
    "                    wait=False,\n",
    "                )\n",
    "                models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Await model completion, or use the UI to check\n",
    "for model in models:\n",
    "  model.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check on the dashboard to see if experiments are running, hopefully you did not see any error!\n",
    "\n",
    "After they all finish, we can run the snippet below to fetch and parse metrics we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(columns=['dataset', 'eps', 'exp_type', 'run_id', 'sqs', 'dps', 'failed_runs', 'oom', 'valid_records', 'training_runtime', 'generation_runtime'])\n",
    "\n",
    "for m in gretel._project.search_models(limit=200):\n",
    "    name = m.name\n",
    "    start_time = pd.to_datetime(m.logs[0]['ts'])\n",
    "\n",
    "    model_id = m.model_id\n",
    "    errors = m.errors\n",
    "    config_params = m.model_config['models'][0]['navigator_ft']['params']\n",
    "    generate_params = m.model_config['models'][0]['navigator_ft']['generate']\n",
    "\n",
    "    print(f'Processing {name}')\n",
    "    dataset = 'ecommerce-data'\n",
    "    name = name.removeprefix('ecommerce-data-')\n",
    "    *exp_type, run_id = name.split('-')\n",
    "    if exp_type[0] == 'nodp':\n",
    "        exp_type, = exp_type\n",
    "        eps = None\n",
    "    elif exp_type[0] == 'dpsetup':\n",
    "        exp_type, = exp_type\n",
    "        eps = None\n",
    "    else:\n",
    "        exp_type, eps = exp_type\n",
    "        eps = float(eps.replace('eps', ''))\n",
    "    run_id = int(run_id.replace('run', ''))\n",
    "\n",
    "    oom = 'noise multiplier' in m.logs[-1]['msg'] or \\\n",
    "        'Training' in m.logs[-1]['msg']\n",
    "\n",
    "    report = m.get_report_summary()\n",
    "    if report is None:\n",
    "        results.loc[len(results)] = dataset, eps, exp_type, run_id, None, None, 1.0 if not oom else 0.0, oom, None, None, None \n",
    "        print(f' Skipping, report not found...')\n",
    "        continue\n",
    "    sqs = report['summary'][0]['value']\n",
    "    dps = report['summary'][-1]['value']\n",
    "\n",
    "    rounds, pvr = 0, 0.0\n",
    "    for l in m.logs:\n",
    "        if \"records that are valid\" in l['msg']:\n",
    "            rounds += 1\n",
    "            regex_match = re.search(r\"records that are valid: (.*)\\%\\n\", l['msg'], re.DOTALL)\n",
    "            if regex_match:\n",
    "                pvr += float(regex_match.group(1))\n",
    "    pvr /= rounds\n",
    "\n",
    "    for log in m.logs:\n",
    "        if 'Training Completed' in log['msg']:\n",
    "            end_training = pd.to_datetime(log['ts'])\n",
    "        if 'Generation complete' in log['msg']:\n",
    "            end_generation = pd.to_datetime(log['ts'])\n",
    "\n",
    "    training_runtime = (end_training - start_time).total_seconds()\n",
    "    generation_runtime = (end_generation - end_training).total_seconds()\n",
    "\n",
    "    results.loc[len(results)] = dataset, eps, exp_type, run_id, sqs, dps, 0.0, 0.0, pvr, training_runtime, generation_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the metrics, before plotting, to see if everything looks okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "df = results.copy()\n",
    "df['exp'] = df['exp_type'] + ', eps ' + df['eps'].astype(str)\n",
    "df.drop(columns=['exp_type', 'eps'], inplace=True)\n",
    "df = df.melt(id_vars=['dataset', 'exp', 'run_id'])\n",
    "\n",
    "order = ['nodp, eps nan', 'dp, eps 16.0', 'dp, eps 8.0', 'dp, eps 4.0']\n",
    "x_labels = ['no DP', 'DP, ε = 16', 'DP, ε = 8', 'DP, ε = 4']\n",
    "y_labels = ['SQS', 'DPS', '% valid records', 'training runtime', 'generation runtime']\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(6, 14))\n",
    "for j, dataset in enumerate(df.dataset.unique()):\n",
    "    for i, var in enumerate(['sqs', 'dps', 'valid_records', 'training_runtime', 'generation_runtime']):\n",
    "        ax = axs[i]\n",
    "\n",
    "        subdf = df[(df['dataset'] == dataset) & (df['variable'] == var)]\n",
    "        sns.boxplot(data=subdf, x='exp', y='value', ax=ax, order=order)\n",
    "\n",
    "        ax.set_ylabel(y_labels[i])\n",
    "        ax.set_xlabel('')\n",
    "        if i == 0:\n",
    "            ax.set_title(dataset)\n",
    "        if i != len(axs) - 1:\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            ax.set_xticklabels(x_labels, rotation=30, ha='right')\n",
    "\n",
    "axs[0].set_ylim([55, 95])\n",
    "axs[1].set_ylim([85, 95])\n",
    "fig.set_tight_layout(tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trends observed are as expected: SQS go down as we decrease epsilon, and DPS goes up. The percentage of valid records is 100% due to the use of structured generation. Training and generation time increase considerably, due to changes required for DP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream task\n",
    "\n",
    "Something else we are interested in is how the synthetic data can be used, and if it provides similar results to the original data. For instance, we could look at some analytics, such as the distribution of ratings in a given category.\n",
    "\n",
    "We load synthetic records generated in the above experiments; the DP one uses epsilon = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = df_train.sample(n=2000)\n",
    "train_data, test_data = orig_data[:1000], orig_data[1000:]\n",
    "synth_data_nodp = pd.read_json(\"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/navft/dp_blogpost/ecomm_synth_nodp.jsonl\", lines=True)\n",
    "synth_data_dp = pd.read_json(\"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/navft/dp_blogpost/ecomm_synth_dp.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"original\", \"no DP\", \"DP\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for i, d in enumerate([train_data, synth_data_nodp, synth_data_dp]):\n",
    "    ax = axs[i]\n",
    "    d.loc[d.department_name == 'Tops'].rating.hist(ax=ax)\n",
    "    ax.set_ylim([0, 300])\n",
    "    ax.set_title(names[i])\n",
    "\n",
    "fig.set_tight_layout(tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP results are slightly more concentrated, as DP is known to make infrequent values even more infrequent. We can also see, e.g., if we can use the synthetic data to predict the rating from the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pycaret import classification\n",
    "\n",
    "transform = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_downstream_metrics(train_data, test_data):\n",
    "    train_embeddings = transform.encode(train_data.review_text.to_list())\n",
    "    train_downstream_data = pd.DataFrame(train_embeddings)\n",
    "    train_downstream_data['target'] = train_data.rating.reset_index(drop=True)\n",
    "\n",
    "    test_embeddings = transform.encode(test_data.review_text.to_list())\n",
    "    test_downstream_data = pd.DataFrame(test_embeddings)\n",
    "    test_downstream_data['target'] = test_data.rating.reset_index(drop=True)\n",
    "\n",
    "    exp = classification.setup(data=train_downstream_data, target='target', verbose=False)\n",
    "    model = exp.create_model(estimator='lr', verbose=False)\n",
    "    exp.predict_model(estimator=model, data=test_downstream_data)\n",
    "\n",
    "get_downstream_metrics(train_data=train_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_downstream_metrics(train_data=synth_data_nodp, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_downstream_metrics(train_data=synth_data_dp, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can see that DP gives slightly worse results, but good enough considering the protection that it adds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
