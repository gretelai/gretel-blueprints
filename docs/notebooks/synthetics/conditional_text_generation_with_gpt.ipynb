{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTRxpSlaczHY"
      },
      "source": [
        "# Fine-tune and conditionally generate text with an LLM\n",
        "\n",
        "This notebook will walk you fine-tuning and applying a cutting edge open-source LLM (MosaicML mpt-7b or Llama2) using Gretel's API service, and then using the LLM to generate additional examples matching the desired label.\n",
        "\n",
        "To run this notebook, you will need an API key from the Gretel console at https://console.gretel.cloud. Running the entire notebook should take about 20 minutes for fine-tuning and generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEM6kjRsczHd"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq gretel-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhBCe4PfrTWW"
      },
      "source": [
        "To get started with your project, you'll need to set up the following parameters:\n",
        "\n",
        "* `DATASET_PATH`: Specify the path to your dataset that you want to use for training and generation.\n",
        "* `LLM`: Choose the Language Model (LLM) you wish to use. This must be a supported model in https://docs.gretel.ai/reference/synthetics/models/gretel-gpt.\n",
        "* `GRETEL_PROJECT`: Define the name of your Gretel project where you'll store the trained model and its results. This should be a unique and descriptive name.\n",
        "* `TEXT_COLUMN`: Specify the name of the column in your training dataset that contains the text data you want to use for training the model.\n",
        "* `LABEL_COLUMN`: Identify the corresponding column in your training dataset that contains the class labels or categories for your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ-TmAdwczHd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from gretel_client import configure_session\n",
        "from gretel_client.helpers import poll\n",
        "from gretel_client.projects import create_or_get_unique_project, get_project\n",
        "from gretel_client.projects.models import read_model_config, Model\n",
        "\n",
        "\n",
        "DATASET_PATH = 'https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/banking77.csv'  # @param {type:\"string\"}\n",
        "LLM = \"gretelai/mpt-7b\"  # @param {type:\"string\"}\n",
        "GRETEL_PROJECT = 'banking77'  # @param {type:\"string\"}\n",
        "TEXT_COLUMN = \"text\"# @param {type:\"string\"}\n",
        "LABEL_COLUMN = \"intent\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOygU-1MrTWY"
      },
      "outputs": [],
      "source": [
        "# Log into Gretel and configure project\n",
        "\n",
        "configure_session(api_key=\"prompt\", cache=\"yes\", endpoint=\"https://api.gretel.cloud\", validate=True, clear=True)\n",
        "\n",
        "project = create_or_get_unique_project(name=GRETEL_PROJECT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9LTh7GO6VIu"
      },
      "source": [
        "## Load and preview the training dataset\n",
        "For fine-tuning the LLM, we need to combine the class labels and the text into a single column that we'll add to the dataset. We'll use `,` as a separator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMg9nX6SczHe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "LABEL_AND_TEXT_COLUMN = 'label_and_text'\n",
        "SEPARATOR = ','\n",
        "\n",
        "def create_finetune_dataset(dataset_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a dataset for fine-tuning a language model by combining class labels and text.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): The path to the input dataset in CSV format.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The dataset augmented with a combined label_and_text column.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        df[LABEL_AND_TEXT_COLUMN] = df[LABEL_COLUMN] + SEPARATOR + df[TEXT_COLUMN]\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at '{dataset_path}'\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Create the fine-tuned dataset\n",
        "df = create_finetune_dataset(DATASET_PATH)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxnH8th-65Dh"
      },
      "source": [
        "## Train the synthetic model\n",
        "In this step, we will task a worker running in the Gretel cloud to fine-tune the GPT language model on the source dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4-E_F0qczHe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Download and update base config\n",
        "config = read_model_config(\"synthetics/natural-language\")\n",
        "config['models'][0]['gpt_x']['pretrained_model'] = LLM\n",
        "config['models'][0]['gpt_x']['column_name'] = LABEL_AND_TEXT_COLUMN\n",
        "config\n",
        "\n",
        "# Create and submit model\n",
        "model = project.create_model_obj(model_config=config, data_source=df)\n",
        "print(f\"Follow along with training in the console: {project.get_console_url()}\")\n",
        "model.name = f\"{GRETEL_PROJECT}-{LLM}\"\n",
        "model.submit_cloud()\n",
        "\n",
        "poll(model, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IkWOnVQ7oo1"
      },
      "source": [
        "## Create prompts\n",
        "As we have fine-tuned the model on examples of the form `<label>,<text>`, we can generate new synthetic text examples by promting the text completion model with `<label>,` for a desired class label.\n",
        "\n",
        "The prompt dataset should have a single column with one prompt record for each synthetic text record we want in the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_LABEL = \"card arrival\"  # @param {type:\"string\"}\n",
        "NUM_RECORDS = 25  # @param {type:\"number\"}\n"
      ],
      "metadata": {
        "id": "06U9N5J8Ht61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8gD9Q2XSFyv"
      },
      "outputs": [],
      "source": [
        "\n",
        "pd.set_option('max_colwidth', 300)\n",
        "\n",
        "def create_prompt_df(prompt_label: str, num_records: int = 25) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a prompt DataFrame with the given number of rows, each containing a prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt_label (str): The class label to use in the prompt.\n",
        "        num_records (int): The number of records to generate in the prompt DataFrame.\n",
        "            The generated synthetic data will have the same number of records.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the given number of rows, each containing a class\n",
        "            label prompt.\n",
        "    \"\"\"\n",
        "    # Note: the column name in this dataframe doesn't matter, as it may only contain a single\n",
        "    # column anyway.\n",
        "    # The column name in the generated synthetic data will be taken from the training dataset\n",
        "    # instead.\n",
        "    return pd.DataFrame([prompt_label + SEPARATOR] * num_records, columns=[\"prompt\"])\n",
        "\n",
        "\n",
        "print(\"Text completion prompts with class labels\")\n",
        "prompt_df = create_prompt_df(PROMPT_LABEL, num_records=NUM_RECORDS)\n",
        "prompt_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy8q3f2dTAHv"
      },
      "source": [
        "# Create synthetic data\n",
        "\n",
        "Prompt our fine-tuned model with the prompt dataset, using the model to create new synthetic text examples for the given class label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fx4aeMOSFyw",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_synthetic_data(model: Model, prompt_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generate synthetic data based on a prompt using an AI model.\n",
        "\n",
        "    Args:\n",
        "        model: The LLM used for generating synthetic data.\n",
        "        prompt_df: A single-column dataframe containing the prompts.\n",
        "\n",
        "    Returns:\n",
        "        df: A dataframe containing the synthetic data generated by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a response handler object\n",
        "    response_handler = model.create_record_handler_obj(\n",
        "        params={\"maximum_text_length\": 50, \"temperature\": 0.7},\n",
        "        data_source=prompt_df\n",
        "    )\n",
        "    response_handler.submit_cloud()\n",
        "    poll(response_handler, verbose=False)\n",
        "\n",
        "    # Read the response into a dataframe\n",
        "    df = pd.read_csv(response_handler.get_artifact_link(\"data\"), compression='gzip')\n",
        "\n",
        "    return df\n",
        "\n",
        "synthetic_data = generate_synthetic_data(model, prompt_df)\n",
        "synthetic_data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}