{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "086ba682",
      "metadata": {
        "id": "086ba682"
      },
      "source": [
        "# Gretel Navigator LLM Inference API\n",
        "\n",
        "This Notebook introduces the Gretel LLM real-time inference. In the examples below we will explore:\n",
        "\n",
        "* Initializing an LLM using Gretel\n",
        "* Generating text from a prompt\n",
        "* An advanced example, generating a domain-specific document from instructions.\n",
        "\n",
        "For more details on Gretel Navigator please see Gretel's documentation: https://docs.gretel.ai/create-synthetic-data/models/navigator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a890b3",
      "metadata": {
        "id": "c0a890b3"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq gretel_client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ¨ Initialize Gretel Client and Generate Text\n",
        "\n",
        "- Sign up and get a free API key from: https://console.gretel.ai/users/me/key"
      ],
      "metadata": {
        "id": "uKDaWO9nud32"
      },
      "id": "uKDaWO9nud32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95389d0f",
      "metadata": {
        "id": "95389d0f"
      },
      "outputs": [],
      "source": [
        "from gretel_client import Gretel\n",
        "\n",
        "# Configure your Gretel session\n",
        "gretel = Gretel(api_key=\"prompt\", cache=\"yes\")\n",
        "\n",
        "# Initialize the Client\n",
        "GRETEL_MODEL = \"gretelai/gpt-auto\"\n",
        "llm = gretel.factories.initialize_inference_api(\"natural_language\", backend_model=GRETEL_MODEL)\n",
        "\n",
        "# Prompt the language model to generate text based on an input prompt.\n",
        "PROMPT = \"In the heart of the TARDIS, the Doctor discovered\"\n",
        "generated_text = llm.generate(prompt=PROMPT, temperature=0.8, max_tokens=512)\n",
        "\n",
        "print(f\"\\n{PROMPT} {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Advanced example\n",
        "\n",
        "In this example, we will generate a domain-specific format.\n",
        "\n",
        "Prompt chaining, where multiple prompts are used in sequence, often generates higher quality results than single-shot prompting. This technique allows for more detailed and refined outputs by breaking down the generation process into smaller, manageable steps.\n",
        "\n",
        "### Step 1: Select a Data Format to Create\n",
        "Prompt Gretel to tell you about a particular dataset type, or select one from the dropdown.\n"
      ],
      "metadata": {
        "id": "jEDFFAEKvhZu"
      },
      "id": "jEDFFAEKvhZu"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FORMAT = \"Netflow (Security)\" #@param [\"HL7 v2.5 (Healthcare)\", \"DICOM (Healthcare)\", \"NIST Common Event Format (Security)\", \"Netflow (Security)\", \"JIRA IT Support Tickets\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kS-YxxDvnv0s"
      },
      "id": "kS-YxxDvnv0s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Generate Dataset Description\n",
        "Rather than asking the model to generate a particular document type directly, we ask for instructions to build that type. This approach allows us to inspect and optionally edit the instructions before using them to generate high-quality synthetic examples."
      ],
      "metadata": {
        "id": "N1vnVCCkwoge"
      },
      "id": "N1vnVCCkwoge"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d147b7c",
      "metadata": {
        "id": "9d147b7c"
      },
      "outputs": [],
      "source": [
        "# Uncomment this to specify your own data format to generate\n",
        "# DATA_FORMAT = \"HL7 v2.5 patient check-in message\"\n",
        "\n",
        "format_prompt = f\"\"\"Generate instructions to create a synthetic version of the following data format: {DATA_FORMAT}.\n",
        "\n",
        "Include the following details:\n",
        "\n",
        "* General structure and syntax rules\n",
        "* Key fields and their purposes\n",
        "* Data types used for these fields\n",
        "\"\"\"\n",
        "\n",
        "response_text = llm.generate(prompt=format_prompt, temperature=0.8, max_tokens=512)\n",
        "\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Generate a Complete Document\n",
        "Use the generated instructions to create a complete, unique, and valid document."
      ],
      "metadata": {
        "id": "tbNT4DJzw0Vk"
      },
      "id": "tbNT4DJzw0Vk"
    },
    {
      "cell_type": "code",
      "source": [
        "document_prompt = f\"\"\"Generate a complete, unique, valid document based on the following instructions. {response_text}\"\"\"\n",
        "text = llm.generate(prompt=document_prompt, temperature=1.0, max_tokens=512)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "id": "YvGlPCVEYG5t"
      },
      "id": "YvGlPCVEYG5t",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}