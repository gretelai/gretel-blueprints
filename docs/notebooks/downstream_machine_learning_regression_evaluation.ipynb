{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "Use this notebook to understand the quality and performance of your synthetic or augmented data on downstream machine learning regression tasks. \n",
    "\n",
    "# Installation\n",
    "Install Gretel Client to use Gretel's synthetic models as well as the Gretel Evaluate Regression model. You'll have to get your API key from the [Gretel console](https://www.console.gretel.ai) to configure your session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(endpoint=\"https://api.gretel.cloud\", api_key=\"prompt\", cache=\"yes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Try: Generate synthetic data, then evaluate the synthetic data on regression models against real-world data\n",
    " First, we'll generate synthetic data using a publicly available Dow Jones stock prediction dataset, which predicts the percentage of return that a stock will have in the next week (\"percent_change_next_weeks_price\"). We'll use Gretel's Amplify model to train on the real-world data and generate the synthetic data.\n",
    " \n",
    " To use the Gretel Evaluate Regression model, you must indicate the target column. Optionally, you can change the test-holdout amount, which is a float indicating the amount of real-world data you want to use as a holdout for testing the downstream regression models. Youc an also optionally select which models to use and which metric to optimize for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SUPPORTED MODELS AND METRICS ####\n",
    "## If you want to only use certain regression models, you can also indicate which models you want the autoML library to use, by indicating from the list below. \n",
    "## By default, all models will be used in the autoML training. \n",
    "## If you want to change the metric that the regression models will use to optimize for, you can select one metric from regression_metrics below. \n",
    "## The default metric is R2.\n",
    "\n",
    "regression_models = [\n",
    "    \"lr\",\n",
    "    \"lasso\",\n",
    "    \"ridge\",\n",
    "    \"en\",\n",
    "    \"lar\",\n",
    "    \"llar\",\n",
    "    \"omp\",\n",
    "    \"br\",\n",
    "    \"ard\",\n",
    "    \"par\",\n",
    "    \"ransac\",\n",
    "    \"tr\",\n",
    "    \"huber\",\n",
    "    \"kr\",\n",
    "    \"svm\",\n",
    "    \"knn\",\n",
    "    \"dt\",\n",
    "    \"rf\",\n",
    "    \"et\",\n",
    "    \"ada\",\n",
    "    \"gbr\",\n",
    "    \"mlp\",\n",
    "    \"xgboost\",\n",
    "    \"lightgbm\",\n",
    "    \"dummy\"\n",
    "]\n",
    "\n",
    "regression_metrics = [\n",
    "    \"mae\",\n",
    "    \"mse\",\n",
    "    \"rmse\",\n",
    "    \"r2\",\n",
    "    \"rmsle\",\n",
    "    \"mape\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a project on Gretel Cloud using the following example project name. Then, notice that the config includes both the synthetic data model and evaluation model. Note we're using the default Gretel Amplify model configuration in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project with a name that describes this use case\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "project = create_or_get_unique_project(name=\"dow-jones-regression-evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects.models import read_model_config\n",
    "\n",
    "# We'll import the Dow Jones stock price dataset from Gretel's public S3 bucket\n",
    "# You can modify this to select a dataset of your choice\n",
    "dataset_path = \"https://gretel-datasets.s3.amazonaws.com/dow_jones_index/data.csv\" \n",
    "\n",
    "# Modify the default config to add an extra downstream task.\n",
    "# We do this by adding an evaluate stanza to our config.\n",
    "# Regression example, uncomment the additional params to change from defaults.\n",
    "config = read_model_config(\"synthetics/amplify\")\n",
    "\n",
    "config[\"models\"][0][\"amplify\"][\"evaluate\"] = {\n",
    "    # Available downstream tasks are \"classification\" or \"regression\"\n",
    "    \"task\": \"regression\",\n",
    "    # Set to the target you wish to predict -- Change this if you try a different data set!\n",
    "    \"target\": \"days_to_next_dividend\",  # target column for regression prediction\n",
    "    # \"holdout\": 0.2,  # default holdout value\n",
    "    # \"models\": regression_models,  # default set of models\n",
    "    # \"metric\": \"r2\",  # default metric used for sorting results, choose one\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and run the model\n",
    "## Note: this will both train and run the model to generate synthetic data as well as \n",
    "## run the downstream metrics evaluation immediately after\n",
    "\n",
    "model = project.create_model_obj(\n",
    "    model_config=config, \n",
    "    data_source=dataset_path\n",
    ")\n",
    "\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model)\n",
    "\n",
    "# Save all artifacts\n",
    "model.download_artifacts(\"/tmp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or: BYO synthetic or augmented data to evaluate downstream metrics against real-world data\n",
    "Already have your synthetic or augmented data? You can use your own CSV or JSON(L) data files in the Gretel Evaluate Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Evaluate SDK using your custom config\n",
    "from gretel_client.evaluation.downstream_regression_report import DownstreamRegressionReport\n",
    "\n",
    "# Params\n",
    "# Synthetic data, REQUIRED for evaluate model\n",
    "# Download this sample Dow Jones synthetic dataset: https://drive.google.com/uc?export=download&id=1IeRas0gPH0jGQ2opnPiYPQCNC47SklXs\n",
    "# And make sure the file path is correct\n",
    "data_source = \"/Users/[YOUR_USERNAME]/Downloads/dow_jones_synthetic_data.csv\" \n",
    "\n",
    "# Real data, REQUIRED for evaluate model\n",
    "ref_data = \"https://gretel-datasets.s3.amazonaws.com/dow_jones_index/data.csv\" \n",
    "\n",
    "# Target to predict, REQUIRED for evaluate model\n",
    "target = 'days_to_next_dividend'  # numeric field for regression example\n",
    "\n",
    "# Default holdout value\n",
    "# test_holdout = 0.2\n",
    "\n",
    "# Supply a subset if you do not want all of these, default is to use all of them\n",
    "# models = regression_models\n",
    "\n",
    "# Metric to use for ordering results, defaults are \"acc\" (Accuracy) for classification, \"r2\" (R2) for regression.\n",
    "# metric = \"r2\"\n",
    "\n",
    "# Create a downstream regression report\n",
    "evaluate = DownstreamRegressionReport(\n",
    "    # project=None,  # Create a temp project\n",
    "    target=target, \n",
    "    data_source=data_source, \n",
    "    ref_data=ref_data,\n",
    "    # holdout=test_holdout,\n",
    "    # models=models,\n",
    "    # metric=metric,\n",
    "    output_dir= '/tmp/regression_evaluation', # directory for saving report\n",
    "    # runner_mode=\"cloud\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can run the model and get the report\n",
    "evaluate.run() # this will wait for the job to finish\n",
    "\n",
    "# This will return the full report JSON details.\n",
    "evaluate.as_dict\n",
    "\n",
    "# This will return the full HTML contents of the report.\n",
    "evaluate.as_html\n",
    "\n",
    "# Returns a dictionary representation of how well the top 3 models trained on synthetic data performed against the \n",
    "# top 3 models trained on real-world data. 'Value' is the synthetic or augmented data's performance against real-world data (averaged)\n",
    "evaluate.peek()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can check out the results of the autoML downstream models and keep synthesizing or augmenting your data to get the best results for you. \n",
    "Happy synthesizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1264641a2296bed54b65447ff0d3f452674f070f0748798274bc429fe6ce8efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
