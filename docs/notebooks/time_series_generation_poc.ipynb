{
 "cells": [
 {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gretelai/gretel-blueprints/blob/main/docs/notebooks/time_series_generation_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbv1HhS1dXQq"
   },
   "source": [
    "# Time Series Proof of of Concept\n",
    "\n",
    "This blueprint demonstrates a full proof of concept for creating a synthetic financial time-series dataset and evaluating its privacy and accuracy for a predictive task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXBi_RW5dXQs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -U gretel-client\n",
    "!pip install numpy pandas statsmodels matplotlib seaborn\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3eKIatM1mo4",
    "outputId": "56320388-d8b7-405f-f8c0-b8e5d1c4742e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.statespace import sarimax\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from typing import List, Dict\n",
    "from gretel_client import configure_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Kyza7XJdXQt",
    "outputId": "b87e0d03-9120-4aaf-ad21-dd211a960cca"
   },
   "outputs": [],
   "source": [
    "# Specify your Gretel API key\n",
    "\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "73ciMrkldXQu",
    "outputId": "e0d39781-e93d-4d08-fa88-139e70e4b662"
   },
   "outputs": [],
   "source": [
    "# Load timeseries example to a dataframe\n",
    "\n",
    "data_source = \"https://gretel-public-website.s3.amazonaws.com/datasets/credit-timeseries-dataset.csv\"\n",
    "original_df = pd.read_csv(data_source)\n",
    "original_df.to_csv(\"original.csv\", index=False)\n",
    "original_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thFAMQaDuE8X"
   },
   "outputs": [],
   "source": [
    "# Gretel Transforms Configuration\n",
    "config = \"\"\"\n",
    "schema_version: \"1.0\"\n",
    "models:\n",
    "    - transforms:\n",
    "        data_source: \"__tmp__\"\n",
    "        policies:\n",
    "            - name: shiftnumbers\n",
    "              rules:\n",
    "                - name: shiftnumbers\n",
    "                  conditions:\n",
    "                    field_name:\n",
    "                        - account_balance\n",
    "                        - credit_amt\n",
    "                        - debit_amt\n",
    "                        - net_amt\n",
    "                  transforms:\n",
    "                    - type: numbershift\n",
    "                      attrs:\n",
    "                        min: 1\n",
    "                        max: 100\n",
    "                        field_name:\n",
    "                            - date\n",
    "                            - district_id\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgPpzZP9uKPx",
    "outputId": "e2bfa43f-4a6a-4f91-ccc5-6604007a1eea"
   },
   "outputs": [],
   "source": [
    "# De-identify the original dataset using the policy above\n",
    "import yaml\n",
    "\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "from gretel_client.helpers import poll\n",
    "\n",
    "# Create a project and model configuration.\n",
    "project = create_or_get_unique_project(name=\"numbershift-transform\")\n",
    "\n",
    "model = project.create_model_obj(\n",
    "    model_config=yaml.safe_load(config), data_source=data_source\n",
    ")\n",
    "\n",
    "# Upload the training data.  Train the model.\n",
    "model.submit_cloud()\n",
    "poll(model)\n",
    "\n",
    "record_handler = model.create_record_handler_obj(data_source=data_source)\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler)\n",
    "\n",
    "deid_df = pd.read_csv(record_handler.get_artifact_link(\"data\"), compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "xFtDkVV_yYjU",
    "outputId": "21dfaa6b-899c-4d0a-cbbb-2ca8585716b4"
   },
   "outputs": [],
   "source": [
    "# View the transformation report\n",
    "\n",
    "import json\n",
    "from smart_open import open\n",
    "\n",
    "report = json.loads(open(model.get_artifact_link(\"report_json\")).read())\n",
    "pd.DataFrame(report[\"metadata\"][\"fields\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "VnCiJT43wc1p",
    "outputId": "a1d2fad7-563a-4df3-cec4-92fa937dd14c"
   },
   "outputs": [],
   "source": [
    "# Here we sort and remove \"net_amt\" as it's a derived column,\n",
    "# We will add back in after the data is synthesized\n",
    "train_df = deid_df.copy()\n",
    "\n",
    "train_df.sort_values(\"date\", inplace=True)\n",
    "train_cols = list(train_df.columns)\n",
    "train_cols.remove(\"net_amt\")\n",
    "train_df = train_df.filter(train_cols)\n",
    "\n",
    "# Here we noticed that some number have extremely long precision,\n",
    "# so we round the data\n",
    "train_df = train_df.round(1)\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3tBtsrRQiawq",
    "outputId": "52121882-aa72-41a1-d34b-62f3cb71147b"
   },
   "outputs": [],
   "source": [
    "from gretel_client.projects.models import read_model_config\n",
    "\n",
    "# Create a project and model configuration.\n",
    "project = create_or_get_unique_project(name=\"ts-5544-regular-seed\")\n",
    "\n",
    "# Pull down the default synthetic config.  We will modify it slightly.\n",
    "config = read_model_config(\"synthetics/default\")\n",
    "\n",
    "# Set up the seed fields\n",
    "seed_fields = [\"date\", \"district_id\"]\n",
    "\n",
    "task = {\n",
    "    \"type\": \"seed\",\n",
    "    \"attrs\": {\n",
    "        \"fields\": seed_fields,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Fine tune model parameters. These are the parameters we found to work best.  This is \"Run 20\" in the document\n",
    "config[\"models\"][0][\"synthetics\"][\"task\"] = task\n",
    "\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"vocab_size\"] = 20\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"learning_rate\"] = 0.005\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"epochs\"] = 100\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"gen_temp\"] = 0.8\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"reset_states\"] = True\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"dropout_rate\"] = 0.5\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"gen_temp\"] = 0.8\n",
    "config[\"models\"][0][\"synthetics\"][\"params\"][\"early_stopping\"] = True\n",
    "config[\"models\"][0][\"synthetics\"][\"privacy_filters\"][\"similarity\"] = None\n",
    "config[\"models\"][0][\"synthetics\"][\"privacy_filters\"][\"outliers\"] = None\n",
    "config[\"models\"][0][\"synthetics\"][\"generate\"][\"num_records\"] = train_df.shape[0]\n",
    "\n",
    "# Get a csv to work with, just dump out the train_df.\n",
    "deid_df.to_csv(\"train.csv\", index=False)\n",
    "\n",
    "# Initiate a new model with the chosen config\n",
    "model = project.create_model_obj(model_config=config, data_source=\"train.csv\")\n",
    "\n",
    "# Upload the training data.  Train the model.\n",
    "model.submit_cloud()\n",
    "poll(model)\n",
    "\n",
    "synthetic = pd.read_csv(model.get_artifact_link(\"data_preview\"), compression=\"gzip\")\n",
    "synthetic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GyCx1wuyB0n"
   },
   "outputs": [],
   "source": [
    "# Add back in the derived column \"net_amt\"\n",
    "net_amt = synthetic[\"credit_amt\"] - synthetic[\"debit_amt\"]\n",
    "synthetic[\"net_amt\"] = net_amt\n",
    "\n",
    "# Save off the new synthetic data\n",
    "synthetic.to_csv(\"synthetic.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JS0hXcJ-Y7Oo",
    "outputId": "1ea4100d-99c7-4164-dfa5-27d2394e8c53"
   },
   "outputs": [],
   "source": [
    "# View the Synthetic Performance Report\n",
    "import IPython\n",
    "from smart_open import open\n",
    "\n",
    "IPython.display.HTML(data=open(model.get_artifact_link(\"report\")).read(), metadata=dict(isolated=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gi4d2NuuKQGV",
    "outputId": "547d3129-676c-4bda-8e44-aea19f38453b"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_district_averages(\n",
    "    synthetic: pd.DataFrame, training: pd.DataFrame, district_id: int\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    synthetic_data = synthetic.loc[synthetic[\"district_id\"] == district_id]\n",
    "    synthetic_data = synthetic_data.set_index(\"date\")\n",
    "\n",
    "    training_data = training.loc[training[\"district_id\"] == district_id]\n",
    "    training_data = training_data.set_index(\"date\")\n",
    "\n",
    "    combined = synthetic_data.join(\n",
    "        training_data, lsuffix=\"_synthetic\", rsuffix=\"_original\"\n",
    "    )\n",
    "    plt.suptitle(\"District #\" + str(district_id))\n",
    "\n",
    "    for col in [\"credit_amt\", \"debit_amt\", \"account_balance\"]:\n",
    "        fig = combined.plot(y=[f\"{col}_synthetic\", f\"{col}_original\"], figsize=(12, 8))\n",
    "        plt.title(\"Time Series for District #\" + str(district_id))\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "combined = plot_district_averages(synthetic, train_df, 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24fAgRdLomsn"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def ARIMA_run(\n",
    "    data_paths: List[str],\n",
    "    targets: List[str] = None,\n",
    "    entity_column: str = \"district_id\",\n",
    "    entities: List = None,\n",
    "    date_column: str = \"date\",\n",
    "    date_threshold: str = None,\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Purpose of this function is to automate the run and scoring of SARIMAX models, so we can benchmark results against various different synthetic data configurations.\n",
    "    The data paths from s3 are passed in, and then entire run, from loading in and sorting the data to creating a model and scoring it, is done via this function.\n",
    "    The outputs are the target scores for each variable on each dataset's model. This gets used to create bar charts of the RMSE.\n",
    "    With some fine tuning, this function can be made as a general purpose SARIMAX benchmark function for a variety of datasets.\n",
    "\n",
    "    Args:\n",
    "      data_paths: a list of paths to the data you want to create models and score with. These can be either local paths or ones from public buckets.\n",
    "      targets: Which columns in the data will be your target variables?\n",
    "      entity_column: This is purely used for datasets that have multiple time series data points from multiple places. Since this function was built with that in mind, it assumes that you will\n",
    "      give a column that denotes those different places/entities. If None is provided, no handler has been built yet that can handle that.\n",
    "      entities: This should be a list of the set of entities within the entity column.\n",
    "      date_column: This should be something we can use to sort the data, so that the time series is read appropriately.\n",
    "      date_threshold: This is to split the data into train and test. Whatever date you want to threshold by to make the train and test should be specified here.\n",
    "\n",
    "    Outputs:\n",
    "      target_scores: This will be a dictionary of RMSE scores for each target variable on each synthetic dataset.\n",
    "    \"\"\"\n",
    "    target_scores = {}\n",
    "    for target in targets:\n",
    "        target_scores[target] = []\n",
    "    for path in data_paths:\n",
    "        sorted_data = pd.read_csv(path)\n",
    "        sorted_data.sort_values(date_column, inplace=True)\n",
    "        sorted_data.drop_duplicates(subset=[date_column, entity_column], inplace=True)\n",
    "\n",
    "        print(\"Path: {}\".format(path))\n",
    "        for entity in entities:\n",
    "            print(\"Entity: {}\".format(entity))\n",
    "            for target in targets:\n",
    "                train_data = sorted_data[sorted_data[entity_column] == entity][\n",
    "                    sorted_data[date_column] < date_threshold\n",
    "                ]\n",
    "                test_data = sorted_data[sorted_data[entity_column] == entity][\n",
    "                    sorted_data[date_column] >= date_threshold\n",
    "                ]\n",
    "\n",
    "                model = sarimax.SARIMAX(\n",
    "                    train_data[target], order=(0, 1, 1), seasonal_order=(1, 1, 0, 12)\n",
    "                )\n",
    "                res = model.fit()\n",
    "\n",
    "                preds = res.forecast(len(test_data[target]))\n",
    "                rmse = mean_squared_error(test_data[target], preds, squared=False)\n",
    "                target_scores[target].append(rmse)\n",
    "                print(\"Target: {}\".format(target))\n",
    "                print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "    return target_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsK5p3YB204I",
    "outputId": "3fbc4baa-6599-4c0f-e5ce-45bcfcecd00e"
   },
   "outputs": [],
   "source": [
    "target_scores = ARIMA_run(\n",
    "    [\"synthetic.csv\", \"original.csv\"],\n",
    "    targets=[\"net_amt\", \"account_balance\", \"credit_amt\", \"debit_amt\"],\n",
    "    entities=[13],\n",
    "    date_threshold=\"1998-01-01\",\n",
    ")\n",
    "target_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "d9BewEn_3B6x",
    "outputId": "7d6946ae-0825-4c4c-abd9-c98106ae1c80"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "results = pd.DataFrame.from_dict(target_scores)\n",
    "results[\"method\"] = [\"synthetic\", \"real world\"]\n",
    "results.plot.bar(x=\"method\", title=\"RMSE per field and run in synthetic timeseries\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Time Series Generation POC - Gretel and Global Financial Institution",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
