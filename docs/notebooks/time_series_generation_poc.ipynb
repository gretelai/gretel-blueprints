{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbv1HhS1dXQq"
   },
   "source": [
    "# Time Series Proof of of Concept\n",
    "This blueprint demonstrates a full proof of concept for creating a synthetic financial time-series dataset and evaluating its privacy and accuracy for a predictive task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXBi_RW5dXQs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -U gretel-client\n",
    "!pip install numpy pandas statsmodels matplotlib seaborn\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3eKIatM1mo4",
    "outputId": "56320388-d8b7-405f-f8c0-b8e5d1c4742e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.statespace import sarimax\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from typing import List, Dict\n",
    "from gretel_client import configure_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Kyza7XJdXQt",
    "outputId": "b87e0d03-9120-4aaf-ad21-dd211a960cca"
   },
   "outputs": [],
   "source": [
    "# Specify your Gretel API key\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "73ciMrkldXQu",
    "outputId": "e0d39781-e93d-4d08-fa88-139e70e4b662"
   },
   "outputs": [],
   "source": [
    "# Load timeseries example to a dataframe\n",
    "\n",
    "data_source = 'https://gretel-public-website.s3.amazonaws.com/datasets/credit-timeseries-dataset.csv'\n",
    "original_df = pd.read_csv(data_source)\n",
    "original_df.to_csv('original.csv', index=False)\n",
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thFAMQaDuE8X"
   },
   "outputs": [],
   "source": [
    "# Gretel Transforms Configuration\n",
    "config = \"\"\"\n",
    "schema_version: \"1.0\"\n",
    "models:\n",
    "    - transforms:\n",
    "        data_source: \"__tmp__\"\n",
    "        policies:\n",
    "            - name: shiftnumbers\n",
    "              rules:\n",
    "                - name: shiftnumbers\n",
    "                  conditions:\n",
    "                    field_name:\n",
    "                        - account_balance\n",
    "                        - credit_amt\n",
    "                        - debit_amt\n",
    "                        - net_amt\n",
    "                  transforms:\n",
    "                    - type: numbershift\n",
    "                      attrs:\n",
    "                        min: 1\n",
    "                        max: 100\n",
    "                        field_name:\n",
    "                            - date\n",
    "                            - district_id\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgPpzZP9uKPx",
    "outputId": "e2bfa43f-4a6a-4f91-ccc5-6604007a1eea"
   },
   "outputs": [],
   "source": [
    "# De-identify the original dataset using the policy above\n",
    "import yaml\n",
    "\n",
    "from gretel_client import create_project\n",
    "from gretel_client.helpers import poll\n",
    "\n",
    "# Create a project and model configuration.\n",
    "project = create_project(display_name=\"numbershift-transform\")\n",
    "\n",
    "model = project.create_model_obj(model_config=yaml.safe_load(config), data_source=data_source)\n",
    "\n",
    "# Upload the training data.  Train the model.\n",
    "model.submit_cloud()\n",
    "poll(model)\n",
    "\n",
    "record_handler = model.create_record_handler_obj(data_source=data_source)\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler)\n",
    "\n",
    "deid_df = pd.read_csv(record_handler.get_artifact_link(\"data\"), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "xFtDkVV_yYjU",
    "outputId": "21dfaa6b-899c-4d0a-cbbb-2ca8585716b4"
   },
   "outputs": [],
   "source": [
    "# View the transformation report\n",
    "\n",
    "import json\n",
    "from smart_open import open\n",
    "\n",
    "report = json.loads(open(model.get_artifact_link(\"report_json\")).read())\n",
    "pd.DataFrame(report[\"metadata\"][\"fields\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "VnCiJT43wc1p",
    "outputId": "a1d2fad7-563a-4df3-cec4-92fa937dd14c"
   },
   "outputs": [],
   "source": [
    "# Here we sort and remove \"net_amt\" as it's a derived column, \n",
    "# We will add back in after the data is synthesized\n",
    "train_df = deid_df.copy()\n",
    "\n",
    "train_df.sort_values('date', inplace=True)\n",
    "train_cols = list(train_df.columns)\n",
    "train_cols.remove(\"net_amt\")\n",
    "train_df = train_df.filter(train_cols)\n",
    "\n",
    "# Here we noticed that some number have extremely long precision, \n",
    "# so we round the data\n",
    "train_df = train_df.round(1)\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3tBtsrRQiawq",
    "outputId": "52121882-aa72-41a1-d34b-62f3cb71147b"
   },
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "import yaml\n",
    "\n",
    "from gretel_client import create_project\n",
    "from gretel_client.helpers import poll\n",
    "\n",
    "# Create a project and model configuration.\n",
    "project = create_project(display_name=\"ts-5544-regular-seed\")\n",
    "\n",
    "# Pull down the default synthetic config.  We will modify it slightly.\n",
    "with open(\"https://raw.githubusercontent.com/gretelai/gretel-blueprints/main/config_templates/gretel/synthetics/default.yml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "#Set up the seed fields\n",
    "seed_fields = [\"date\", \"district_id\"]\n",
    "\n",
    "task = {\n",
    "    'type': 'seed',\n",
    "    'attrs': {\n",
    "        'fields': seed_fields,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fine tune model parameters. These are the parameters we found to work best.  This is \"Run 20\" in the document\n",
    "config['models'][0]['synthetics']['task'] = task\n",
    "\n",
    "config['models'][0]['synthetics']['params']['vocab_size'] = 20\n",
    "config['models'][0]['synthetics']['params']['learning_rate'] = 0.005\n",
    "config['models'][0]['synthetics']['params']['epochs'] = 100\n",
    "config['models'][0]['synthetics']['params']['gen_temp'] = 0.8\n",
    "config['models'][0]['synthetics']['params']['reset_states'] = True\n",
    "config['models'][0]['synthetics']['params']['dropout_rate'] = 0.5\n",
    "config['models'][0]['synthetics']['params']['gen_temp'] = 0.8\n",
    "config['models'][0]['synthetics']['params']['early_stopping'] = True\n",
    "config['models'][0]['synthetics']['privacy_filters']['similarity'] = None\n",
    "config['models'][0]['synthetics']['privacy_filters']['outliers'] = None\n",
    "config['models'][0]['synthetics']['generate']['num_records'] = train_df.shape[0]\n",
    "\n",
    "# Get a csv to work with, just dump out the train_df.\n",
    "deid_df.to_csv('train.csv', index=False)\n",
    "\n",
    "# Initiate a new model with the chosen config\n",
    "model = project.create_model_obj(model_config=config, data_source='train.csv')\n",
    "\n",
    "# Upload the training data.  Train the model.\n",
    "model.submit_cloud()\n",
    "poll(model)\n",
    "\n",
    "synthetic = pd.read_csv(model.get_artifact_link(\"data_preview\"), compression='gzip')\n",
    "synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GyCx1wuyB0n"
   },
   "outputs": [],
   "source": [
    "# Add back in the derived column \"net_amt\"\n",
    "net_amt = synthetic[\"credit_amt\"] - synthetic[\"debit_amt\"]\n",
    "synthetic[\"net_amt\"] = net_amt\n",
    "\n",
    "# Save off the new synthetic data\n",
    "synthetic.to_csv(\"synthetic.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JS0hXcJ-Y7Oo",
    "outputId": "1ea4100d-99c7-4164-dfa5-27d2394e8c53"
   },
   "outputs": [],
   "source": [
    "# View the Synthetic Performance Report\n",
    "import IPython\n",
    "from smart_open import open\n",
    "\n",
    "IPython.display.HTML(data=open(model.get_artifact_link(\"report\")).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gi4d2NuuKQGV",
    "outputId": "547d3129-676c-4bda-8e44-aea19f38453b"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "def plot_district_averages(synthetic: pd.DataFrame, training: pd.DataFrame, district_id: int) -> pd.DataFrame:\n",
    "\n",
    "    synthetic_data = synthetic.loc[synthetic['district_id']==district_id]\n",
    "    synthetic_data = synthetic_data.set_index('date')\n",
    "\n",
    "    training_data = training.loc[training['district_id']==district_id]\n",
    "    training_data = training_data.set_index('date')\n",
    "    \n",
    "    combined = synthetic_data.join(training_data, lsuffix='_synthetic', rsuffix='_original')\n",
    "    plt.suptitle('District #'+str(district_id))\n",
    "\n",
    "    for col in ['credit_amt', 'debit_amt', 'account_balance']:\n",
    "        fig = combined.plot(y=[f'{col}_synthetic', f'{col}_original'], figsize=(12, 8))\n",
    "        plt.title('Time Series for District #'+str(district_id))\n",
    "\n",
    "    return combined\n",
    "        \n",
    "combined = plot_district_averages(synthetic, train_df, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24fAgRdLomsn"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ARIMA_run(data_paths: List[str], \n",
    "              targets: List[str] = None, \n",
    "              entity_column: str = 'district_id', \n",
    "              entities: List = None, \n",
    "              date_column: str = 'date', \n",
    "              date_threshold: str = None) -> Dict[str, List[float]]:\n",
    "  '''\n",
    "  Purpose of this function is to automate the run and scoring of SARIMAX models, so we can benchmark results against various different synthetic data configurations.\n",
    "  The data paths from s3 are passed in, and then entire run, from loading in and sorting the data to creating a model and scoring it, is done via this function.\n",
    "  The outputs are the target scores for each variable on each dataset's model. This gets used to create bar charts of the RMSE.\n",
    "  With some fine tuning, this function can be made as a general purpose SARIMAX benchmark function for a variety of datasets.\n",
    "\n",
    "  Args:\n",
    "    data_paths: a list of paths to the data you want to create models and score with. These can be either local paths or ones from public buckets.\n",
    "    targets: Which columns in the data will be your target variables?\n",
    "    entity_column: This is purely used for datasets that have multiple time series data points from multiple places. Since this function was built with that in mind, it assumes that you will\n",
    "    give a column that denotes those different places/entities. If None is provided, no handler has been built yet that can handle that.\n",
    "    entities: This should be a list of the set of entities within the entity column.\n",
    "    date_column: This should be something we can use to sort the data, so that the time series is read appropriately.\n",
    "    date_threshold: This is to split the data into train and test. Whatever date you want to threshold by to make the train and test should be specified here.\n",
    "\n",
    "  Outputs:\n",
    "    target_scores: This will be a dictionary of RMSE scores for each target variable on each synthetic dataset.\n",
    "  '''\n",
    "  target_scores = {}\n",
    "  for target in targets:\n",
    "    target_scores[target] = []\n",
    "  for path in data_paths:\n",
    "    sorted_data = pd.read_csv(path)\n",
    "    sorted_data.sort_values(date_column, inplace=True)\n",
    "    sorted_data.drop_duplicates(subset = [date_column, entity_column], inplace = True)\n",
    "\n",
    "    print('Path: {}'.format(path))\n",
    "    for entity in entities:\n",
    "      print('Entity: {}'.format(entity))\n",
    "      for target in targets:\n",
    "        train_data = sorted_data[sorted_data[entity_column] == entity][sorted_data[date_column] < date_threshold]\n",
    "        test_data = sorted_data[sorted_data[entity_column] == entity][sorted_data[date_column] >= date_threshold]\n",
    "\n",
    "        model = sarimax.SARIMAX(train_data[target], order=(0,1,1), seasonal_order=(1,1,0,12))\n",
    "        res = model.fit()\n",
    "\n",
    "        preds = res.forecast(len(test_data[target]))\n",
    "        rmse = mean_squared_error(test_data[target], preds, squared = False)\n",
    "        target_scores[target].append(rmse)\n",
    "        print('Target: {}'.format(target))\n",
    "        print('RMSE: {}'.format(rmse))\n",
    "\n",
    "  return target_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsK5p3YB204I",
    "outputId": "3fbc4baa-6599-4c0f-e5ce-45bcfcecd00e"
   },
   "outputs": [],
   "source": [
    "target_scores = ARIMA_run(['synthetic.csv', 'original.csv'], targets = ['net_amt', 'account_balance', 'credit_amt', 'debit_amt'], entities = [13], date_threshold = '1998-01-01')\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "d9BewEn_3B6x",
    "outputId": "7d6946ae-0825-4c4c-abd9-c98106ae1c80"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "results = pd.DataFrame.from_dict(target_scores)\n",
    "results['method'] = ['synthetic', 'real world']\n",
    "results.plot.bar(x = 'method', title=\"RMSE per field and run in synthetic timeseries\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Time Series Generation POC - Gretel and Global Financial Institution",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
